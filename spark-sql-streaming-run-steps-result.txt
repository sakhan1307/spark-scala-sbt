--Open new terminal and start zookeeper and kafka
$ cd /usr/lib/kafka_2.12-0.11.0.0
$ bin/zookeeper-server-start.sh config/zookeeper.properties

--Open new terminal and start  kafka
$ cd /usr/lib/kafka_2.12-0.11.0.0
$ bin/kafka-server-start.sh config/server.properties

 --Open new terminal and check list of the topics
 $ cd /usr/lib/kafka_2.12-0.11.0.0
 $ bin/kafka-topics.sh --zookeeper localhost:2181 --list
  
 -- if not exist topic then create topic 
$ bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic kafka_topic

------------------------------------------------------------
	libraryDependencies ++= Seq(
		"org.apache.spark" %% "spark-core" % "2.1.1",
		"org.apache.spark" %% "spark-sql" % "2.1.1",
		"org.apache.kafka" %% "kafka" % "0.11.0.0",
		"org.apache.spark" %% "spark-streaming" % "2.1.1" % "provided",
		"org.apache.spark" %% "spark-streaming-kafka-0-10" % "2.0.0"
	)

--open new terminal and start consumer -- it needs spark streaming kafka jars as per scala version --repositories https://repo.maven.apache.org/maven2 --packages org.apache.spark:spark-streaming-kafka-0-10_2.11:2.0.0
$ cd /home/edureka/SHAHBAZWS/BATCH170917/sparkjobs

$ spark-submit --verbose --master local --deploy-mode client --conf spark.driver.extraClassPath=/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/mysql-connector-java-5.1.43-bin.jar:/usr/lib/kafka_2.12-0.11.0.0/libs/kafka-clients-0.11.0.0.jar --repositories https://repo.maven.apache.org/maven2 --packages org.apache.spark:spark-streaming-kafka-0-10_2.11:2.0.0 --class com.laboros.spark.sql.stream.SparkStreamingConsumer sparkdemo_2.11-0.1.0-SNAPSHOT.jar kafka_topic
 

--open new terminal and start producer
$ cd /home/edureka/SHAHBAZWS/BATCH170917/sparkjobs

$ spark-submit --verbose --master local --deploy-mode client --conf spark.driver.extraClassPath=/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/mysql-connector-java-5.1.43-bin.jar:/usr/lib/kafka_2.12-0.11.0.0/libs/kafka-clients-0.11.0.0.jar --class com.laboros.spark.sql.stream.SparkSqlProducer sparkdemo_2.11-0.1.0-SNAPSHOT.jar kafka_topic

------------------------------------------------------------------------------------------------------------------------------------------------
--zookeeper

[edureka@localhost sparkjobs]$ cd /usr/lib/kafka_2.12-0.11.0.0
[edureka@localhost kafka_2.12-0.11.0.0]$ bin/zookeeper-server-start.sh config/zookeeper.properties
[2018-03-02 22:12:39,658] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2018-03-02 22:12:39,667] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2018-03-02 22:12:39,667] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2018-03-02 22:12:39,667] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2018-03-02 22:12:39,667] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2018-03-02 22:12:39,700] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2018-03-02 22:12:39,701] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2018-03-02 22:12:39,716] INFO Server environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2018-03-02 22:12:39,716] INFO Server environment:host.name=localhost (org.apache.zookeeper.server.ZooKeeperServer)
[2018-03-02 22:12:39,717] INFO Server environment:java.version=1.8.0_144 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-03-02 22:12:39,717] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2018-03-02 22:12:39,717] INFO Server environment:java.home=/usr/lib/jvm/jdk1.8.0_144/jre (org.apache.zookeeper.server.ZooKeeperServer)
[2018-03-02 22:12:39,717] INFO Server environment:java.class.path=:/FLUME_HOME/lib/*:/FLUME_HOME/lib/*:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/aopalliance-repackaged-2.5.0-b05.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/argparse4j-0.7.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/commons-lang3-3.5.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/connect-api-0.11.0.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/connect-file-0.11.0.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/connect-json-0.11.0.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/connect-runtime-0.11.0.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/connect-transforms-0.11.0.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/guava-20.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/hk2-api-2.5.0-b05.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/hk2-locator-2.5.0-b05.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/hk2-utils-2.5.0-b05.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jackson-annotations-2.8.5.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jackson-core-2.8.5.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jackson-databind-2.8.5.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jackson-jaxrs-base-2.8.5.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jackson-jaxrs-json-provider-2.8.5.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jackson-module-jaxb-annotations-2.8.5.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/javassist-3.21.0-GA.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/javax.annotation-api-1.2.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/javax.inject-1.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/javax.inject-2.5.0-b05.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/javax.servlet-api-3.1.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/javax.ws.rs-api-2.0.1.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jersey-client-2.24.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jersey-common-2.24.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jersey-container-servlet-2.24.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jersey-container-servlet-core-2.24.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jersey-guava-2.24.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jersey-media-jaxb-2.24.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jersey-server-2.24.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jetty-continuation-9.2.15.v20160210.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jetty-http-9.2.15.v20160210.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jetty-io-9.2.15.v20160210.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jetty-security-9.2.15.v20160210.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jetty-server-9.2.15.v20160210.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jetty-servlet-9.2.15.v20160210.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jetty-servlets-9.2.15.v20160210.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jetty-util-9.2.15.v20160210.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jopt-simple-5.0.3.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/kafka_2.12-0.11.0.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/kafka_2.12-0.11.0.0-sources.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/kafka_2.12-0.11.0.0-test-sources.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/kafka-clients-0.11.0.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/kafka-log4j-appender-0.11.0.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/kafka-streams-0.11.0.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/kafka-streams-examples-0.11.0.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/kafka-tools-0.11.0.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/log4j-1.2.17.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/lz4-1.3.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/maven-artifact-3.5.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/metrics-core-2.2.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/osgi-resource-locator-1.0.1.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/plexus-utils-3.0.24.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/reflections-0.9.11.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/rocksdbjni-5.0.1.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/scala-library-2.12.2.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/scala-parser-combinators_2.12-1.0.4.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/slf4j-api-1.7.25.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/slf4j-log4j12-1.7.25.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/snappy-java-1.1.2.6.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/validation-api-1.1.0.Final.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/zkclient-0.10.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/zookeeper-3.4.10.jar (org.apache.zookeeper.server.ZooKeeperServer)
[2018-03-02 22:12:39,718] INFO Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer)
[2018-03-02 22:12:39,718] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)
[2018-03-02 22:12:39,718] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2018-03-02 22:12:39,718] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)
[2018-03-02 22:12:39,718] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-03-02 22:12:39,724] INFO Server environment:os.version=3.10.0-514.26.2.el7.x86_64 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-03-02 22:12:39,724] INFO Server environment:user.name=edureka (org.apache.zookeeper.server.ZooKeeperServer)
[2018-03-02 22:12:39,724] INFO Server environment:user.home=/home/edureka (org.apache.zookeeper.server.ZooKeeperServer)
[2018-03-02 22:12:39,724] INFO Server environment:user.dir=/usr/lib/kafka_2.12-0.11.0.0 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-03-02 22:12:39,742] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-03-02 22:12:39,742] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-03-02 22:12:39,742] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-03-02 22:12:39,766] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-03-02 22:13:43,245] INFO Accepted socket connection from /127.0.0.1:40546 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-03-02 22:13:43,263] INFO Client attempting to establish new session at /127.0.0.1:40546 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-03-02 22:13:43,265] INFO Creating new log file: log.4a (org.apache.zookeeper.server.persistence.FileTxnLog)
[2018-03-02 22:13:43,294] INFO Established session 0x161e798179f0000 with negotiated timeout 6000 for client /127.0.0.1:40546 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-03-02 22:13:45,219] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:delete cxid:0x2f zxid:0x4e txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:13:45,309] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x39 zxid:0x4f txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:13:45,310] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x3a zxid:0x50 txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:14:58,166] INFO Accepted socket connection from /127.0.0.1:40552 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-03-02 22:14:58,171] INFO Client attempting to establish new session at /127.0.0.1:40552 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-03-02 22:14:58,173] INFO Established session 0x161e798179f0001 with negotiated timeout 30000 for client /127.0.0.1:40552 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-03-02 22:14:58,209] INFO Processed session termination for sessionid: 0x161e798179f0001 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:14:58,216] INFO Closed socket connection for client /127.0.0.1:40552 which had sessionid 0x161e798179f0001 (org.apache.zookeeper.server.NIOServerCnxn)
[2018-03-02 22:40:08,691] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:setData cxid:0x47 zxid:0x55 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:08,700] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x49 zxid:0x56 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:08,990] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x88 zxid:0x59 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/19 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/19 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,012] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x8c zxid:0x5a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,055] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x90 zxid:0x5e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/30 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/30 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,100] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x95 zxid:0x61 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/47 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/47 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,117] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x99 zxid:0x64 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/29 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/29 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,140] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x9c zxid:0x67 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/41 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/41 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,151] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x9f zxid:0x6a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/39 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/39 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,158] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xa2 zxid:0x6d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/10 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/10 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,169] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xa5 zxid:0x70 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/17 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/17 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,177] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xa8 zxid:0x73 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/14 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/14 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,185] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xab zxid:0x76 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/40 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/40 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,208] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xb0 zxid:0x79 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/18 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/18 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,223] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xb4 zxid:0x7c txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/26 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/26 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,230] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xb7 zxid:0x7f txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,247] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xba zxid:0x82 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/24 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/24 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,256] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xbd zxid:0x85 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/33 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/33 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,262] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xc0 zxid:0x88 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/20 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/20 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,271] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xc3 zxid:0x8b txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/21 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/21 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,284] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xc6 zxid:0x8e txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/3 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/3 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,302] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xc9 zxid:0x91 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/5 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/5 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,311] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xce zxid:0x94 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/22 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/22 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,327] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xd2 zxid:0x97 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/12 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/12 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,333] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xd5 zxid:0x9a txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/8 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/8 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,340] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xd8 zxid:0x9d txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/23 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/23 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,347] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xdb zxid:0xa0 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/15 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/15 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,360] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xde zxid:0xa3 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/48 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/48 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,369] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xe1 zxid:0xa6 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/11 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/11 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,378] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xe4 zxid:0xa9 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/13 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/13 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,389] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xe7 zxid:0xac txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/49 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/49 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,405] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xea zxid:0xaf txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/6 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/6 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,421] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xef zxid:0xb2 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/28 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/28 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,451] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xf3 zxid:0xb5 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/4 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/4 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,462] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xf6 zxid:0xb8 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/37 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/37 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,472] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xf9 zxid:0xbb txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/31 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/31 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,481] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xfc zxid:0xbe txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/44 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/44 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,492] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0xff zxid:0xc1 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/42 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/42 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,505] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x102 zxid:0xc4 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/34 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/34 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,530] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x108 zxid:0xc7 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/46 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/46 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,565] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x10b zxid:0xca txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/25 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/25 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,578] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x10e zxid:0xcd txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/45 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/45 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,593] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x111 zxid:0xd0 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/27 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/27 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,603] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x114 zxid:0xd3 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/32 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/32 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,621] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x11a zxid:0xd6 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/43 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/43 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,639] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x11d zxid:0xd9 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/36 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/36 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,647] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x120 zxid:0xdc txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/35 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/35 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,654] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x123 zxid:0xdf txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/7 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/7 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,667] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x126 zxid:0xe2 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/9 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/9 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:09,695] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x129 zxid:0xe5 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/38 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/38 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:11,123] WARN fsync-ing the write ahead log in SyncThread:0 took 1426ms which will adversely effect operation latency. See the ZooKeeper troubleshooting guide (org.apache.zookeeper.server.persistence.FileTxnLog)
[2018-03-02 22:40:11,189] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x130 zxid:0xe8 txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/1 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/1 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:11,221] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x135 zxid:0xeb txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/16 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/16 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-03-02 22:40:11,228] INFO Got user-level KeeperException when processing sessionid:0x161e798179f0000 type:create cxid:0x138 zxid:0xee txntype:-1 reqpath:n/a Error Path:/brokers/topics/__consumer_offsets/partitions/2 Error:KeeperErrorCode = NoNode for /brokers/topics/__consumer_offsets/partitions/2 (org.apache.zookeeper.server.PrepRequestProcessor)

--kafka broker server
[edureka@localhost ~]$ cd /usr/lib/kafka_2.12-0.11.0.0
[edureka@localhost kafka_2.12-0.11.0.0]$ bin/kafka-server-start.sh config/server.properties
[2018-03-02 22:13:42,978] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = false
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 0.11.0-IV2
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,TRACE:TRACE,SASL_SSL:SASL_SSL,PLAINTEXT:PLAINTEXT
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 0.11.0-IV2
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 9092
	principal.builder.class = class org.apache.kafka.common.security.auth.DefaultPrincipalBuilder
	producer.purgatory.purge.interval.requests = 1000
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2018-03-02 22:13:43,140] INFO starting (kafka.server.KafkaServer)
[2018-03-02 22:13:43,147] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2018-03-02 22:13:43,194] INFO Starting ZkClient event thread. (org.I0Itec.zkclient.ZkEventThread)
[2018-03-02 22:13:43,198] INFO Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT (org.apache.zookeeper.ZooKeeper)
[2018-03-02 22:13:43,199] INFO Client environment:host.name=localhost (org.apache.zookeeper.ZooKeeper)
[2018-03-02 22:13:43,199] INFO Client environment:java.version=1.8.0_144 (org.apache.zookeeper.ZooKeeper)
[2018-03-02 22:13:43,199] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2018-03-02 22:13:43,199] INFO Client environment:java.home=/usr/lib/jvm/jdk1.8.0_144/jre (org.apache.zookeeper.ZooKeeper)
[2018-03-02 22:13:43,199] INFO Client environment:java.class.path=:/FLUME_HOME/lib/*:/FLUME_HOME/lib/*:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/aopalliance-repackaged-2.5.0-b05.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/argparse4j-0.7.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/commons-lang3-3.5.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/connect-api-0.11.0.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/connect-file-0.11.0.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/connect-json-0.11.0.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/connect-runtime-0.11.0.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/connect-transforms-0.11.0.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/guava-20.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/hk2-api-2.5.0-b05.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/hk2-locator-2.5.0-b05.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/hk2-utils-2.5.0-b05.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jackson-annotations-2.8.5.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jackson-core-2.8.5.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jackson-databind-2.8.5.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jackson-jaxrs-base-2.8.5.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jackson-jaxrs-json-provider-2.8.5.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jackson-module-jaxb-annotations-2.8.5.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/javassist-3.21.0-GA.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/javax.annotation-api-1.2.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/javax.inject-1.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/javax.inject-2.5.0-b05.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/javax.servlet-api-3.1.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/javax.ws.rs-api-2.0.1.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jersey-client-2.24.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jersey-common-2.24.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jersey-container-servlet-2.24.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jersey-container-servlet-core-2.24.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jersey-guava-2.24.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jersey-media-jaxb-2.24.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jersey-server-2.24.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jetty-continuation-9.2.15.v20160210.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jetty-http-9.2.15.v20160210.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jetty-io-9.2.15.v20160210.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jetty-security-9.2.15.v20160210.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jetty-server-9.2.15.v20160210.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jetty-servlet-9.2.15.v20160210.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jetty-servlets-9.2.15.v20160210.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jetty-util-9.2.15.v20160210.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/jopt-simple-5.0.3.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/kafka_2.12-0.11.0.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/kafka_2.12-0.11.0.0-sources.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/kafka_2.12-0.11.0.0-test-sources.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/kafka-clients-0.11.0.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/kafka-log4j-appender-0.11.0.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/kafka-streams-0.11.0.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/kafka-streams-examples-0.11.0.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/kafka-tools-0.11.0.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/log4j-1.2.17.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/lz4-1.3.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/maven-artifact-3.5.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/metrics-core-2.2.0.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/osgi-resource-locator-1.0.1.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/plexus-utils-3.0.24.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/reflections-0.9.11.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/rocksdbjni-5.0.1.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/scala-library-2.12.2.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/scala-parser-combinators_2.12-1.0.4.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/slf4j-api-1.7.25.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/slf4j-log4j12-1.7.25.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/snappy-java-1.1.2.6.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/validation-api-1.1.0.Final.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/zkclient-0.10.jar:/usr/lib/kafka_2.12-0.11.0.0/bin/../libs/zookeeper-3.4.10.jar (org.apache.zookeeper.ZooKeeper)
[2018-03-02 22:13:43,200] INFO Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[2018-03-02 22:13:43,201] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[2018-03-02 22:13:43,201] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2018-03-02 22:13:43,201] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[2018-03-02 22:13:43,201] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2018-03-02 22:13:43,201] INFO Client environment:os.version=3.10.0-514.26.2.el7.x86_64 (org.apache.zookeeper.ZooKeeper)
[2018-03-02 22:13:43,201] INFO Client environment:user.name=edureka (org.apache.zookeeper.ZooKeeper)
[2018-03-02 22:13:43,201] INFO Client environment:user.home=/home/edureka (org.apache.zookeeper.ZooKeeper)
[2018-03-02 22:13:43,201] INFO Client environment:user.dir=/usr/lib/kafka_2.12-0.11.0.0 (org.apache.zookeeper.ZooKeeper)
[2018-03-02 22:13:43,203] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@5e316c74 (org.apache.zookeeper.ZooKeeper)
[2018-03-02 22:13:43,235] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-03-02 22:13:43,244] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2018-03-02 22:13:43,250] INFO Waiting for keeper state SyncConnected (org.I0Itec.zkclient.ZkClient)
[2018-03-02 22:13:43,300] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x161e798179f0000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2018-03-02 22:13:43,305] INFO zookeeper state changed (SyncConnected) (org.I0Itec.zkclient.ZkClient)
[2018-03-02 22:13:43,606] INFO Cluster ID = Fp2ChIY0R4i3QVAmk1NUpg (kafka.server.KafkaServer)
[2018-03-02 22:13:43,731] INFO [ThrottledRequestReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-03-02 22:13:43,734] INFO [ThrottledRequestReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-03-02 22:13:43,735] INFO [ThrottledRequestReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-03-02 22:13:43,885] INFO Loading logs. (kafka.log.LogManager)
[2018-03-02 22:13:44,035] INFO Loading producer state from offset 25 for partition kafka_topic-0 with message format version 2 (kafka.log.Log)
[2018-03-02 22:13:44,111] INFO Loading producer state from snapshot file 00000000000000000025.snapshot for partition kafka_topic-0 (kafka.log.ProducerStateManager)
[2018-03-02 22:13:44,167] INFO Completed load of log kafka_topic-0 with 1 log segments, log start offset 0 and log end offset 25 in 193 ms (kafka.log.Log)
[2018-03-02 22:13:44,181] INFO Logs loading complete in 296 ms. (kafka.log.LogManager)
[2018-03-02 22:13:44,385] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2018-03-02 22:13:44,389] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2018-03-02 22:13:44,509] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2018-03-02 22:13:44,514] INFO [Socket Server on Broker 0], Started 1 acceptor threads (kafka.network.SocketServer)
[2018-03-02 22:13:44,548] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-03-02 22:13:44,551] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-03-02 22:13:44,558] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-03-02 22:13:44,685] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-03-02 22:13:44,695] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-03-02 22:13:44,699] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-03-02 22:13:44,739] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2018-03-02 22:13:44,741] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2018-03-02 22:13:44,752] INFO [Group Metadata Manager on Broker 0]: Removed 0 expired offsets in 4 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:13:44,759] INFO Creating /controller (is it secure? false) (kafka.utils.ZKCheckedEphemeral)
[2018-03-02 22:13:44,793] INFO Result of znode creation is: OK (kafka.utils.ZKCheckedEphemeral)
[2018-03-02 22:13:44,858] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:1000,blockEndProducerId:1999) by writing to Zk with path version 2 (kafka.coordinator.transaction.ProducerIdManager)
[2018-03-02 22:13:45,007] INFO [Transaction Coordinator 0]: Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2018-03-02 22:13:45,009] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2018-03-02 22:13:45,022] INFO [Transaction Coordinator 0]: Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2018-03-02 22:13:45,183] INFO Will not load MX4J, mx4j-tools.jar is not in the classpath (kafka.utils.Mx4jLoader$)
[2018-03-02 22:13:45,307] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.utils.ZKCheckedEphemeral)
[2018-03-02 22:13:45,323] INFO Result of znode creation is: OK (kafka.utils.ZKCheckedEphemeral)
[2018-03-02 22:13:45,325] INFO Registered broker 0 at path /brokers/ids/0 with addresses: EndPoint(localhost,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.utils.ZkUtils)
[2018-03-02 22:13:45,335] INFO Kafka version : 0.11.0.0 (org.apache.kafka.common.utils.AppInfoParser)
[2018-03-02 22:13:45,335] INFO Kafka commitId : cb8625948210849f (org.apache.kafka.common.utils.AppInfoParser)
[2018-03-02 22:13:45,337] INFO [Kafka Server 0], started (kafka.server.KafkaServer)
[2018-03-02 22:13:45,919] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions kafka_topic-0 (kafka.server.ReplicaFetcherManager)
[2018-03-02 22:13:45,978] INFO Partition [kafka_topic,0] on broker 0: kafka_topic-0 starts at Leader Epoch 0 from offset 25. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:13:46,028] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions kafka_topic-0 (kafka.server.ReplicaFetcherManager)
[2018-03-02 22:13:46,045] INFO Partition [kafka_topic,0] on broker 0: kafka_topic-0 starts at Leader Epoch 1 from offset 25. Previous Leader Epoch was: 0 (kafka.cluster.Partition)
[2018-03-02 22:23:44,740] INFO [Group Metadata Manager on Broker 0]: Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:33:44,742] INFO [Group Metadata Manager on Broker 0]: Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:08,715] INFO Topic creation {"version":1,"partitions":{"45":[0],"34":[0],"12":[0],"8":[0],"19":[0],"23":[0],"4":[0],"40":[0],"15":[0],"11":[0],"9":[0],"44":[0],"33":[0],"22":[0],"26":[0],"37":[0],"13":[0],"46":[0],"24":[0],"35":[0],"16":[0],"5":[0],"10":[0],"48":[0],"21":[0],"43":[0],"32":[0],"49":[0],"6":[0],"36":[0],"1":[0],"39":[0],"17":[0],"25":[0],"14":[0],"47":[0],"31":[0],"42":[0],"0":[0],"20":[0],"27":[0],"2":[0],"38":[0],"18":[0],"30":[0],"7":[0],"29":[0],"41":[0],"3":[0],"28":[0]}} (kafka.admin.AdminUtils$)
[2018-03-02 22:40:08,721] INFO [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2018-03-02 22:40:11,428] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.server.ReplicaFetcherManager)
[2018-03-02 22:40:11,438] INFO Loading producer state from offset 0 for partition __consumer_offsets-0 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,442] INFO Completed load of log __consumer_offsets-0 with 1 log segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2018-03-02 22:40:11,445] INFO Created log for partition [__consumer_offsets,0] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,446] INFO Partition [__consumer_offsets,0] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2018-03-02 22:40:11,447] INFO Partition [__consumer_offsets,0] on broker 0: __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,456] INFO Loading producer state from offset 0 for partition __consumer_offsets-29 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,456] INFO Completed load of log __consumer_offsets-29 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-03-02 22:40:11,459] INFO Created log for partition [__consumer_offsets,29] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,459] INFO Partition [__consumer_offsets,29] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2018-03-02 22:40:11,460] INFO Partition [__consumer_offsets,29] on broker 0: __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,468] INFO Loading producer state from offset 0 for partition __consumer_offsets-48 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,469] INFO Completed load of log __consumer_offsets-48 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:11,470] INFO Created log for partition [__consumer_offsets,48] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,471] INFO Partition [__consumer_offsets,48] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2018-03-02 22:40:11,471] INFO Partition [__consumer_offsets,48] on broker 0: __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,488] INFO Loading producer state from offset 0 for partition __consumer_offsets-10 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,489] INFO Completed load of log __consumer_offsets-10 with 1 log segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-03-02 22:40:11,491] INFO Created log for partition [__consumer_offsets,10] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,492] INFO Partition [__consumer_offsets,10] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2018-03-02 22:40:11,492] INFO Partition [__consumer_offsets,10] on broker 0: __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,516] INFO Loading producer state from offset 0 for partition __consumer_offsets-45 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,517] INFO Completed load of log __consumer_offsets-45 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:11,520] INFO Created log for partition [__consumer_offsets,45] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,535] INFO Partition [__consumer_offsets,45] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2018-03-02 22:40:11,535] INFO Partition [__consumer_offsets,45] on broker 0: __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,541] INFO Loading producer state from offset 0 for partition __consumer_offsets-26 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,542] INFO Completed load of log __consumer_offsets-26 with 1 log segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-03-02 22:40:11,543] INFO Created log for partition [__consumer_offsets,26] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,544] INFO Partition [__consumer_offsets,26] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2018-03-02 22:40:11,544] INFO Partition [__consumer_offsets,26] on broker 0: __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,554] INFO Loading producer state from offset 0 for partition __consumer_offsets-7 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,554] INFO Completed load of log __consumer_offsets-7 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-03-02 22:40:11,556] INFO Created log for partition [__consumer_offsets,7] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,556] INFO Partition [__consumer_offsets,7] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2018-03-02 22:40:11,557] INFO Partition [__consumer_offsets,7] on broker 0: __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,562] INFO Loading producer state from offset 0 for partition __consumer_offsets-42 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,563] INFO Completed load of log __consumer_offsets-42 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:11,565] INFO Created log for partition [__consumer_offsets,42] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,565] INFO Partition [__consumer_offsets,42] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2018-03-02 22:40:11,565] INFO Partition [__consumer_offsets,42] on broker 0: __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,580] INFO Loading producer state from offset 0 for partition __consumer_offsets-4 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,581] INFO Completed load of log __consumer_offsets-4 with 1 log segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-03-02 22:40:11,583] INFO Created log for partition [__consumer_offsets,4] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,583] INFO Partition [__consumer_offsets,4] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2018-03-02 22:40:11,583] INFO Partition [__consumer_offsets,4] on broker 0: __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,591] INFO Loading producer state from offset 0 for partition __consumer_offsets-23 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,592] INFO Completed load of log __consumer_offsets-23 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:11,593] INFO Created log for partition [__consumer_offsets,23] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,594] INFO Partition [__consumer_offsets,23] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2018-03-02 22:40:11,594] INFO Partition [__consumer_offsets,23] on broker 0: __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,613] INFO Loading producer state from offset 0 for partition __consumer_offsets-1 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,614] INFO Completed load of log __consumer_offsets-1 with 1 log segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2018-03-02 22:40:11,616] INFO Created log for partition [__consumer_offsets,1] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,616] INFO Partition [__consumer_offsets,1] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,616] INFO Partition [__consumer_offsets,1] on broker 0: __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,627] INFO Loading producer state from offset 0 for partition __consumer_offsets-20 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,628] INFO Completed load of log __consumer_offsets-20 with 1 log segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-03-02 22:40:11,629] INFO Created log for partition [__consumer_offsets,20] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,630] INFO Partition [__consumer_offsets,20] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2018-03-02 22:40:11,630] INFO Partition [__consumer_offsets,20] on broker 0: __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,637] INFO Loading producer state from offset 0 for partition __consumer_offsets-39 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,638] INFO Completed load of log __consumer_offsets-39 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:11,640] INFO Created log for partition [__consumer_offsets,39] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,640] INFO Partition [__consumer_offsets,39] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2018-03-02 22:40:11,640] INFO Partition [__consumer_offsets,39] on broker 0: __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,647] INFO Loading producer state from offset 0 for partition __consumer_offsets-17 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,648] INFO Completed load of log __consumer_offsets-17 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:11,650] INFO Created log for partition [__consumer_offsets,17] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,650] INFO Partition [__consumer_offsets,17] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2018-03-02 22:40:11,651] INFO Partition [__consumer_offsets,17] on broker 0: __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,667] INFO Loading producer state from offset 0 for partition __consumer_offsets-36 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,668] INFO Completed load of log __consumer_offsets-36 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:11,669] INFO Created log for partition [__consumer_offsets,36] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,670] INFO Partition [__consumer_offsets,36] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2018-03-02 22:40:11,670] INFO Partition [__consumer_offsets,36] on broker 0: __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,678] INFO Loading producer state from offset 0 for partition __consumer_offsets-14 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,679] INFO Completed load of log __consumer_offsets-14 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:11,681] INFO Created log for partition [__consumer_offsets,14] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,681] INFO Partition [__consumer_offsets,14] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2018-03-02 22:40:11,682] INFO Partition [__consumer_offsets,14] on broker 0: __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,688] INFO Loading producer state from offset 0 for partition __consumer_offsets-33 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,688] INFO Completed load of log __consumer_offsets-33 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-03-02 22:40:11,690] INFO Created log for partition [__consumer_offsets,33] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,690] INFO Partition [__consumer_offsets,33] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2018-03-02 22:40:11,691] INFO Partition [__consumer_offsets,33] on broker 0: __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,719] INFO Loading producer state from offset 0 for partition __consumer_offsets-49 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,719] INFO Completed load of log __consumer_offsets-49 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:11,721] INFO Created log for partition [__consumer_offsets,49] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,722] INFO Partition [__consumer_offsets,49] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2018-03-02 22:40:11,722] INFO Partition [__consumer_offsets,49] on broker 0: __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,745] INFO Loading producer state from offset 0 for partition __consumer_offsets-11 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,746] INFO Completed load of log __consumer_offsets-11 with 1 log segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-03-02 22:40:11,748] INFO Created log for partition [__consumer_offsets,11] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,748] INFO Partition [__consumer_offsets,11] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2018-03-02 22:40:11,749] INFO Partition [__consumer_offsets,11] on broker 0: __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,756] INFO Loading producer state from offset 0 for partition __consumer_offsets-30 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,757] INFO Completed load of log __consumer_offsets-30 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:11,759] INFO Created log for partition [__consumer_offsets,30] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,759] INFO Partition [__consumer_offsets,30] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2018-03-02 22:40:11,759] INFO Partition [__consumer_offsets,30] on broker 0: __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,766] INFO Loading producer state from offset 0 for partition __consumer_offsets-46 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,767] INFO Completed load of log __consumer_offsets-46 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:11,768] INFO Created log for partition [__consumer_offsets,46] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,768] INFO Partition [__consumer_offsets,46] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2018-03-02 22:40:11,769] INFO Partition [__consumer_offsets,46] on broker 0: __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,780] INFO Loading producer state from offset 0 for partition __consumer_offsets-27 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,781] INFO Completed load of log __consumer_offsets-27 with 1 log segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[2018-03-02 22:40:11,782] INFO Created log for partition [__consumer_offsets,27] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,783] INFO Partition [__consumer_offsets,27] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2018-03-02 22:40:11,783] INFO Partition [__consumer_offsets,27] on broker 0: __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,789] INFO Loading producer state from offset 0 for partition __consumer_offsets-8 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,790] INFO Completed load of log __consumer_offsets-8 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:11,791] INFO Created log for partition [__consumer_offsets,8] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,792] INFO Partition [__consumer_offsets,8] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2018-03-02 22:40:11,792] INFO Partition [__consumer_offsets,8] on broker 0: __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,799] INFO Loading producer state from offset 0 for partition __consumer_offsets-24 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,800] INFO Completed load of log __consumer_offsets-24 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:11,802] INFO Created log for partition [__consumer_offsets,24] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,802] INFO Partition [__consumer_offsets,24] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2018-03-02 22:40:11,803] INFO Partition [__consumer_offsets,24] on broker 0: __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,831] INFO Loading producer state from offset 0 for partition __consumer_offsets-43 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,832] INFO Completed load of log __consumer_offsets-43 with 1 log segments, log start offset 0 and log end offset 0 in 24 ms (kafka.log.Log)
[2018-03-02 22:40:11,834] INFO Created log for partition [__consumer_offsets,43] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,834] INFO Partition [__consumer_offsets,43] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2018-03-02 22:40:11,835] INFO Partition [__consumer_offsets,43] on broker 0: __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,845] INFO Loading producer state from offset 0 for partition __consumer_offsets-5 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,847] INFO Completed load of log __consumer_offsets-5 with 1 log segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2018-03-02 22:40:11,848] INFO Created log for partition [__consumer_offsets,5] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,848] INFO Partition [__consumer_offsets,5] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2018-03-02 22:40:11,849] INFO Partition [__consumer_offsets,5] on broker 0: __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,861] INFO Loading producer state from offset 0 for partition __consumer_offsets-21 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,862] INFO Completed load of log __consumer_offsets-21 with 1 log segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2018-03-02 22:40:11,864] INFO Created log for partition [__consumer_offsets,21] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,865] INFO Partition [__consumer_offsets,21] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2018-03-02 22:40:11,865] INFO Partition [__consumer_offsets,21] on broker 0: __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,886] INFO Loading producer state from offset 0 for partition __consumer_offsets-2 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,887] INFO Completed load of log __consumer_offsets-2 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:11,888] INFO Created log for partition [__consumer_offsets,2] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,889] INFO Partition [__consumer_offsets,2] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2018-03-02 22:40:11,889] INFO Partition [__consumer_offsets,2] on broker 0: __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,897] INFO Loading producer state from offset 0 for partition __consumer_offsets-40 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,897] INFO Completed load of log __consumer_offsets-40 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:11,899] INFO Created log for partition [__consumer_offsets,40] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,899] INFO Partition [__consumer_offsets,40] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2018-03-02 22:40:11,899] INFO Partition [__consumer_offsets,40] on broker 0: __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,914] INFO Loading producer state from offset 0 for partition __consumer_offsets-37 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,915] INFO Completed load of log __consumer_offsets-37 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:11,917] INFO Created log for partition [__consumer_offsets,37] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,917] INFO Partition [__consumer_offsets,37] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2018-03-02 22:40:11,917] INFO Partition [__consumer_offsets,37] on broker 0: __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,942] INFO Loading producer state from offset 0 for partition __consumer_offsets-18 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,943] INFO Completed load of log __consumer_offsets-18 with 1 log segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-03-02 22:40:11,944] INFO Created log for partition [__consumer_offsets,18] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,945] INFO Partition [__consumer_offsets,18] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2018-03-02 22:40:11,945] INFO Partition [__consumer_offsets,18] on broker 0: __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,958] INFO Loading producer state from offset 0 for partition __consumer_offsets-34 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,959] INFO Completed load of log __consumer_offsets-34 with 1 log segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-03-02 22:40:11,960] INFO Created log for partition [__consumer_offsets,34] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,961] INFO Partition [__consumer_offsets,34] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2018-03-02 22:40:11,961] INFO Partition [__consumer_offsets,34] on broker 0: __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:11,984] INFO Loading producer state from offset 0 for partition __consumer_offsets-15 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:11,985] INFO Completed load of log __consumer_offsets-15 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:11,986] INFO Created log for partition [__consumer_offsets,15] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:11,986] INFO Partition [__consumer_offsets,15] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2018-03-02 22:40:11,986] INFO Partition [__consumer_offsets,15] on broker 0: __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:12,019] INFO Loading producer state from offset 0 for partition __consumer_offsets-12 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:12,020] INFO Completed load of log __consumer_offsets-12 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:12,021] INFO Created log for partition [__consumer_offsets,12] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:12,022] INFO Partition [__consumer_offsets,12] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2018-03-02 22:40:12,022] INFO Partition [__consumer_offsets,12] on broker 0: __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:12,032] INFO Loading producer state from offset 0 for partition __consumer_offsets-31 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:12,033] INFO Completed load of log __consumer_offsets-31 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:12,034] INFO Created log for partition [__consumer_offsets,31] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:12,035] INFO Partition [__consumer_offsets,31] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2018-03-02 22:40:12,035] INFO Partition [__consumer_offsets,31] on broker 0: __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:12,042] INFO Loading producer state from offset 0 for partition __consumer_offsets-9 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:12,043] INFO Completed load of log __consumer_offsets-9 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:12,044] INFO Created log for partition [__consumer_offsets,9] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:12,044] INFO Partition [__consumer_offsets,9] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2018-03-02 22:40:12,044] INFO Partition [__consumer_offsets,9] on broker 0: __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:12,059] INFO Loading producer state from offset 0 for partition __consumer_offsets-47 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:12,060] INFO Completed load of log __consumer_offsets-47 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:12,062] INFO Created log for partition [__consumer_offsets,47] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:12,062] INFO Partition [__consumer_offsets,47] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2018-03-02 22:40:12,063] INFO Partition [__consumer_offsets,47] on broker 0: __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:12,081] INFO Loading producer state from offset 0 for partition __consumer_offsets-19 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:12,082] INFO Completed load of log __consumer_offsets-19 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-03-02 22:40:12,083] INFO Created log for partition [__consumer_offsets,19] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:12,083] INFO Partition [__consumer_offsets,19] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2018-03-02 22:40:12,083] INFO Partition [__consumer_offsets,19] on broker 0: __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:12,095] INFO Loading producer state from offset 0 for partition __consumer_offsets-28 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:12,096] INFO Completed load of log __consumer_offsets-28 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:12,097] INFO Created log for partition [__consumer_offsets,28] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:12,097] INFO Partition [__consumer_offsets,28] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2018-03-02 22:40:12,097] INFO Partition [__consumer_offsets,28] on broker 0: __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:12,103] INFO Loading producer state from offset 0 for partition __consumer_offsets-38 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:12,104] INFO Completed load of log __consumer_offsets-38 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:12,106] INFO Created log for partition [__consumer_offsets,38] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:12,106] INFO Partition [__consumer_offsets,38] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2018-03-02 22:40:12,106] INFO Partition [__consumer_offsets,38] on broker 0: __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:12,112] INFO Loading producer state from offset 0 for partition __consumer_offsets-35 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:12,113] INFO Completed load of log __consumer_offsets-35 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-03-02 22:40:12,114] INFO Created log for partition [__consumer_offsets,35] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:12,114] INFO Partition [__consumer_offsets,35] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2018-03-02 22:40:12,114] INFO Partition [__consumer_offsets,35] on broker 0: __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:12,132] INFO Loading producer state from offset 0 for partition __consumer_offsets-44 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:12,133] INFO Completed load of log __consumer_offsets-44 with 1 log segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[2018-03-02 22:40:12,135] INFO Created log for partition [__consumer_offsets,44] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:12,135] INFO Partition [__consumer_offsets,44] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2018-03-02 22:40:12,136] INFO Partition [__consumer_offsets,44] on broker 0: __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:12,142] INFO Loading producer state from offset 0 for partition __consumer_offsets-6 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:12,142] INFO Completed load of log __consumer_offsets-6 with 1 log segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-03-02 22:40:12,143] INFO Created log for partition [__consumer_offsets,6] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:12,144] INFO Partition [__consumer_offsets,6] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2018-03-02 22:40:12,144] INFO Partition [__consumer_offsets,6] on broker 0: __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:12,149] INFO Loading producer state from offset 0 for partition __consumer_offsets-25 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:12,150] INFO Completed load of log __consumer_offsets-25 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:12,151] INFO Created log for partition [__consumer_offsets,25] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:12,151] INFO Partition [__consumer_offsets,25] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2018-03-02 22:40:12,152] INFO Partition [__consumer_offsets,25] on broker 0: __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:12,171] INFO Loading producer state from offset 0 for partition __consumer_offsets-16 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:12,172] INFO Completed load of log __consumer_offsets-16 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:12,174] INFO Created log for partition [__consumer_offsets,16] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:12,175] INFO Partition [__consumer_offsets,16] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2018-03-02 22:40:12,175] INFO Partition [__consumer_offsets,16] on broker 0: __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:12,214] INFO Loading producer state from offset 0 for partition __consumer_offsets-22 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:12,214] INFO Completed load of log __consumer_offsets-22 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:12,216] INFO Created log for partition [__consumer_offsets,22] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:12,216] INFO Partition [__consumer_offsets,22] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2018-03-02 22:40:12,217] INFO Partition [__consumer_offsets,22] on broker 0: __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:12,226] INFO Loading producer state from offset 0 for partition __consumer_offsets-41 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:12,227] INFO Completed load of log __consumer_offsets-41 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:12,228] INFO Created log for partition [__consumer_offsets,41] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:12,229] INFO Partition [__consumer_offsets,41] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2018-03-02 22:40:12,229] INFO Partition [__consumer_offsets,41] on broker 0: __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:12,252] INFO Loading producer state from offset 0 for partition __consumer_offsets-32 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:12,253] INFO Completed load of log __consumer_offsets-32 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:12,254] INFO Created log for partition [__consumer_offsets,32] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:12,254] INFO Partition [__consumer_offsets,32] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2018-03-02 22:40:12,255] INFO Partition [__consumer_offsets,32] on broker 0: __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:12,263] INFO Loading producer state from offset 0 for partition __consumer_offsets-3 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:12,264] INFO Completed load of log __consumer_offsets-3 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:12,265] INFO Created log for partition [__consumer_offsets,3] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:12,266] INFO Partition [__consumer_offsets,3] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2018-03-02 22:40:12,266] INFO Partition [__consumer_offsets,3] on broker 0: __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:12,324] INFO Loading producer state from offset 0 for partition __consumer_offsets-13 with message format version 2 (kafka.log.Log)
[2018-03-02 22:40:12,324] INFO Completed load of log __consumer_offsets-13 with 1 log segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-03-02 22:40:12,326] INFO Created log for partition [__consumer_offsets,13] in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 0.11.0-IV2, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-03-02 22:40:12,327] INFO Partition [__consumer_offsets,13] on broker 0: No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2018-03-02 22:40:12,327] INFO Partition [__consumer_offsets,13] on broker 0: __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-03-02 22:40:12,348] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,350] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,350] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,350] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,350] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,350] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,350] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,350] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,350] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,351] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,351] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,351] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,351] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,351] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,351] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,351] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,351] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,351] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,351] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,351] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,352] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,352] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,352] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,352] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,352] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,352] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,352] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,352] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,352] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,352] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,352] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,352] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,363] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,363] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,363] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,364] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,364] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,364] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,364] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,364] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,364] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,364] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,364] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,364] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,364] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,364] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,365] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,365] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,365] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,365] INFO [Group Metadata Manager on Broker 0]: Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,369] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-22 in 10 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,387] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,388] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,388] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,388] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,388] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,388] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,389] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,389] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,389] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,389] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,389] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,390] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-47 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,390] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,390] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,390] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,390] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,391] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,391] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,391] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,391] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,391] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,392] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,392] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,392] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,392] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,392] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,392] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,393] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,393] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,393] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,393] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,395] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,395] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,396] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,396] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,396] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,397] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-12 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,397] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,397] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,397] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,398] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,398] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,398] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,399] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-33 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,399] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,399] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,399] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,400] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,400] INFO [Group Metadata Manager on Broker 0]: Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:40:12,508] INFO [GroupCoordinator 0]: Preparing to rebalance group use_a_separate_group_id_for_each_stream with old generation 0 (__consumer_offsets-10) (kafka.coordinator.group.GroupCoordinator)
[2018-03-02 22:40:12,542] INFO [GroupCoordinator 0]: Stabilized group use_a_separate_group_id_for_each_stream generation 1 (__consumer_offsets-10) (kafka.coordinator.group.GroupCoordinator)
[2018-03-02 22:40:12,576] INFO [GroupCoordinator 0]: Assignment received from leader for group use_a_separate_group_id_for_each_stream for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2018-03-02 22:40:12,634] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset-1} for Partition: __consumer_offsets-10. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2018-03-02 22:42:06,482] INFO Updated PartitionLeaderEpoch. New: {epoch:1, offset:25}, Current: {epoch:0, offset0} for Partition: kafka_topic-0. Cache now contains 1 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2018-03-02 22:42:18,271] INFO [GroupCoordinator 0]: Preparing to rebalance group use_a_separate_group_id_for_each_stream with old generation 1 (__consumer_offsets-10) (kafka.coordinator.group.GroupCoordinator)
[2018-03-02 22:42:18,273] INFO [GroupCoordinator 0]: Group use_a_separate_group_id_for_each_stream with generation 2 is now empty (__consumer_offsets-10) (kafka.coordinator.group.GroupCoordinator)
[2018-03-02 22:43:44,742] INFO [Group Metadata Manager on Broker 0]: Group use_a_separate_group_id_for_each_stream transitioned to Dead in generation 2 (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:43:44,747] INFO [Group Metadata Manager on Broker 0]: Removed 0 expired offsets in 7 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 22:53:44,740] INFO [Group Metadata Manager on Broker 0]: Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 23:03:44,743] INFO [Group Metadata Manager on Broker 0]: Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-03-02 23:06:55,449] INFO [GroupCoordinator 0]: Preparing to rebalance group use_a_separate_group_id_for_each_stream with old generation 0 (__consumer_offsets-10) (kafka.coordinator.group.GroupCoordinator)
[2018-03-02 23:06:55,453] INFO [GroupCoordinator 0]: Stabilized group use_a_separate_group_id_for_each_stream generation 1 (__consumer_offsets-10) (kafka.coordinator.group.GroupCoordinator)
[2018-03-02 23:06:55,462] INFO [GroupCoordinator 0]: Assignment received from leader for group use_a_separate_group_id_for_each_stream for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2018-03-02 23:07:02,628] INFO [GroupCoordinator 0]: Preparing to rebalance group use_a_separate_group_id_for_each_stream with old generation 1 (__consumer_offsets-10) (kafka.coordinator.group.GroupCoordinator)
[2018-03-02 23:07:02,629] INFO [GroupCoordinator 0]: Group use_a_separate_group_id_for_each_stream with generation 2 is now empty (__consumer_offsets-10) (kafka.coordinator.group.GroupCoordinator)

---kafka consumer
[edureka@localhost ~]$ cd SHAHBAZWS/BATCH170917/sparkjobs/
[edureka@localhost sparkjobs]$ spark-submit --verbose --master local --deploy-mode client --conf spark.driver.extraClassPath=/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/mysql-connector-java-5.1.43-bin.jar:/usr/lib/kafka_2.12-0.11.0.0/libs/kafka-clients-0.11.0.0.jar --repositories https://repo.maven.apache.org/maven2 --packages org.apache.spark:spark-streaming-kafka-0-10_2.11:2.0.0 --class com.laboros.spark.sql.stream.SparkStreamingConsumer sparkdemo_2.11-0.1.0-SNAPSHOT.jar kafka_topic
Using properties file: /usr/lib/spark-2.1.1-bin-hadoop2.7/conf/spark-defaults.conf
Adding default property: spark.master=spark://localhost:7077
Parsed arguments:
  master                  local
  deployMode              client
  executorMemory          null
  executorCores           null
  totalExecutorCores      null
  propertiesFile          /usr/lib/spark-2.1.1-bin-hadoop2.7/conf/spark-defaults.conf
  driverMemory            null
  driverCores             null
  driverExtraClassPath    /home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/mysql-connector-java-5.1.43-bin.jar:/usr/lib/kafka_2.12-0.11.0.0/libs/kafka-clients-0.11.0.0.jar
  driverExtraLibraryPath  null
  driverExtraJavaOptions  null
  supervise               false
  queue                   null
  numExecutors            null
  files                   null
  pyFiles                 null
  archives                null
  mainClass               com.laboros.spark.sql.stream.SparkStreamingConsumer
  primaryResource         file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar
  name                    com.laboros.spark.sql.stream.SparkStreamingConsumer
  childArgs               [kafka_topic]
  jars                    null
  packages                org.apache.spark:spark-streaming-kafka-0-10_2.11:2.0.0
  packagesExclusions      null
  repositories            https://repo.maven.apache.org/maven2
  verbose                 true

Spark properties used, including those specified through
 --conf and those from the properties file /usr/lib/spark-2.1.1-bin-hadoop2.7/conf/spark-defaults.conf:
  spark.master -> spark://localhost:7077
  spark.driver.extraClassPath -> /home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/mysql-connector-java-5.1.43-bin.jar:/usr/lib/kafka_2.12-0.11.0.0/libs/kafka-clients-0.11.0.0.jar

    
Ivy Default Cache set to: /home/edureka/.ivy2/cache
The jars for the packages stored in: /home/edureka/.ivy2/jars
https://repo.maven.apache.org/maven2 added as a remote repository with the name: repo-1
:: loading settings :: url = jar:file:/usr/lib/spark-2.1.1-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
org.apache.spark#spark-streaming-kafka-0-10_2.11 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
	confs: [default]
	found org.apache.spark#spark-streaming-kafka-0-10_2.11;2.0.0 in repo-1
	found org.apache.kafka#kafka_2.11;0.10.0.0 in repo-1
	found com.101tec#zkclient;0.8 in repo-1
	found org.slf4j#slf4j-api;1.7.16 in repo-1
	found org.slf4j#slf4j-log4j12;1.7.16 in repo-1
	found log4j#log4j;1.2.17 in repo-1
	found com.yammer.metrics#metrics-core;2.2.0 in repo-1
	found org.scala-lang.modules#scala-parser-combinators_2.11;1.0.4 in repo-1
	found org.apache.kafka#kafka-clients;0.10.0.0 in repo-1
	found net.jpountz.lz4#lz4;1.3.0 in repo-1
	found org.xerial.snappy#snappy-java;1.1.2.4 in repo-1
	found org.apache.spark#spark-tags_2.11;2.0.0 in repo-1
	found org.scalatest#scalatest_2.11;2.2.6 in repo-1
	found org.scala-lang#scala-reflect;2.11.8 in repo-1
	found org.scala-lang.modules#scala-xml_2.11;1.0.2 in repo-1
	found org.spark-project.spark#unused;1.0.0 in repo-1
downloading https://repo.maven.apache.org/maven2/org/apache/spark/spark-streaming-kafka-0-10_2.11/2.0.0/spark-streaming-kafka-0-10_2.11-2.0.0.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-streaming-kafka-0-10_2.11;2.0.0!spark-streaming-kafka-0-10_2.11.jar (11414ms)
downloading https://repo.maven.apache.org/maven2/org/apache/kafka/kafka_2.11/0.10.0.0/kafka_2.11-0.10.0.0.jar ...
	[SUCCESSFUL ] org.apache.kafka#kafka_2.11;0.10.0.0!kafka_2.11.jar (209112ms)
downloading https://repo.maven.apache.org/maven2/org/apache/spark/spark-tags_2.11/2.0.0/spark-tags_2.11-2.0.0.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-tags_2.11;2.0.0!spark-tags_2.11.jar (515ms)
downloading https://repo.maven.apache.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (293ms)
downloading https://repo.maven.apache.org/maven2/com/101tec/zkclient/0.8/zkclient-0.8.jar ...
	[SUCCESSFUL ] com.101tec#zkclient;0.8!zkclient.jar (1706ms)
downloading https://repo.maven.apache.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar ...
	[SUCCESSFUL ] org.slf4j#slf4j-log4j12;1.7.16!slf4j-log4j12.jar (306ms)
downloading https://repo.maven.apache.org/maven2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar ...
	[SUCCESSFUL ] com.yammer.metrics#metrics-core;2.2.0!metrics-core.jar (1714ms)
downloading https://repo.maven.apache.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar ...
	[SUCCESSFUL ] org.scala-lang.modules#scala-parser-combinators_2.11;1.0.4!scala-parser-combinators_2.11.jar(bundle) (12218ms)
downloading https://repo.maven.apache.org/maven2/org/apache/kafka/kafka-clients/0.10.0.0/kafka-clients-0.10.0.0.jar ...
	[SUCCESSFUL ] org.apache.kafka#kafka-clients;0.10.0.0!kafka-clients.jar (32339ms)
downloading https://repo.maven.apache.org/maven2/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar ...
	[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.16!slf4j-api.jar (1432ms)
downloading https://repo.maven.apache.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.jar ...
	[SUCCESSFUL ] log4j#log4j;1.2.17!log4j.jar(bundle) (19297ms)
downloading https://repo.maven.apache.org/maven2/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar ...
	[SUCCESSFUL ] net.jpountz.lz4#lz4;1.3.0!lz4.jar (7001ms)
downloading https://repo.maven.apache.org/maven2/org/xerial/snappy/snappy-java/1.1.2.4/snappy-java-1.1.2.4.jar ...
	[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.2.4!snappy-java.jar(bundle) (26808ms)
downloading https://repo.maven.apache.org/maven2/org/scalatest/scalatest_2.11/2.2.6/scalatest_2.11-2.2.6.jar ...
	[SUCCESSFUL ] org.scalatest#scalatest_2.11;2.2.6!scalatest_2.11.jar(bundle) (216317ms)
downloading https://repo.maven.apache.org/maven2/org/scala-lang/scala-reflect/2.11.8/scala-reflect-2.11.8.jar ...
	[SUCCESSFUL ] org.scala-lang#scala-reflect;2.11.8!scala-reflect.jar (123027ms)
downloading https://repo.maven.apache.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.2/scala-xml_2.11-1.0.2.jar ...
	[SUCCESSFUL ] org.scala-lang.modules#scala-xml_2.11;1.0.2!scala-xml_2.11.jar(bundle) (22077ms)
:: resolution report :: resolve 47002ms :: artifacts dl 685619ms
	:: modules in use:
	com.101tec#zkclient;0.8 from repo-1 in [default]
	com.yammer.metrics#metrics-core;2.2.0 from repo-1 in [default]
	log4j#log4j;1.2.17 from repo-1 in [default]
	net.jpountz.lz4#lz4;1.3.0 from repo-1 in [default]
	org.apache.kafka#kafka-clients;0.10.0.0 from repo-1 in [default]
	org.apache.kafka#kafka_2.11;0.10.0.0 from repo-1 in [default]
	org.apache.spark#spark-streaming-kafka-0-10_2.11;2.0.0 from repo-1 in [default]
	org.apache.spark#spark-tags_2.11;2.0.0 from repo-1 in [default]
	org.scala-lang#scala-reflect;2.11.8 from repo-1 in [default]
	org.scala-lang.modules#scala-parser-combinators_2.11;1.0.4 from repo-1 in [default]
	org.scala-lang.modules#scala-xml_2.11;1.0.2 from repo-1 in [default]
	org.scalatest#scalatest_2.11;2.2.6 from repo-1 in [default]
	org.slf4j#slf4j-api;1.7.16 from repo-1 in [default]
	org.slf4j#slf4j-log4j12;1.7.16 from repo-1 in [default]
	org.spark-project.spark#unused;1.0.0 from repo-1 in [default]
	org.xerial.snappy#snappy-java;1.1.2.4 from repo-1 in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   16  |   16  |   16  |   0   ||   16  |   16  |
	---------------------------------------------------------------------

:: problems summary ::
:::: ERRORS
	unknown resolver sbt-chain

	unknown resolver null


:: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS
:: retrieving :: org.apache.spark#spark-submit-parent
	confs: [default]
	16 artifacts copied, 0 already retrieved (20689kB/95ms)
Main class:
com.laboros.spark.sql.stream.SparkStreamingConsumer
Arguments:
kafka_topic
System properties:
SPARK_SUBMIT -> true
spark.app.name -> com.laboros.spark.sql.stream.SparkStreamingConsumer
spark.jars -> file:/home/edureka/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-10_2.11-2.0.0.jar,file:/home/edureka/.ivy2/jars/org.apache.kafka_kafka_2.11-0.10.0.0.jar,file:/home/edureka/.ivy2/jars/org.apache.spark_spark-tags_2.11-2.0.0.jar,file:/home/edureka/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:/home/edureka/.ivy2/jars/com.101tec_zkclient-0.8.jar,file:/home/edureka/.ivy2/jars/org.slf4j_slf4j-log4j12-1.7.16.jar,file:/home/edureka/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar,file:/home/edureka/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.0.4.jar,file:/home/edureka/.ivy2/jars/org.apache.kafka_kafka-clients-0.10.0.0.jar,file:/home/edureka/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar,file:/home/edureka/.ivy2/jars/log4j_log4j-1.2.17.jar,file:/home/edureka/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar,file:/home/edureka/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.2.4.jar,file:/home/edureka/.ivy2/jars/org.scalatest_scalatest_2.11-2.2.6.jar,file:/home/edureka/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar,file:/home/edureka/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar,file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar
spark.submit.deployMode -> client
spark.master -> local
spark.driver.extraClassPath -> /home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/mysql-connector-java-5.1.43-bin.jar:/usr/lib/kafka_2.12-0.11.0.0/libs/kafka-clients-0.11.0.0.jar
Classpath elements:
file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar
/home/edureka/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-10_2.11-2.0.0.jar
/home/edureka/.ivy2/jars/org.apache.kafka_kafka_2.11-0.10.0.0.jar
/home/edureka/.ivy2/jars/org.apache.spark_spark-tags_2.11-2.0.0.jar
/home/edureka/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar
/home/edureka/.ivy2/jars/com.101tec_zkclient-0.8.jar
/home/edureka/.ivy2/jars/org.slf4j_slf4j-log4j12-1.7.16.jar
/home/edureka/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar
/home/edureka/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.0.4.jar
/home/edureka/.ivy2/jars/org.apache.kafka_kafka-clients-0.10.0.0.jar
/home/edureka/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar
/home/edureka/.ivy2/jars/log4j_log4j-1.2.17.jar
/home/edureka/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar
/home/edureka/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.2.4.jar
/home/edureka/.ivy2/jars/org.scalatest_scalatest_2.11-2.2.6.jar
/home/edureka/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar
/home/edureka/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar


Current Topic Name kafka_topic
18/03/02 22:40:04 INFO SparkContext: Running Spark version 2.1.1
18/03/02 22:40:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/03/02 22:40:05 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 10.0.2.15 instead (on interface enp0s3)
18/03/02 22:40:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/03/02 22:40:05 INFO SecurityManager: Changing view acls to: edureka
18/03/02 22:40:05 INFO SecurityManager: Changing modify acls to: edureka
18/03/02 22:40:05 INFO SecurityManager: Changing view acls groups to: 
18/03/02 22:40:05 INFO SecurityManager: Changing modify acls groups to: 
18/03/02 22:40:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(edureka); groups with view permissions: Set(); users  with modify permissions: Set(edureka); groups with modify permissions: Set()
18/03/02 22:40:05 INFO Utils: Successfully started service 'sparkDriver' on port 34822.
18/03/02 22:40:05 INFO SparkEnv: Registering MapOutputTracker
18/03/02 22:40:05 INFO SparkEnv: Registering BlockManagerMaster
18/03/02 22:40:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/03/02 22:40:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/03/02 22:40:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9e19f4fb-fc5f-43e2-a584-60364a7d06b8
18/03/02 22:40:06 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
18/03/02 22:40:06 INFO SparkEnv: Registering OutputCommitCoordinator
18/03/02 22:40:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/03/02 22:40:06 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4040
18/03/02 22:40:06 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-10_2.11-2.0.0.jar at spark://10.0.2.15:34822/jars/org.apache.spark_spark-streaming-kafka-0-10_2.11-2.0.0.jar with timestamp 1520010606759
18/03/02 22:40:06 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.apache.kafka_kafka_2.11-0.10.0.0.jar at spark://10.0.2.15:34822/jars/org.apache.kafka_kafka_2.11-0.10.0.0.jar with timestamp 1520010606760
18/03/02 22:40:06 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.apache.spark_spark-tags_2.11-2.0.0.jar at spark://10.0.2.15:34822/jars/org.apache.spark_spark-tags_2.11-2.0.0.jar with timestamp 1520010606760
18/03/02 22:40:06 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://10.0.2.15:34822/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1520010606760
18/03/02 22:40:06 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/com.101tec_zkclient-0.8.jar at spark://10.0.2.15:34822/jars/com.101tec_zkclient-0.8.jar with timestamp 1520010606760
18/03/02 22:40:06 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.slf4j_slf4j-log4j12-1.7.16.jar at spark://10.0.2.15:34822/jars/org.slf4j_slf4j-log4j12-1.7.16.jar with timestamp 1520010606760
18/03/02 22:40:06 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://10.0.2.15:34822/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1520010606761
18/03/02 22:40:06 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.0.4.jar at spark://10.0.2.15:34822/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.0.4.jar with timestamp 1520010606761
18/03/02 22:40:06 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.apache.kafka_kafka-clients-0.10.0.0.jar at spark://10.0.2.15:34822/jars/org.apache.kafka_kafka-clients-0.10.0.0.jar with timestamp 1520010606773
18/03/02 22:40:06 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://10.0.2.15:34822/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1520010606773
18/03/02 22:40:06 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://10.0.2.15:34822/jars/log4j_log4j-1.2.17.jar with timestamp 1520010606773
18/03/02 22:40:06 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar at spark://10.0.2.15:34822/jars/net.jpountz.lz4_lz4-1.3.0.jar with timestamp 1520010606773
18/03/02 22:40:06 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.2.4.jar at spark://10.0.2.15:34822/jars/org.xerial.snappy_snappy-java-1.1.2.4.jar with timestamp 1520010606774
18/03/02 22:40:06 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.scalatest_scalatest_2.11-2.2.6.jar at spark://10.0.2.15:34822/jars/org.scalatest_scalatest_2.11-2.2.6.jar with timestamp 1520010606774
18/03/02 22:40:06 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar at spark://10.0.2.15:34822/jars/org.scala-lang_scala-reflect-2.11.8.jar with timestamp 1520010606774
18/03/02 22:40:06 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://10.0.2.15:34822/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1520010606774
18/03/02 22:40:06 INFO SparkContext: Added JAR file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar at spark://10.0.2.15:34822/jars/sparkdemo_2.11-0.1.0-SNAPSHOT.jar with timestamp 1520010606774
18/03/02 22:40:06 INFO Executor: Starting executor ID driver on host localhost
18/03/02 22:40:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39100.
18/03/02 22:40:07 INFO NettyBlockTransferService: Server created on 10.0.2.15:39100
18/03/02 22:40:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/03/02 22:40:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 39100, None)
18/03/02 22:40:07 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:39100 with 366.3 MB RAM, BlockManagerId(driver, 10.0.2.15, 39100, None)
18/03/02 22:40:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 39100, None)
18/03/02 22:40:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 39100, None)
18/03/02 22:40:07 WARN KafkaUtils: overriding enable.auto.commit to false for executor
18/03/02 22:40:07 WARN KafkaUtils: overriding auto.offset.reset to none for executor
18/03/02 22:40:07 WARN KafkaUtils: overriding executor group.id to spark-executor-use_a_separate_group_id_for_each_stream
18/03/02 22:40:07 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
18/03/02 22:40:08 INFO DirectKafkaInputDStream: Slide time = 5000 ms
18/03/02 22:40:08 INFO DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
18/03/02 22:40:08 INFO DirectKafkaInputDStream: Checkpoint interval = null
18/03/02 22:40:08 INFO DirectKafkaInputDStream: Remember interval = 5000 ms
18/03/02 22:40:08 INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@4f651499
18/03/02 22:40:08 INFO ForEachDStream: Slide time = 5000 ms
18/03/02 22:40:08 INFO ForEachDStream: Storage level = Serialized 1x Replicated
18/03/02 22:40:08 INFO ForEachDStream: Checkpoint interval = null
18/03/02 22:40:08 INFO ForEachDStream: Remember interval = 5000 ms
18/03/02 22:40:08 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@43858aa9
18/03/02 22:40:08 INFO DirectKafkaInputDStream: Slide time = 5000 ms
18/03/02 22:40:08 INFO DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
18/03/02 22:40:08 INFO DirectKafkaInputDStream: Checkpoint interval = null
18/03/02 22:40:08 INFO DirectKafkaInputDStream: Remember interval = 5000 ms
18/03/02 22:40:08 INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@4f651499
18/03/02 22:40:08 INFO ForEachDStream: Slide time = 5000 ms
18/03/02 22:40:08 INFO ForEachDStream: Storage level = Serialized 1x Replicated
18/03/02 22:40:08 INFO ForEachDStream: Checkpoint interval = null
18/03/02 22:40:08 INFO ForEachDStream: Remember interval = 5000 ms
18/03/02 22:40:08 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@5748ba3
18/03/02 22:40:08 INFO ConsumerConfig: ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = use_a_separate_group_id_for_each_stream
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

18/03/02 22:40:08 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 22:40:08 INFO AppInfoParser: Kafka commitId : cb8625948210849f
18/03/02 22:40:12 INFO AbstractCoordinator: Discovered coordinator localhost:9092 (id: 2147483647 rack: null) for group use_a_separate_group_id_for_each_stream.
18/03/02 22:40:12 INFO ConsumerCoordinator: Revoking previously assigned partitions [] for group use_a_separate_group_id_for_each_stream
18/03/02 22:40:12 INFO AbstractCoordinator: (Re-)joining group use_a_separate_group_id_for_each_stream
18/03/02 22:40:12 INFO AbstractCoordinator: Successfully joined group use_a_separate_group_id_for_each_stream with generation 1
18/03/02 22:40:12 INFO ConsumerCoordinator: Setting newly assigned partitions [kafka_topic-0] for group use_a_separate_group_id_for_each_stream
18/03/02 22:40:12 INFO RecurringTimer: Started timer for JobGenerator at time 1520010610000
18/03/02 22:40:12 INFO JobGenerator: Started JobGenerator at 1520010610000 ms
18/03/02 22:40:12 INFO JobScheduler: Started JobScheduler
18/03/02 22:40:12 INFO StreamingContext: StreamingContext started
18/03/02 22:40:13 INFO JobScheduler: Added jobs for time 1520010610000 ms
18/03/02 22:40:13 INFO JobScheduler: Starting job streaming job 1520010610000 ms.0 from job set of time 1520010610000 ms
18/03/02 22:40:13 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:40:13 INFO DAGScheduler: Got job 0 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:40:13 INFO DAGScheduler: Final stage: ResultStage 0 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:40:13 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:40:13 INFO DAGScheduler: Missing parents: List()
18/03/02 22:40:13 INFO DAGScheduler: Submitting ResultStage 0 (KafkaRDD[0] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:40:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:40:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1621.0 B, free 366.3 MB)
18/03/02 22:40:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:39100 (size: 1621.0 B, free: 366.3 MB)
18/03/02 22:40:13 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:996
18/03/02 22:40:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (KafkaRDD[0] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:40:13 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
18/03/02 22:40:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7563 bytes)
18/03/02 22:40:13 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
18/03/02 22:40:13 INFO Executor: Fetching spark://10.0.2.15:34822/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1520010606773
18/03/02 22:40:14 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:34822 after 68 ms (0 ms spent in bootstraps)
18/03/02 22:40:14 INFO Utils: Fetching spark://10.0.2.15:34822/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/fetchFileTemp8783709014661704033.tmp
18/03/02 22:40:14 INFO Executor: Adding file:/tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/org.slf4j_slf4j-api-1.7.16.jar to class loader
18/03/02 22:40:14 INFO Executor: Fetching spark://10.0.2.15:34822/jars/org.apache.kafka_kafka-clients-0.10.0.0.jar with timestamp 1520010606773
18/03/02 22:40:14 INFO Utils: Fetching spark://10.0.2.15:34822/jars/org.apache.kafka_kafka-clients-0.10.0.0.jar to /tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/fetchFileTemp3934567754684266206.tmp
18/03/02 22:40:14 INFO Executor: Adding file:/tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/org.apache.kafka_kafka-clients-0.10.0.0.jar to class loader
18/03/02 22:40:14 INFO Executor: Fetching spark://10.0.2.15:34822/jars/sparkdemo_2.11-0.1.0-SNAPSHOT.jar with timestamp 1520010606774
18/03/02 22:40:14 INFO Utils: Fetching spark://10.0.2.15:34822/jars/sparkdemo_2.11-0.1.0-SNAPSHOT.jar to /tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/fetchFileTemp3735615198582362303.tmp
18/03/02 22:40:14 INFO Executor: Adding file:/tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/sparkdemo_2.11-0.1.0-SNAPSHOT.jar to class loader
18/03/02 22:40:14 INFO Executor: Fetching spark://10.0.2.15:34822/jars/com.101tec_zkclient-0.8.jar with timestamp 1520010606760
18/03/02 22:40:14 INFO Utils: Fetching spark://10.0.2.15:34822/jars/com.101tec_zkclient-0.8.jar to /tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/fetchFileTemp8295150087436788082.tmp
18/03/02 22:40:14 INFO Executor: Adding file:/tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/com.101tec_zkclient-0.8.jar to class loader
18/03/02 22:40:14 INFO Executor: Fetching spark://10.0.2.15:34822/jars/org.apache.kafka_kafka_2.11-0.10.0.0.jar with timestamp 1520010606760
18/03/02 22:40:14 INFO Utils: Fetching spark://10.0.2.15:34822/jars/org.apache.kafka_kafka_2.11-0.10.0.0.jar to /tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/fetchFileTemp4431639681474082650.tmp
18/03/02 22:40:14 INFO Executor: Adding file:/tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/org.apache.kafka_kafka_2.11-0.10.0.0.jar to class loader
18/03/02 22:40:14 INFO Executor: Fetching spark://10.0.2.15:34822/jars/org.scalatest_scalatest_2.11-2.2.6.jar with timestamp 1520010606774
18/03/02 22:40:14 INFO Utils: Fetching spark://10.0.2.15:34822/jars/org.scalatest_scalatest_2.11-2.2.6.jar to /tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/fetchFileTemp1630927915380916790.tmp
18/03/02 22:40:14 INFO Executor: Adding file:/tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/org.scalatest_scalatest_2.11-2.2.6.jar to class loader
18/03/02 22:40:14 INFO Executor: Fetching spark://10.0.2.15:34822/jars/org.slf4j_slf4j-log4j12-1.7.16.jar with timestamp 1520010606760
18/03/02 22:40:14 INFO Utils: Fetching spark://10.0.2.15:34822/jars/org.slf4j_slf4j-log4j12-1.7.16.jar to /tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/fetchFileTemp4794242268580053060.tmp
18/03/02 22:40:14 INFO Executor: Adding file:/tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/org.slf4j_slf4j-log4j12-1.7.16.jar to class loader
18/03/02 22:40:14 INFO Executor: Fetching spark://10.0.2.15:34822/jars/org.xerial.snappy_snappy-java-1.1.2.4.jar with timestamp 1520010606774
18/03/02 22:40:14 INFO Utils: Fetching spark://10.0.2.15:34822/jars/org.xerial.snappy_snappy-java-1.1.2.4.jar to /tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/fetchFileTemp4773407992940798169.tmp
18/03/02 22:40:14 INFO Executor: Adding file:/tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/org.xerial.snappy_snappy-java-1.1.2.4.jar to class loader
18/03/02 22:40:14 INFO Executor: Fetching spark://10.0.2.15:34822/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.0.4.jar with timestamp 1520010606761
18/03/02 22:40:14 INFO Utils: Fetching spark://10.0.2.15:34822/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.0.4.jar to /tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/fetchFileTemp2081873740408371366.tmp
18/03/02 22:40:14 INFO Executor: Adding file:/tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/org.scala-lang.modules_scala-parser-combinators_2.11-1.0.4.jar to class loader
18/03/02 22:40:14 INFO Executor: Fetching spark://10.0.2.15:34822/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1520010606761
18/03/02 22:40:14 INFO Utils: Fetching spark://10.0.2.15:34822/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/fetchFileTemp894124122568490969.tmp
18/03/02 22:40:14 INFO Executor: Adding file:/tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/com.yammer.metrics_metrics-core-2.2.0.jar to class loader
18/03/02 22:40:14 INFO Executor: Fetching spark://10.0.2.15:34822/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1520010606774
18/03/02 22:40:14 INFO Utils: Fetching spark://10.0.2.15:34822/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/fetchFileTemp861906953676708234.tmp
18/03/02 22:40:14 INFO Executor: Adding file:/tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader
18/03/02 22:40:14 INFO Executor: Fetching spark://10.0.2.15:34822/jars/net.jpountz.lz4_lz4-1.3.0.jar with timestamp 1520010606773
18/03/02 22:40:14 INFO Utils: Fetching spark://10.0.2.15:34822/jars/net.jpountz.lz4_lz4-1.3.0.jar to /tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/fetchFileTemp5047050647856933037.tmp
18/03/02 22:40:14 INFO Executor: Adding file:/tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/net.jpountz.lz4_lz4-1.3.0.jar to class loader
18/03/02 22:40:14 INFO Executor: Fetching spark://10.0.2.15:34822/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1520010606760
18/03/02 22:40:14 INFO Utils: Fetching spark://10.0.2.15:34822/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/fetchFileTemp8569980128616957368.tmp
18/03/02 22:40:14 INFO Executor: Adding file:/tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/org.spark-project.spark_unused-1.0.0.jar to class loader
18/03/02 22:40:14 INFO Executor: Fetching spark://10.0.2.15:34822/jars/org.apache.spark_spark-streaming-kafka-0-10_2.11-2.0.0.jar with timestamp 1520010606759
18/03/02 22:40:14 INFO Utils: Fetching spark://10.0.2.15:34822/jars/org.apache.spark_spark-streaming-kafka-0-10_2.11-2.0.0.jar to /tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/fetchFileTemp8706588071454308960.tmp
18/03/02 22:40:14 INFO Executor: Adding file:/tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/org.apache.spark_spark-streaming-kafka-0-10_2.11-2.0.0.jar to class loader
18/03/02 22:40:14 INFO Executor: Fetching spark://10.0.2.15:34822/jars/org.apache.spark_spark-tags_2.11-2.0.0.jar with timestamp 1520010606760
18/03/02 22:40:14 INFO Utils: Fetching spark://10.0.2.15:34822/jars/org.apache.spark_spark-tags_2.11-2.0.0.jar to /tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/fetchFileTemp1366433409188374902.tmp
18/03/02 22:40:14 INFO Executor: Adding file:/tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/org.apache.spark_spark-tags_2.11-2.0.0.jar to class loader
18/03/02 22:40:14 INFO Executor: Fetching spark://10.0.2.15:34822/jars/log4j_log4j-1.2.17.jar with timestamp 1520010606773
18/03/02 22:40:14 INFO Utils: Fetching spark://10.0.2.15:34822/jars/log4j_log4j-1.2.17.jar to /tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/fetchFileTemp5260672017565720154.tmp
18/03/02 22:40:14 INFO Executor: Adding file:/tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/log4j_log4j-1.2.17.jar to class loader
18/03/02 22:40:14 INFO Executor: Fetching spark://10.0.2.15:34822/jars/org.scala-lang_scala-reflect-2.11.8.jar with timestamp 1520010606774
18/03/02 22:40:14 INFO Utils: Fetching spark://10.0.2.15:34822/jars/org.scala-lang_scala-reflect-2.11.8.jar to /tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/fetchFileTemp3314710718781917.tmp
18/03/02 22:40:14 INFO Executor: Adding file:/tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10/userFiles-16515697-4dd4-4c45-a93b-d3fb80509ec3/org.scala-lang_scala-reflect-2.11.8.jar to class loader
18/03/02 22:40:14 INFO KafkaRDD: Computing topic kafka_topic, partition 0 offsets 0 -> 25
18/03/02 22:40:14 INFO CachedKafkaConsumer: Initializing cache 16 64 0.75
18/03/02 22:40:14 INFO CachedKafkaConsumer: Cache miss for CacheKey(spark-executor-use_a_separate_group_id_for_each_stream,kafka_topic,0)
18/03/02 22:40:14 INFO ConsumerConfig: ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-use_a_separate_group_id_for_each_stream
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

18/03/02 22:40:14 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 22:40:14 INFO AppInfoParser: Kafka commitId : cb8625948210849f
kafka_topic 0 0 25
18/03/02 22:40:14 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1012 bytes result sent to driver
18/03/02 22:40:14 INFO DAGScheduler: ResultStage 0 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 1.330 s
18/03/02 22:40:14 INFO DAGScheduler: Job 0 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 1.776637 s
18/03/02 22:40:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1269 ms on localhost (executor driver) (1/1)
18/03/02 22:40:14 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
18/03/02 22:40:14 INFO JobScheduler: Finished job streaming job 1520010610000 ms.0 from job set of time 1520010610000 ms
18/03/02 22:40:14 INFO JobScheduler: Starting job streaming job 1520010610000 ms.1 from job set of time 1520010610000 ms
18/03/02 22:40:15 INFO JobScheduler: Added jobs for time 1520010615000 ms
18/03/02 22:40:15 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:40:15 INFO DAGScheduler: Got job 1 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:40:15 INFO DAGScheduler: Final stage: ResultStage 1 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:40:15 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:40:15 INFO DAGScheduler: Missing parents: List()
18/03/02 22:40:15 INFO DAGScheduler: Submitting ResultStage 1 (KafkaRDD[0] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:40:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:40:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1605.0 B, free 366.3 MB)
18/03/02 22:40:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:39100 (size: 1605.0 B, free: 366.3 MB)
18/03/02 22:40:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:996
18/03/02 22:40:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (KafkaRDD[0] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:40:15 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
18/03/02 22:40:15 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7554 bytes)
18/03/02 22:40:15 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
18/03/02 22:40:15 INFO KafkaRDD: Computing topic kafka_topic, partition 0 offsets 0 -> 25
18/03/02 22:40:15 INFO CachedKafkaConsumer: Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream kafka_topic 0 0
null,Hi hello,Hi hello
null,how r u?,how r u?
null,aFFDVF,aFFDVF
null,FA,FA
null,FA,FA
null,FSAF,FSAF
null,FS,FS
null,SFF,SFF
null,SF,SF
null,FSA,FSA
null,SF,SF
null,DSDA,DSDA
null,FSA,FSA
null,F,F
null,FA,FA
null,SFA,SFA
null,A,A
null,FSAAF,FSAAF
null,SAF,SAF
null,FSAD,FSAD
null,FAA,FAA
null,SF,SF
null,AF,AF
null,F,F
null,FADS,FADS
18/03/02 22:40:15 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 998 bytes result sent to driver
18/03/02 22:40:15 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 435 ms on localhost (executor driver) (1/1)
18/03/02 22:40:15 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
18/03/02 22:40:15 INFO DAGScheduler: ResultStage 1 (foreach at SparkStreamingConsumer.scala:70) finished in 0.439 s
18/03/02 22:40:15 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:39100 in memory (size: 1621.0 B, free: 366.3 MB)
18/03/02 22:40:15 INFO DAGScheduler: Job 1 finished: foreach at SparkStreamingConsumer.scala:70, took 0.476920 s
18/03/02 22:40:15 INFO JobScheduler: Finished job streaming job 1520010610000 ms.1 from job set of time 1520010610000 ms
18/03/02 22:40:15 INFO JobScheduler: Total delay: 5.504 s for time 1520010610000 ms (execution: 2.449 s)
18/03/02 22:40:15 INFO JobScheduler: Starting job streaming job 1520010615000 ms.0 from job set of time 1520010615000 ms
18/03/02 22:40:15 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:40:15 INFO DAGScheduler: Got job 2 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:40:15 INFO DAGScheduler: Final stage: ResultStage 2 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:40:15 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:40:15 INFO DAGScheduler: Missing parents: List()
18/03/02 22:40:15 INFO DAGScheduler: Submitting ResultStage 2 (KafkaRDD[1] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:40:15 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:40:15 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.3 MB)
18/03/02 22:40:15 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:40:15 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996
18/03/02 22:40:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (KafkaRDD[1] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:40:15 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
18/03/02 22:40:15 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:40:15 INFO InputInfoTracker: remove old batch metadata: 
18/03/02 22:40:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7563 bytes)
18/03/02 22:40:15 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
18/03/02 22:40:15 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:40:15 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 925 bytes result sent to driver
18/03/02 22:40:15 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 89 ms on localhost (executor driver) (1/1)
18/03/02 22:40:15 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
18/03/02 22:40:15 INFO DAGScheduler: ResultStage 2 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.102 s
18/03/02 22:40:15 INFO DAGScheduler: Job 2 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.133263 s
18/03/02 22:40:15 INFO JobScheduler: Finished job streaming job 1520010615000 ms.0 from job set of time 1520010615000 ms
18/03/02 22:40:15 INFO JobScheduler: Starting job streaming job 1520010615000 ms.1 from job set of time 1520010615000 ms
18/03/02 22:40:15 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:40:15 INFO DAGScheduler: Got job 3 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:40:15 INFO DAGScheduler: Final stage: ResultStage 3 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:40:15 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:40:15 INFO DAGScheduler: Missing parents: List()
18/03/02 22:40:15 INFO DAGScheduler: Submitting ResultStage 3 (KafkaRDD[1] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:40:15 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:40:15 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.3 MB)
18/03/02 22:40:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:40:15 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:996
18/03/02 22:40:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (KafkaRDD[1] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:40:15 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
18/03/02 22:40:15 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7554 bytes)
18/03/02 22:40:15 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
18/03/02 22:40:15 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:40:15 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 925 bytes result sent to driver
18/03/02 22:40:15 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 57 ms on localhost (executor driver) (1/1)
18/03/02 22:40:15 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
18/03/02 22:40:15 INFO DAGScheduler: ResultStage 3 (foreach at SparkStreamingConsumer.scala:70) finished in 0.059 s
18/03/02 22:40:15 INFO DAGScheduler: Job 3 finished: foreach at SparkStreamingConsumer.scala:70, took 0.097612 s
18/03/02 22:40:15 INFO JobScheduler: Finished job streaming job 1520010615000 ms.1 from job set of time 1520010615000 ms
18/03/02 22:40:15 INFO JobScheduler: Total delay: 0.771 s for time 1520010615000 ms (execution: 0.265 s)
18/03/02 22:40:15 INFO KafkaRDD: Removing RDD 0 from persistence list
18/03/02 22:40:15 INFO BlockManager: Removing RDD 0
18/03/02 22:40:15 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:40:15 INFO InputInfoTracker: remove old batch metadata: 
18/03/02 22:40:20 INFO JobScheduler: Added jobs for time 1520010620000 ms
18/03/02 22:40:20 INFO JobScheduler: Starting job streaming job 1520010620000 ms.0 from job set of time 1520010620000 ms
18/03/02 22:40:20 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:40:20 INFO DAGScheduler: Got job 4 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:40:20 INFO DAGScheduler: Final stage: ResultStage 4 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:40:20 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:40:20 INFO DAGScheduler: Missing parents: List()
18/03/02 22:40:20 INFO DAGScheduler: Submitting ResultStage 4 (KafkaRDD[2] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:40:20 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:40:20 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.3 MB)
18/03/02 22:40:20 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:40:20 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:996
18/03/02 22:40:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (KafkaRDD[2] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:40:20 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
18/03/02 22:40:20 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:40:20 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
18/03/02 22:40:20 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:40:20 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 925 bytes result sent to driver
18/03/02 22:40:20 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 31 ms on localhost (executor driver) (1/1)
18/03/02 22:40:20 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
18/03/02 22:40:20 INFO DAGScheduler: ResultStage 4 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.032 s
18/03/02 22:40:20 INFO DAGScheduler: Job 4 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.056194 s
18/03/02 22:40:20 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:40:20 INFO JobScheduler: Finished job streaming job 1520010620000 ms.0 from job set of time 1520010620000 ms
18/03/02 22:40:20 INFO JobScheduler: Starting job streaming job 1520010620000 ms.1 from job set of time 1520010620000 ms
18/03/02 22:40:20 INFO DAGScheduler: Got job 5 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:40:20 INFO DAGScheduler: Final stage: ResultStage 5 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:40:20 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:40:20 INFO DAGScheduler: Missing parents: List()
18/03/02 22:40:20 INFO DAGScheduler: Submitting ResultStage 5 (KafkaRDD[2] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:40:20 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:40:20 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.3 MB)
18/03/02 22:40:20 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:40:20 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:996
18/03/02 22:40:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (KafkaRDD[2] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:40:20 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
18/03/02 22:40:20 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes)
18/03/02 22:40:20 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
18/03/02 22:40:20 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:40:20 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 925 bytes result sent to driver
18/03/02 22:40:20 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 20 ms on localhost (executor driver) (1/1)
18/03/02 22:40:20 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
18/03/02 22:40:20 INFO DAGScheduler: ResultStage 5 (foreach at SparkStreamingConsumer.scala:70) finished in 0.022 s
18/03/02 22:40:20 INFO DAGScheduler: Job 5 finished: foreach at SparkStreamingConsumer.scala:70, took 0.046110 s
18/03/02 22:40:20 INFO JobScheduler: Finished job streaming job 1520010620000 ms.1 from job set of time 1520010620000 ms
18/03/02 22:40:20 INFO JobScheduler: Total delay: 0.123 s for time 1520010620000 ms (execution: 0.112 s)
18/03/02 22:40:20 INFO KafkaRDD: Removing RDD 1 from persistence list
18/03/02 22:40:20 INFO BlockManager: Removing RDD 1
18/03/02 22:40:20 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:40:20 INFO InputInfoTracker: remove old batch metadata: 1520010610000 ms
18/03/02 22:40:25 INFO JobScheduler: Added jobs for time 1520010625000 ms
18/03/02 22:40:25 INFO JobScheduler: Starting job streaming job 1520010625000 ms.0 from job set of time 1520010625000 ms
18/03/02 22:40:25 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:40:25 INFO DAGScheduler: Got job 6 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:40:25 INFO DAGScheduler: Final stage: ResultStage 6 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:40:25 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:40:25 INFO DAGScheduler: Missing parents: List()
18/03/02 22:40:25 INFO DAGScheduler: Submitting ResultStage 6 (KafkaRDD[3] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:40:25 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:40:25 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.3 MB)
18/03/02 22:40:25 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:40:25 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:996
18/03/02 22:40:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (KafkaRDD[3] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:40:25 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
18/03/02 22:40:25 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:40:25 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
18/03/02 22:40:25 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:40:25 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 925 bytes result sent to driver
18/03/02 22:40:25 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 66 ms on localhost (executor driver) (1/1)
18/03/02 22:40:25 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
18/03/02 22:40:25 INFO DAGScheduler: ResultStage 6 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.068 s
18/03/02 22:40:25 INFO DAGScheduler: Job 6 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.091859 s
18/03/02 22:40:25 INFO JobScheduler: Finished job streaming job 1520010625000 ms.0 from job set of time 1520010625000 ms
18/03/02 22:40:25 INFO JobScheduler: Starting job streaming job 1520010625000 ms.1 from job set of time 1520010625000 ms
18/03/02 22:40:25 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:40:25 INFO DAGScheduler: Got job 7 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:40:25 INFO DAGScheduler: Final stage: ResultStage 7 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:40:25 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:40:25 INFO DAGScheduler: Missing parents: List()
18/03/02 22:40:25 INFO DAGScheduler: Submitting ResultStage 7 (KafkaRDD[3] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:40:25 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:40:25 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.3 MB)
18/03/02 22:40:25 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:40:25 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:996
18/03/02 22:40:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (KafkaRDD[3] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:40:25 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
18/03/02 22:40:25 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes)
18/03/02 22:40:25 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
18/03/02 22:40:25 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:40:25 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 846 bytes result sent to driver
18/03/02 22:40:25 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 33 ms on localhost (executor driver) (1/1)
18/03/02 22:40:25 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
18/03/02 22:40:25 INFO DAGScheduler: ResultStage 7 (foreach at SparkStreamingConsumer.scala:70) finished in 0.037 s
18/03/02 22:40:25 INFO DAGScheduler: Job 7 finished: foreach at SparkStreamingConsumer.scala:70, took 0.085581 s
18/03/02 22:40:25 INFO JobScheduler: Finished job streaming job 1520010625000 ms.1 from job set of time 1520010625000 ms
18/03/02 22:40:25 INFO JobScheduler: Total delay: 0.216 s for time 1520010625000 ms (execution: 0.209 s)
18/03/02 22:40:25 INFO KafkaRDD: Removing RDD 2 from persistence list
18/03/02 22:40:25 INFO BlockManager: Removing RDD 2
18/03/02 22:40:25 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:40:25 INFO InputInfoTracker: remove old batch metadata: 1520010615000 ms
18/03/02 22:40:30 INFO JobScheduler: Added jobs for time 1520010630000 ms
18/03/02 22:40:30 INFO JobScheduler: Starting job streaming job 1520010630000 ms.0 from job set of time 1520010630000 ms
18/03/02 22:40:30 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:40:30 INFO DAGScheduler: Got job 8 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:40:30 INFO DAGScheduler: Final stage: ResultStage 8 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:40:30 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:40:30 INFO DAGScheduler: Missing parents: List()
18/03/02 22:40:30 INFO DAGScheduler: Submitting ResultStage 8 (KafkaRDD[4] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:40:30 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:40:30 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.3 MB)
18/03/02 22:40:30 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:40:30 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:996
18/03/02 22:40:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (KafkaRDD[4] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:40:30 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
18/03/02 22:40:30 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:40:30 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
18/03/02 22:40:30 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:40:30 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 925 bytes result sent to driver
18/03/02 22:40:30 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 20 ms on localhost (executor driver) (1/1)
18/03/02 22:40:30 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
18/03/02 22:40:30 INFO DAGScheduler: ResultStage 8 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.021 s
18/03/02 22:40:30 INFO DAGScheduler: Job 8 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.056239 s
18/03/02 22:40:30 INFO JobScheduler: Finished job streaming job 1520010630000 ms.0 from job set of time 1520010630000 ms
18/03/02 22:40:30 INFO JobScheduler: Starting job streaming job 1520010630000 ms.1 from job set of time 1520010630000 ms
18/03/02 22:40:30 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:40:30 INFO DAGScheduler: Got job 9 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:40:30 INFO DAGScheduler: Final stage: ResultStage 9 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:40:30 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:40:30 INFO DAGScheduler: Missing parents: List()
18/03/02 22:40:30 INFO DAGScheduler: Submitting ResultStage 9 (KafkaRDD[4] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:40:30 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:40:30 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.3 MB)
18/03/02 22:40:30 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:40:30 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:996
18/03/02 22:40:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (KafkaRDD[4] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:40:30 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
18/03/02 22:40:30 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes)
18/03/02 22:40:30 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
18/03/02 22:40:30 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:40:30 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 925 bytes result sent to driver
18/03/02 22:40:30 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 19 ms on localhost (executor driver) (1/1)
18/03/02 22:40:30 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
18/03/02 22:40:30 INFO DAGScheduler: ResultStage 9 (foreach at SparkStreamingConsumer.scala:70) finished in 0.020 s
18/03/02 22:40:30 INFO DAGScheduler: Job 9 finished: foreach at SparkStreamingConsumer.scala:70, took 0.056472 s
18/03/02 22:40:30 INFO JobScheduler: Finished job streaming job 1520010630000 ms.1 from job set of time 1520010630000 ms
18/03/02 22:40:30 INFO JobScheduler: Total delay: 0.134 s for time 1520010630000 ms (execution: 0.128 s)
18/03/02 22:40:30 INFO KafkaRDD: Removing RDD 3 from persistence list
18/03/02 22:40:30 INFO BlockManager: Removing RDD 3
18/03/02 22:40:30 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:40:30 INFO InputInfoTracker: remove old batch metadata: 1520010620000 ms
18/03/02 22:40:35 INFO JobScheduler: Added jobs for time 1520010635000 ms
18/03/02 22:40:35 INFO JobScheduler: Starting job streaming job 1520010635000 ms.0 from job set of time 1520010635000 ms
18/03/02 22:40:35 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:40:35 INFO DAGScheduler: Got job 10 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:40:35 INFO DAGScheduler: Final stage: ResultStage 10 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:40:35 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:40:35 INFO DAGScheduler: Missing parents: List()
18/03/02 22:40:35 INFO DAGScheduler: Submitting ResultStage 10 (KafkaRDD[5] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:40:35 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:40:35 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.3 MB)
18/03/02 22:40:35 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:40:35 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:996
18/03/02 22:40:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (KafkaRDD[5] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:40:35 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
18/03/02 22:40:35 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:40:35 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
18/03/02 22:40:35 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:40:35 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 846 bytes result sent to driver
18/03/02 22:40:35 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 21 ms on localhost (executor driver) (1/1)
18/03/02 22:40:35 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
18/03/02 22:40:35 INFO DAGScheduler: ResultStage 10 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.029 s
18/03/02 22:40:35 INFO DAGScheduler: Job 10 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.047551 s
18/03/02 22:40:35 INFO JobScheduler: Finished job streaming job 1520010635000 ms.0 from job set of time 1520010635000 ms
18/03/02 22:40:35 INFO JobScheduler: Starting job streaming job 1520010635000 ms.1 from job set of time 1520010635000 ms
18/03/02 22:40:35 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:40:35 INFO DAGScheduler: Got job 11 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:40:35 INFO DAGScheduler: Final stage: ResultStage 11 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:40:35 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:40:35 INFO DAGScheduler: Missing parents: List()
18/03/02 22:40:35 INFO DAGScheduler: Submitting ResultStage 11 (KafkaRDD[5] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:40:35 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:40:35 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.3 MB)
18/03/02 22:40:35 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:40:35 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:996
18/03/02 22:40:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (KafkaRDD[5] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:40:35 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
18/03/02 22:40:35 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes)
18/03/02 22:40:35 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
18/03/02 22:40:35 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:40:35 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 925 bytes result sent to driver
18/03/02 22:40:35 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 9 ms on localhost (executor driver) (1/1)
18/03/02 22:40:35 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
18/03/02 22:40:35 INFO DAGScheduler: ResultStage 11 (foreach at SparkStreamingConsumer.scala:70) finished in 0.015 s
18/03/02 22:40:35 INFO DAGScheduler: Job 11 finished: foreach at SparkStreamingConsumer.scala:70, took 0.024477 s
18/03/02 22:40:35 INFO JobScheduler: Finished job streaming job 1520010635000 ms.1 from job set of time 1520010635000 ms
18/03/02 22:40:35 INFO JobScheduler: Total delay: 0.098 s for time 1520010635000 ms (execution: 0.092 s)
18/03/02 22:40:35 INFO KafkaRDD: Removing RDD 4 from persistence list
18/03/02 22:40:35 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:40:35 INFO InputInfoTracker: remove old batch metadata: 1520010625000 ms
18/03/02 22:40:35 INFO BlockManager: Removing RDD 4
18/03/02 22:40:40 INFO JobScheduler: Added jobs for time 1520010640000 ms
18/03/02 22:40:40 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:40:40 INFO JobScheduler: Starting job streaming job 1520010640000 ms.0 from job set of time 1520010640000 ms
18/03/02 22:40:40 INFO DAGScheduler: Got job 12 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:40:40 INFO DAGScheduler: Final stage: ResultStage 12 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:40:40 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:40:40 INFO DAGScheduler: Missing parents: List()
18/03/02 22:40:40 INFO DAGScheduler: Submitting ResultStage 12 (KafkaRDD[6] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:40:40 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:40:40 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.3 MB)
18/03/02 22:40:40 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:40:40 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:996
18/03/02 22:40:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (KafkaRDD[6] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:40:40 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
18/03/02 22:40:40 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:40:40 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
18/03/02 22:40:40 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:40:40 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 925 bytes result sent to driver
18/03/02 22:40:40 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 12 ms on localhost (executor driver) (1/1)
18/03/02 22:40:40 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
18/03/02 22:40:40 INFO DAGScheduler: ResultStage 12 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.015 s
18/03/02 22:40:40 INFO DAGScheduler: Job 12 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.037071 s
18/03/02 22:40:40 INFO JobScheduler: Finished job streaming job 1520010640000 ms.0 from job set of time 1520010640000 ms
18/03/02 22:40:40 INFO JobScheduler: Starting job streaming job 1520010640000 ms.1 from job set of time 1520010640000 ms
18/03/02 22:40:40 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:40:40 INFO DAGScheduler: Got job 13 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:40:40 INFO DAGScheduler: Final stage: ResultStage 13 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:40:40 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:40:40 INFO DAGScheduler: Missing parents: List()
18/03/02 22:40:40 INFO DAGScheduler: Submitting ResultStage 13 (KafkaRDD[6] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:40:40 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:40:40 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.2 MB)
18/03/02 22:40:40 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:40:40 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:996
18/03/02 22:40:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (KafkaRDD[6] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:40:40 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
18/03/02 22:40:40 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes)
18/03/02 22:40:40 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
18/03/02 22:40:40 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:40:40 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 925 bytes result sent to driver
18/03/02 22:40:40 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 25 ms on localhost (executor driver) (1/1)
18/03/02 22:40:40 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
18/03/02 22:40:40 INFO DAGScheduler: ResultStage 13 (foreach at SparkStreamingConsumer.scala:70) finished in 0.026 s
18/03/02 22:40:40 INFO DAGScheduler: Job 13 finished: foreach at SparkStreamingConsumer.scala:70, took 0.068144 s
18/03/02 22:40:40 INFO JobScheduler: Finished job streaming job 1520010640000 ms.1 from job set of time 1520010640000 ms
18/03/02 22:40:40 INFO JobScheduler: Total delay: 0.126 s for time 1520010640000 ms (execution: 0.114 s)
18/03/02 22:40:40 INFO KafkaRDD: Removing RDD 5 from persistence list
18/03/02 22:40:40 INFO BlockManager: Removing RDD 5
18/03/02 22:40:40 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:40:40 INFO InputInfoTracker: remove old batch metadata: 1520010630000 ms
18/03/02 22:40:45 INFO JobScheduler: Added jobs for time 1520010645000 ms
18/03/02 22:40:45 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:40:45 INFO DAGScheduler: Got job 14 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:40:45 INFO DAGScheduler: Final stage: ResultStage 14 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:40:45 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:40:45 INFO DAGScheduler: Missing parents: List()
18/03/02 22:40:45 INFO DAGScheduler: Submitting ResultStage 14 (KafkaRDD[7] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:40:45 INFO JobScheduler: Starting job streaming job 1520010645000 ms.0 from job set of time 1520010645000 ms
18/03/02 22:40:45 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 2.4 KB, free 366.2 MB)
18/03/02 22:40:45 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 1629.0 B, free 366.2 MB)
18/03/02 22:40:45 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.2.15:39100 (size: 1629.0 B, free: 366.3 MB)
18/03/02 22:40:45 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:996
18/03/02 22:40:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (KafkaRDD[7] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:40:45 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
18/03/02 22:40:45 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:40:45 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
18/03/02 22:40:45 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:40:45 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 925 bytes result sent to driver
18/03/02 22:40:45 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 30 ms on localhost (executor driver) (1/1)
18/03/02 22:40:45 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
18/03/02 22:40:45 INFO DAGScheduler: ResultStage 14 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.037 s
18/03/02 22:40:45 INFO DAGScheduler: Job 14 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.069610 s
18/03/02 22:40:45 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:40:45 INFO JobScheduler: Finished job streaming job 1520010645000 ms.0 from job set of time 1520010645000 ms
18/03/02 22:40:45 INFO JobScheduler: Starting job streaming job 1520010645000 ms.1 from job set of time 1520010645000 ms
18/03/02 22:40:45 INFO DAGScheduler: Got job 15 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:40:45 INFO DAGScheduler: Final stage: ResultStage 15 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:40:45 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:40:45 INFO DAGScheduler: Missing parents: List()
18/03/02 22:40:45 INFO DAGScheduler: Submitting ResultStage 15 (KafkaRDD[7] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:40:45 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 2.4 KB, free 366.2 MB)
18/03/02 22:40:45 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 1613.0 B, free 366.2 MB)
18/03/02 22:40:45 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.2.15:39100 (size: 1613.0 B, free: 366.3 MB)
18/03/02 22:40:45 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:996
18/03/02 22:40:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (KafkaRDD[7] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:40:45 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
18/03/02 22:40:45 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes)
18/03/02 22:40:45 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
18/03/02 22:40:45 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:40:45 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 925 bytes result sent to driver
18/03/02 22:40:45 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 33 ms on localhost (executor driver) (1/1)
18/03/02 22:40:45 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
18/03/02 22:40:45 INFO DAGScheduler: ResultStage 15 (foreach at SparkStreamingConsumer.scala:70) finished in 0.035 s
18/03/02 22:40:45 INFO DAGScheduler: Job 15 finished: foreach at SparkStreamingConsumer.scala:70, took 0.062100 s
18/03/02 22:40:45 INFO JobScheduler: Finished job streaming job 1520010645000 ms.1 from job set of time 1520010645000 ms
18/03/02 22:40:45 INFO JobScheduler: Total delay: 0.166 s for time 1520010645000 ms (execution: 0.135 s)
18/03/02 22:40:45 INFO KafkaRDD: Removing RDD 6 from persistence list
18/03/02 22:40:45 INFO BlockManager: Removing RDD 6
18/03/02 22:40:45 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:40:45 INFO InputInfoTracker: remove old batch metadata: 1520010635000 ms
18/03/02 22:40:50 INFO JobScheduler: Added jobs for time 1520010650000 ms
18/03/02 22:40:50 INFO JobScheduler: Starting job streaming job 1520010650000 ms.0 from job set of time 1520010650000 ms
18/03/02 22:40:50 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:40:50 INFO DAGScheduler: Got job 16 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:40:50 INFO DAGScheduler: Final stage: ResultStage 16 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:40:50 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:40:50 INFO DAGScheduler: Missing parents: List()
18/03/02 22:40:50 INFO DAGScheduler: Submitting ResultStage 16 (KafkaRDD[8] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:40:50 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 2.4 KB, free 366.2 MB)
18/03/02 22:40:50 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.2 MB)
18/03/02 22:40:50 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:40:50 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:996
18/03/02 22:40:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (KafkaRDD[8] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:40:50 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
18/03/02 22:40:50 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:40:50 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
18/03/02 22:40:50 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:40:50 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 846 bytes result sent to driver
18/03/02 22:40:50 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 6 ms on localhost (executor driver) (1/1)
18/03/02 22:40:50 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
18/03/02 22:40:50 INFO DAGScheduler: ResultStage 16 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.015 s
18/03/02 22:40:50 INFO DAGScheduler: Job 16 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.026821 s
18/03/02 22:40:50 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:40:50 INFO DAGScheduler: Got job 17 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:40:50 INFO DAGScheduler: Final stage: ResultStage 17 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:40:50 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:40:50 INFO DAGScheduler: Missing parents: List()
18/03/02 22:40:50 INFO DAGScheduler: Submitting ResultStage 17 (KafkaRDD[8] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:40:50 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 2.4 KB, free 366.2 MB)
18/03/02 22:40:50 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.2 MB)
18/03/02 22:40:50 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:40:50 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:996
18/03/02 22:40:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (KafkaRDD[8] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:40:50 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks
18/03/02 22:40:50 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17, localhost, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes)
18/03/02 22:40:50 INFO JobScheduler: Finished job streaming job 1520010650000 ms.0 from job set of time 1520010650000 ms
18/03/02 22:40:50 INFO JobScheduler: Starting job streaming job 1520010650000 ms.1 from job set of time 1520010650000 ms
18/03/02 22:40:50 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.2.15:39100 in memory (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:40:50 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
18/03/02 22:40:50 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:40:50 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 925 bytes result sent to driver
18/03/02 22:40:50 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.2.15:39100 in memory (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:40:50 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.2.15:39100 in memory (size: 1629.0 B, free: 366.3 MB)
18/03/02 22:40:50 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.2.15:39100 in memory (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:40:50 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 37 ms on localhost (executor driver) (1/1)
18/03/02 22:40:50 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
18/03/02 22:40:50 INFO DAGScheduler: ResultStage 17 (foreach at SparkStreamingConsumer.scala:70) finished in 0.037 s
18/03/02 22:40:50 INFO DAGScheduler: Job 17 finished: foreach at SparkStreamingConsumer.scala:70, took 0.053515 s
18/03/02 22:40:50 INFO JobScheduler: Finished job streaming job 1520010650000 ms.1 from job set of time 1520010650000 ms
18/03/02 22:40:50 INFO JobScheduler: Total delay: 0.100 s for time 1520010650000 ms (execution: 0.093 s)
18/03/02 22:40:50 INFO KafkaRDD: Removing RDD 7 from persistence list
18/03/02 22:40:50 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.2.15:39100 in memory (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:40:50 INFO BlockManager: Removing RDD 7
18/03/02 22:40:50 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:40:50 INFO InputInfoTracker: remove old batch metadata: 1520010640000 ms
18/03/02 22:40:50 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.2.15:39100 in memory (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:40:50 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.2.15:39100 in memory (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:40:50 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.2.15:39100 in memory (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:40:50 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.2.15:39100 in memory (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:40:50 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.2.15:39100 in memory (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:40:50 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.2.15:39100 in memory (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:40:50 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.2.15:39100 in memory (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:40:50 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.2.15:39100 in memory (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:40:50 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.2.15:39100 in memory (size: 1613.0 B, free: 366.3 MB)
18/03/02 22:40:50 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.2.15:39100 in memory (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:40:55 INFO JobScheduler: Added jobs for time 1520010655000 ms
18/03/02 22:40:55 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:40:55 INFO DAGScheduler: Got job 18 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:40:55 INFO DAGScheduler: Final stage: ResultStage 18 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:40:55 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:40:55 INFO DAGScheduler: Missing parents: List()
18/03/02 22:40:55 INFO DAGScheduler: Submitting ResultStage 18 (KafkaRDD[9] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:40:55 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:40:55 INFO JobScheduler: Starting job streaming job 1520010655000 ms.0 from job set of time 1520010655000 ms
18/03/02 22:40:55 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.3 MB)
18/03/02 22:40:55 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:40:55 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:996
18/03/02 22:40:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (KafkaRDD[9] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:40:55 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks
18/03/02 22:40:55 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:40:55 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
18/03/02 22:40:55 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:40:55 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 846 bytes result sent to driver
18/03/02 22:40:55 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 7 ms on localhost (executor driver) (1/1)
18/03/02 22:40:55 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
18/03/02 22:40:55 INFO DAGScheduler: ResultStage 18 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.008 s
18/03/02 22:40:55 INFO DAGScheduler: Job 18 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.016522 s
18/03/02 22:40:55 INFO JobScheduler: Finished job streaming job 1520010655000 ms.0 from job set of time 1520010655000 ms
18/03/02 22:40:55 INFO JobScheduler: Starting job streaming job 1520010655000 ms.1 from job set of time 1520010655000 ms
18/03/02 22:40:55 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:40:55 INFO DAGScheduler: Got job 19 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:40:55 INFO DAGScheduler: Final stage: ResultStage 19 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:40:55 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:40:55 INFO DAGScheduler: Missing parents: List()
18/03/02 22:40:55 INFO DAGScheduler: Submitting ResultStage 19 (KafkaRDD[9] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:40:55 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:40:55 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.3 MB)
18/03/02 22:40:55 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:40:55 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:996
18/03/02 22:40:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (KafkaRDD[9] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:40:55 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks
18/03/02 22:40:55 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19, localhost, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes)
18/03/02 22:40:55 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
18/03/02 22:40:55 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:40:55 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 846 bytes result sent to driver
18/03/02 22:40:55 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 9 ms on localhost (executor driver) (1/1)
18/03/02 22:40:55 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
18/03/02 22:40:55 INFO DAGScheduler: ResultStage 19 (foreach at SparkStreamingConsumer.scala:70) finished in 0.016 s
18/03/02 22:40:55 INFO DAGScheduler: Job 19 finished: foreach at SparkStreamingConsumer.scala:70, took 0.030530 s
18/03/02 22:40:55 INFO JobScheduler: Finished job streaming job 1520010655000 ms.1 from job set of time 1520010655000 ms
18/03/02 22:40:55 INFO JobScheduler: Total delay: 0.067 s for time 1520010655000 ms (execution: 0.051 s)
18/03/02 22:40:55 INFO KafkaRDD: Removing RDD 8 from persistence list
18/03/02 22:40:55 INFO BlockManager: Removing RDD 8
18/03/02 22:40:55 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:40:55 INFO InputInfoTracker: remove old batch metadata: 1520010645000 ms
18/03/02 22:41:00 INFO JobScheduler: Added jobs for time 1520010660000 ms
18/03/02 22:41:00 INFO JobScheduler: Starting job streaming job 1520010660000 ms.0 from job set of time 1520010660000 ms
18/03/02 22:41:00 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:41:00 INFO DAGScheduler: Got job 20 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:41:00 INFO DAGScheduler: Final stage: ResultStage 20 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:41:00 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:00 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:00 INFO DAGScheduler: Submitting ResultStage 20 (KafkaRDD[10] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:00 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:41:00 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.3 MB)
18/03/02 22:41:00 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:41:00 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (KafkaRDD[10] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:00 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks
18/03/02 22:41:00 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:41:00 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)
18/03/02 22:41:00 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:41:00 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 925 bytes result sent to driver
18/03/02 22:41:00 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 9 ms on localhost (executor driver) (1/1)
18/03/02 22:41:00 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
18/03/02 22:41:00 INFO DAGScheduler: ResultStage 20 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.017 s
18/03/02 22:41:00 INFO DAGScheduler: Job 20 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.027779 s
18/03/02 22:41:00 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:41:00 INFO JobScheduler: Finished job streaming job 1520010660000 ms.0 from job set of time 1520010660000 ms
18/03/02 22:41:00 INFO JobScheduler: Starting job streaming job 1520010660000 ms.1 from job set of time 1520010660000 ms
18/03/02 22:41:00 INFO DAGScheduler: Got job 21 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:41:00 INFO DAGScheduler: Final stage: ResultStage 21 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:41:00 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:00 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:00 INFO DAGScheduler: Submitting ResultStage 21 (KafkaRDD[10] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:00 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:41:00 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.3 MB)
18/03/02 22:41:00 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:41:00 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (KafkaRDD[10] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:00 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks
18/03/02 22:41:00 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21, localhost, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes)
18/03/02 22:41:00 INFO Executor: Running task 0.0 in stage 21.0 (TID 21)
18/03/02 22:41:00 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:41:00 INFO Executor: Finished task 0.0 in stage 21.0 (TID 21). 925 bytes result sent to driver
18/03/02 22:41:00 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 9 ms on localhost (executor driver) (1/1)
18/03/02 22:41:00 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
18/03/02 22:41:00 INFO DAGScheduler: ResultStage 21 (foreach at SparkStreamingConsumer.scala:70) finished in 0.020 s
18/03/02 22:41:00 INFO DAGScheduler: Job 21 finished: foreach at SparkStreamingConsumer.scala:70, took 0.035657 s
18/03/02 22:41:00 INFO JobScheduler: Finished job streaming job 1520010660000 ms.1 from job set of time 1520010660000 ms
18/03/02 22:41:00 INFO JobScheduler: Total delay: 0.086 s for time 1520010660000 ms (execution: 0.075 s)
18/03/02 22:41:00 INFO KafkaRDD: Removing RDD 9 from persistence list
18/03/02 22:41:00 INFO BlockManager: Removing RDD 9
18/03/02 22:41:00 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:41:00 INFO InputInfoTracker: remove old batch metadata: 1520010650000 ms
18/03/02 22:41:05 INFO JobScheduler: Added jobs for time 1520010665000 ms
18/03/02 22:41:05 INFO JobScheduler: Starting job streaming job 1520010665000 ms.0 from job set of time 1520010665000 ms
18/03/02 22:41:05 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:41:05 INFO DAGScheduler: Got job 22 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:41:05 INFO DAGScheduler: Final stage: ResultStage 22 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:41:05 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:05 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:05 INFO DAGScheduler: Submitting ResultStage 22 (KafkaRDD[11] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:05 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:41:05 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.3 MB)
18/03/02 22:41:05 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:41:05 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (KafkaRDD[11] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:05 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks
18/03/02 22:41:05 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:41:05 INFO Executor: Running task 0.0 in stage 22.0 (TID 22)
18/03/02 22:41:05 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:41:05 INFO Executor: Finished task 0.0 in stage 22.0 (TID 22). 925 bytes result sent to driver
18/03/02 22:41:05 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 29 ms on localhost (executor driver) (1/1)
18/03/02 22:41:05 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
18/03/02 22:41:05 INFO DAGScheduler: ResultStage 22 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.033 s
18/03/02 22:41:05 INFO DAGScheduler: Job 22 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.046378 s
18/03/02 22:41:05 INFO JobScheduler: Finished job streaming job 1520010665000 ms.0 from job set of time 1520010665000 ms
18/03/02 22:41:05 INFO JobScheduler: Starting job streaming job 1520010665000 ms.1 from job set of time 1520010665000 ms
18/03/02 22:41:05 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:41:05 INFO DAGScheduler: Got job 23 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:41:05 INFO DAGScheduler: Final stage: ResultStage 23 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:41:05 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:05 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:05 INFO DAGScheduler: Submitting ResultStage 23 (KafkaRDD[11] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:05 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:41:05 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.3 MB)
18/03/02 22:41:05 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:41:05 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (KafkaRDD[11] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:05 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
18/03/02 22:41:05 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23, localhost, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes)
18/03/02 22:41:05 INFO Executor: Running task 0.0 in stage 23.0 (TID 23)
18/03/02 22:41:05 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:41:05 INFO Executor: Finished task 0.0 in stage 23.0 (TID 23). 846 bytes result sent to driver
18/03/02 22:41:05 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 6 ms on localhost (executor driver) (1/1)
18/03/02 22:41:05 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
18/03/02 22:41:05 INFO DAGScheduler: ResultStage 23 (foreach at SparkStreamingConsumer.scala:70) finished in 0.008 s
18/03/02 22:41:05 INFO DAGScheduler: Job 23 finished: foreach at SparkStreamingConsumer.scala:70, took 0.025249 s
18/03/02 22:41:05 INFO JobScheduler: Finished job streaming job 1520010665000 ms.1 from job set of time 1520010665000 ms
18/03/02 22:41:05 INFO JobScheduler: Total delay: 0.090 s for time 1520010665000 ms (execution: 0.085 s)
18/03/02 22:41:05 INFO KafkaRDD: Removing RDD 10 from persistence list
18/03/02 22:41:05 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:41:05 INFO InputInfoTracker: remove old batch metadata: 1520010655000 ms
18/03/02 22:41:05 INFO BlockManager: Removing RDD 10
18/03/02 22:41:10 INFO JobScheduler: Added jobs for time 1520010670000 ms
18/03/02 22:41:10 INFO JobScheduler: Starting job streaming job 1520010670000 ms.0 from job set of time 1520010670000 ms
18/03/02 22:41:10 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:41:10 INFO DAGScheduler: Got job 24 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:41:10 INFO DAGScheduler: Final stage: ResultStage 24 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:41:10 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:10 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:10 INFO DAGScheduler: Submitting ResultStage 24 (KafkaRDD[12] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:10 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:41:10 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.3 MB)
18/03/02 22:41:10 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:41:10 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (KafkaRDD[12] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:10 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks
18/03/02 22:41:10 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:41:10 INFO Executor: Running task 0.0 in stage 24.0 (TID 24)
18/03/02 22:41:10 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:41:10 INFO Executor: Finished task 0.0 in stage 24.0 (TID 24). 925 bytes result sent to driver
18/03/02 22:41:10 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 7 ms on localhost (executor driver) (1/1)
18/03/02 22:41:10 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
18/03/02 22:41:10 INFO DAGScheduler: ResultStage 24 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.010 s
18/03/02 22:41:10 INFO DAGScheduler: Job 24 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.019538 s
18/03/02 22:41:10 INFO JobScheduler: Finished job streaming job 1520010670000 ms.0 from job set of time 1520010670000 ms
18/03/02 22:41:10 INFO JobScheduler: Starting job streaming job 1520010670000 ms.1 from job set of time 1520010670000 ms
18/03/02 22:41:10 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:41:10 INFO DAGScheduler: Got job 25 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:41:10 INFO DAGScheduler: Final stage: ResultStage 25 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:41:10 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:10 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:10 INFO DAGScheduler: Submitting ResultStage 25 (KafkaRDD[12] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:10 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:41:10 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.3 MB)
18/03/02 22:41:10 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:41:10 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (KafkaRDD[12] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:10 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks
18/03/02 22:41:10 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 25, localhost, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes)
18/03/02 22:41:10 INFO Executor: Running task 0.0 in stage 25.0 (TID 25)
18/03/02 22:41:10 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:41:10 INFO Executor: Finished task 0.0 in stage 25.0 (TID 25). 925 bytes result sent to driver
18/03/02 22:41:10 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 25) in 7 ms on localhost (executor driver) (1/1)
18/03/02 22:41:10 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
18/03/02 22:41:10 INFO DAGScheduler: ResultStage 25 (foreach at SparkStreamingConsumer.scala:70) finished in 0.008 s
18/03/02 22:41:10 INFO DAGScheduler: Job 25 finished: foreach at SparkStreamingConsumer.scala:70, took 0.027305 s
18/03/02 22:41:10 INFO JobScheduler: Finished job streaming job 1520010670000 ms.1 from job set of time 1520010670000 ms
18/03/02 22:41:10 INFO JobScheduler: Total delay: 0.070 s for time 1520010670000 ms (execution: 0.058 s)
18/03/02 22:41:10 INFO KafkaRDD: Removing RDD 11 from persistence list
18/03/02 22:41:10 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:41:10 INFO InputInfoTracker: remove old batch metadata: 1520010660000 ms
18/03/02 22:41:10 INFO BlockManager: Removing RDD 11
18/03/02 22:41:15 INFO JobScheduler: Added jobs for time 1520010675000 ms
18/03/02 22:41:15 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:41:15 INFO JobScheduler: Starting job streaming job 1520010675000 ms.0 from job set of time 1520010675000 ms
18/03/02 22:41:15 INFO DAGScheduler: Got job 26 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:41:15 INFO DAGScheduler: Final stage: ResultStage 26 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:41:15 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:15 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:15 INFO DAGScheduler: Submitting ResultStage 26 (KafkaRDD[13] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:15 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:41:15 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.3 MB)
18/03/02 22:41:15 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:41:15 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (KafkaRDD[13] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:15 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
18/03/02 22:41:15 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 26, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:41:15 INFO Executor: Running task 0.0 in stage 26.0 (TID 26)
18/03/02 22:41:15 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:41:15 INFO Executor: Finished task 0.0 in stage 26.0 (TID 26). 846 bytes result sent to driver
18/03/02 22:41:15 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 26) in 5 ms on localhost (executor driver) (1/1)
18/03/02 22:41:15 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
18/03/02 22:41:15 INFO DAGScheduler: ResultStage 26 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.006 s
18/03/02 22:41:15 INFO DAGScheduler: Job 26 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.041969 s
18/03/02 22:41:15 INFO JobScheduler: Finished job streaming job 1520010675000 ms.0 from job set of time 1520010675000 ms
18/03/02 22:41:15 INFO JobScheduler: Starting job streaming job 1520010675000 ms.1 from job set of time 1520010675000 ms
18/03/02 22:41:15 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:41:15 INFO DAGScheduler: Got job 27 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:41:15 INFO DAGScheduler: Final stage: ResultStage 27 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:41:15 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:15 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:15 INFO DAGScheduler: Submitting ResultStage 27 (KafkaRDD[13] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:15 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:41:15 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.3 MB)
18/03/02 22:41:15 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:41:15 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (KafkaRDD[13] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:15 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks
18/03/02 22:41:15 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 27, localhost, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes)
18/03/02 22:41:15 INFO Executor: Running task 0.0 in stage 27.0 (TID 27)
18/03/02 22:41:15 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:41:15 INFO Executor: Finished task 0.0 in stage 27.0 (TID 27). 933 bytes result sent to driver
18/03/02 22:41:15 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 27) in 5 ms on localhost (executor driver) (1/1)
18/03/02 22:41:15 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
18/03/02 22:41:15 INFO DAGScheduler: ResultStage 27 (foreach at SparkStreamingConsumer.scala:70) finished in 0.006 s
18/03/02 22:41:15 INFO DAGScheduler: Job 27 finished: foreach at SparkStreamingConsumer.scala:70, took 0.044687 s
18/03/02 22:41:15 INFO JobScheduler: Finished job streaming job 1520010675000 ms.1 from job set of time 1520010675000 ms
18/03/02 22:41:15 INFO JobScheduler: Total delay: 0.118 s for time 1520010675000 ms (execution: 0.091 s)
18/03/02 22:41:15 INFO KafkaRDD: Removing RDD 12 from persistence list
18/03/02 22:41:15 INFO BlockManager: Removing RDD 12
18/03/02 22:41:15 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:41:15 INFO InputInfoTracker: remove old batch metadata: 1520010665000 ms
18/03/02 22:41:20 INFO JobScheduler: Added jobs for time 1520010680000 ms
18/03/02 22:41:20 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:41:20 INFO JobScheduler: Starting job streaming job 1520010680000 ms.0 from job set of time 1520010680000 ms
18/03/02 22:41:20 INFO DAGScheduler: Got job 28 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:41:20 INFO DAGScheduler: Final stage: ResultStage 28 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:41:20 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:20 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:20 INFO DAGScheduler: Submitting ResultStage 28 (KafkaRDD[14] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:20 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:41:20 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.2 MB)
18/03/02 22:41:20 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:41:20 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (KafkaRDD[14] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:20 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks
18/03/02 22:41:20 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 28, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:41:20 INFO Executor: Running task 0.0 in stage 28.0 (TID 28)
18/03/02 22:41:20 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:41:20 INFO Executor: Finished task 0.0 in stage 28.0 (TID 28). 925 bytes result sent to driver
18/03/02 22:41:20 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 28) in 5 ms on localhost (executor driver) (1/1)
18/03/02 22:41:20 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
18/03/02 22:41:20 INFO DAGScheduler: ResultStage 28 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.006 s
18/03/02 22:41:20 INFO DAGScheduler: Job 28 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.029539 s
18/03/02 22:41:20 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:41:20 INFO JobScheduler: Finished job streaming job 1520010680000 ms.0 from job set of time 1520010680000 ms
18/03/02 22:41:20 INFO JobScheduler: Starting job streaming job 1520010680000 ms.1 from job set of time 1520010680000 ms
18/03/02 22:41:20 INFO DAGScheduler: Got job 29 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:41:20 INFO DAGScheduler: Final stage: ResultStage 29 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:41:20 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:20 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:20 INFO DAGScheduler: Submitting ResultStage 29 (KafkaRDD[14] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:20 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 2.4 KB, free 366.2 MB)
18/03/02 22:41:20 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.2 MB)
18/03/02 22:41:20 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:41:20 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (KafkaRDD[14] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:20 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks
18/03/02 22:41:20 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 29, localhost, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes)
18/03/02 22:41:20 INFO Executor: Running task 0.0 in stage 29.0 (TID 29)
18/03/02 22:41:20 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:41:20 INFO Executor: Finished task 0.0 in stage 29.0 (TID 29). 846 bytes result sent to driver
18/03/02 22:41:20 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 29) in 7 ms on localhost (executor driver) (1/1)
18/03/02 22:41:20 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
18/03/02 22:41:20 INFO DAGScheduler: ResultStage 29 (foreach at SparkStreamingConsumer.scala:70) finished in 0.032 s
18/03/02 22:41:20 INFO DAGScheduler: Job 29 finished: foreach at SparkStreamingConsumer.scala:70, took 0.060193 s
18/03/02 22:41:20 INFO JobScheduler: Finished job streaming job 1520010680000 ms.1 from job set of time 1520010680000 ms
18/03/02 22:41:20 INFO JobScheduler: Total delay: 0.112 s for time 1520010680000 ms (execution: 0.097 s)
18/03/02 22:41:20 INFO KafkaRDD: Removing RDD 13 from persistence list
18/03/02 22:41:20 INFO BlockManager: Removing RDD 13
18/03/02 22:41:20 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:41:20 INFO InputInfoTracker: remove old batch metadata: 1520010670000 ms
18/03/02 22:41:25 INFO JobScheduler: Added jobs for time 1520010685000 ms
18/03/02 22:41:25 INFO JobScheduler: Starting job streaming job 1520010685000 ms.0 from job set of time 1520010685000 ms
18/03/02 22:41:25 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:41:25 INFO DAGScheduler: Got job 30 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:41:25 INFO DAGScheduler: Final stage: ResultStage 30 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:41:25 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:25 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:25 INFO DAGScheduler: Submitting ResultStage 30 (KafkaRDD[15] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:25 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 2.4 KB, free 366.2 MB)
18/03/02 22:41:25 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.2 MB)
18/03/02 22:41:25 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:41:25 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (KafkaRDD[15] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:25 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks
18/03/02 22:41:25 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 30, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:41:25 INFO Executor: Running task 0.0 in stage 30.0 (TID 30)
18/03/02 22:41:25 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:41:25 INFO Executor: Finished task 0.0 in stage 30.0 (TID 30). 933 bytes result sent to driver
18/03/02 22:41:25 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 30) in 14 ms on localhost (executor driver) (1/1)
18/03/02 22:41:25 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
18/03/02 22:41:25 INFO DAGScheduler: ResultStage 30 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.016 s
18/03/02 22:41:25 INFO DAGScheduler: Job 30 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.024529 s
18/03/02 22:41:25 INFO JobScheduler: Finished job streaming job 1520010685000 ms.0 from job set of time 1520010685000 ms
18/03/02 22:41:25 INFO JobScheduler: Starting job streaming job 1520010685000 ms.1 from job set of time 1520010685000 ms
18/03/02 22:41:25 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:41:25 INFO DAGScheduler: Got job 31 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:41:25 INFO DAGScheduler: Final stage: ResultStage 31 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:41:25 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:25 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:25 INFO DAGScheduler: Submitting ResultStage 31 (KafkaRDD[15] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:25 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 2.4 KB, free 366.2 MB)
18/03/02 22:41:25 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.2 MB)
18/03/02 22:41:25 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:41:25 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (KafkaRDD[15] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:25 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks
18/03/02 22:41:25 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 31, localhost, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes)
18/03/02 22:41:25 INFO Executor: Running task 0.0 in stage 31.0 (TID 31)
18/03/02 22:41:25 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:41:25 INFO Executor: Finished task 0.0 in stage 31.0 (TID 31). 925 bytes result sent to driver
18/03/02 22:41:25 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 31) in 11 ms on localhost (executor driver) (1/1)
18/03/02 22:41:25 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
18/03/02 22:41:25 INFO DAGScheduler: ResultStage 31 (foreach at SparkStreamingConsumer.scala:70) finished in 0.019 s
18/03/02 22:41:25 INFO DAGScheduler: Job 31 finished: foreach at SparkStreamingConsumer.scala:70, took 0.045162 s
18/03/02 22:41:25 INFO JobScheduler: Finished job streaming job 1520010685000 ms.1 from job set of time 1520010685000 ms
18/03/02 22:41:25 INFO JobScheduler: Total delay: 0.093 s for time 1520010685000 ms (execution: 0.088 s)
18/03/02 22:41:25 INFO KafkaRDD: Removing RDD 14 from persistence list
18/03/02 22:41:25 INFO BlockManager: Removing RDD 14
18/03/02 22:41:25 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:41:25 INFO InputInfoTracker: remove old batch metadata: 1520010675000 ms
18/03/02 22:41:30 INFO JobScheduler: Added jobs for time 1520010690000 ms
18/03/02 22:41:30 INFO JobScheduler: Starting job streaming job 1520010690000 ms.0 from job set of time 1520010690000 ms
18/03/02 22:41:30 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:41:30 INFO DAGScheduler: Got job 32 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:41:30 INFO DAGScheduler: Final stage: ResultStage 32 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:41:30 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:30 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:30 INFO DAGScheduler: Submitting ResultStage 32 (KafkaRDD[16] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:30 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 2.4 KB, free 366.2 MB)
18/03/02 22:41:30 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 1629.0 B, free 366.2 MB)
18/03/02 22:41:30 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.0.2.15:39100 (size: 1629.0 B, free: 366.3 MB)
18/03/02 22:41:30 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (KafkaRDD[16] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:30 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks
18/03/02 22:41:30 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 32, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:41:30 INFO Executor: Running task 0.0 in stage 32.0 (TID 32)
18/03/02 22:41:30 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:41:30 INFO Executor: Finished task 0.0 in stage 32.0 (TID 32). 933 bytes result sent to driver
18/03/02 22:41:30 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 32) in 9 ms on localhost (executor driver) (1/1)
18/03/02 22:41:30 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
18/03/02 22:41:30 INFO DAGScheduler: ResultStage 32 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.012 s
18/03/02 22:41:30 INFO DAGScheduler: Job 32 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.034347 s
18/03/02 22:41:30 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:41:30 INFO JobScheduler: Finished job streaming job 1520010690000 ms.0 from job set of time 1520010690000 ms
18/03/02 22:41:30 INFO JobScheduler: Starting job streaming job 1520010690000 ms.1 from job set of time 1520010690000 ms
18/03/02 22:41:30 INFO DAGScheduler: Got job 33 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:41:30 INFO DAGScheduler: Final stage: ResultStage 33 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:41:30 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:30 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:30 INFO DAGScheduler: Submitting ResultStage 33 (KafkaRDD[16] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:30 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 2.4 KB, free 366.2 MB)
18/03/02 22:41:30 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 1613.0 B, free 366.2 MB)
18/03/02 22:41:30 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.0.2.15:39100 (size: 1613.0 B, free: 366.3 MB)
18/03/02 22:41:30 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (KafkaRDD[16] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:30 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks
18/03/02 22:41:30 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 33, localhost, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes)
18/03/02 22:41:30 INFO Executor: Running task 0.0 in stage 33.0 (TID 33)
18/03/02 22:41:30 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:41:30 INFO Executor: Finished task 0.0 in stage 33.0 (TID 33). 925 bytes result sent to driver
18/03/02 22:41:30 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 33) in 6 ms on localhost (executor driver) (1/1)
18/03/02 22:41:30 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
18/03/02 22:41:30 INFO DAGScheduler: ResultStage 33 (foreach at SparkStreamingConsumer.scala:70) finished in 0.016 s
18/03/02 22:41:30 INFO DAGScheduler: Job 33 finished: foreach at SparkStreamingConsumer.scala:70, took 0.043644 s
18/03/02 22:41:30 INFO JobScheduler: Finished job streaming job 1520010690000 ms.1 from job set of time 1520010690000 ms
18/03/02 22:41:30 INFO JobScheduler: Total delay: 0.097 s for time 1520010690000 ms (execution: 0.090 s)
18/03/02 22:41:30 INFO KafkaRDD: Removing RDD 15 from persistence list
18/03/02 22:41:30 INFO BlockManager: Removing RDD 15
18/03/02 22:41:30 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:41:30 INFO InputInfoTracker: remove old batch metadata: 1520010680000 ms
18/03/02 22:41:30 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.2.15:39100 in memory (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:41:30 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 10.0.2.15:39100 in memory (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:41:30 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 10.0.2.15:39100 in memory (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:41:30 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.2.15:39100 in memory (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:41:30 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 10.0.2.15:39100 in memory (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:41:30 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 10.0.2.15:39100 in memory (size: 1629.0 B, free: 366.3 MB)
18/03/02 22:41:30 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 10.0.2.15:39100 in memory (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:41:30 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.2.15:39100 in memory (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:41:30 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 10.0.2.15:39100 in memory (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:41:30 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 10.0.2.15:39100 in memory (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:41:30 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 10.0.2.15:39100 in memory (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:41:30 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 10.0.2.15:39100 in memory (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:41:30 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.2.15:39100 in memory (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:41:30 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.2.15:39100 in memory (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:41:30 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 10.0.2.15:39100 in memory (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:41:30 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.2.15:39100 in memory (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:41:35 INFO JobScheduler: Added jobs for time 1520010695000 ms
18/03/02 22:41:35 INFO JobScheduler: Starting job streaming job 1520010695000 ms.0 from job set of time 1520010695000 ms
18/03/02 22:41:35 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:41:35 INFO DAGScheduler: Got job 34 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:41:35 INFO DAGScheduler: Final stage: ResultStage 34 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:41:35 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:35 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:35 INFO DAGScheduler: Submitting ResultStage 34 (KafkaRDD[17] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:35 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:41:35 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.3 MB)
18/03/02 22:41:35 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:41:35 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (KafkaRDD[17] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:35 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks
18/03/02 22:41:35 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 34, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:41:35 INFO Executor: Running task 0.0 in stage 34.0 (TID 34)
18/03/02 22:41:35 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:41:35 INFO Executor: Finished task 0.0 in stage 34.0 (TID 34). 846 bytes result sent to driver
18/03/02 22:41:35 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 34) in 15 ms on localhost (executor driver) (1/1)
18/03/02 22:41:35 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
18/03/02 22:41:35 INFO DAGScheduler: ResultStage 34 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.017 s
18/03/02 22:41:35 INFO DAGScheduler: Job 34 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.027258 s
18/03/02 22:41:35 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:41:35 INFO DAGScheduler: Got job 35 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:41:35 INFO DAGScheduler: Final stage: ResultStage 35 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:41:35 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:35 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:35 INFO DAGScheduler: Submitting ResultStage 35 (KafkaRDD[17] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:35 INFO JobScheduler: Finished job streaming job 1520010695000 ms.0 from job set of time 1520010695000 ms
18/03/02 22:41:35 INFO JobScheduler: Starting job streaming job 1520010695000 ms.1 from job set of time 1520010695000 ms
18/03/02 22:41:35 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:41:35 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.3 MB)
18/03/02 22:41:35 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:41:35 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (KafkaRDD[17] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:35 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks
18/03/02 22:41:35 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 35, localhost, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes)
18/03/02 22:41:35 INFO Executor: Running task 0.0 in stage 35.0 (TID 35)
18/03/02 22:41:35 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:41:35 INFO Executor: Finished task 0.0 in stage 35.0 (TID 35). 846 bytes result sent to driver
18/03/02 22:41:35 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 35) in 16 ms on localhost (executor driver) (1/1)
18/03/02 22:41:35 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
18/03/02 22:41:35 INFO DAGScheduler: ResultStage 35 (foreach at SparkStreamingConsumer.scala:70) finished in 0.026 s
18/03/02 22:41:35 INFO DAGScheduler: Job 35 finished: foreach at SparkStreamingConsumer.scala:70, took 0.048241 s
18/03/02 22:41:35 INFO JobScheduler: Finished job streaming job 1520010695000 ms.1 from job set of time 1520010695000 ms
18/03/02 22:41:35 INFO JobScheduler: Total delay: 0.097 s for time 1520010695000 ms (execution: 0.091 s)
18/03/02 22:41:35 INFO KafkaRDD: Removing RDD 16 from persistence list
18/03/02 22:41:35 INFO BlockManager: Removing RDD 16
18/03/02 22:41:35 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:41:35 INFO InputInfoTracker: remove old batch metadata: 1520010685000 ms
18/03/02 22:41:40 INFO JobScheduler: Added jobs for time 1520010700000 ms
18/03/02 22:41:40 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:41:40 INFO JobScheduler: Starting job streaming job 1520010700000 ms.0 from job set of time 1520010700000 ms
18/03/02 22:41:40 INFO DAGScheduler: Got job 36 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:41:40 INFO DAGScheduler: Final stage: ResultStage 36 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:41:40 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:40 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:40 INFO DAGScheduler: Submitting ResultStage 36 (KafkaRDD[18] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:40 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:41:40 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.3 MB)
18/03/02 22:41:40 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:41:40 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (KafkaRDD[18] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:40 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks
18/03/02 22:41:40 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 36, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:41:40 INFO Executor: Running task 0.0 in stage 36.0 (TID 36)
18/03/02 22:41:40 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:41:40 INFO Executor: Finished task 0.0 in stage 36.0 (TID 36). 925 bytes result sent to driver
18/03/02 22:41:40 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 36) in 27 ms on localhost (executor driver) (1/1)
18/03/02 22:41:40 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
18/03/02 22:41:40 INFO DAGScheduler: ResultStage 36 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.028 s
18/03/02 22:41:40 INFO DAGScheduler: Job 36 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.072508 s
18/03/02 22:41:40 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:41:40 INFO DAGScheduler: Got job 37 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:41:40 INFO DAGScheduler: Final stage: ResultStage 37 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:41:40 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:40 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:40 INFO DAGScheduler: Submitting ResultStage 37 (KafkaRDD[18] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:40 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:41:40 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.3 MB)
18/03/02 22:41:40 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:41:40 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (KafkaRDD[18] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:40 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks
18/03/02 22:41:40 INFO JobScheduler: Finished job streaming job 1520010700000 ms.0 from job set of time 1520010700000 ms
18/03/02 22:41:40 INFO JobScheduler: Starting job streaming job 1520010700000 ms.1 from job set of time 1520010700000 ms
18/03/02 22:41:40 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 37, localhost, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes)
18/03/02 22:41:40 INFO Executor: Running task 0.0 in stage 37.0 (TID 37)
18/03/02 22:41:40 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:41:40 INFO Executor: Finished task 0.0 in stage 37.0 (TID 37). 925 bytes result sent to driver
18/03/02 22:41:40 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 37) in 12 ms on localhost (executor driver) (1/1)
18/03/02 22:41:40 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool 
18/03/02 22:41:40 INFO DAGScheduler: ResultStage 37 (foreach at SparkStreamingConsumer.scala:70) finished in 0.024 s
18/03/02 22:41:40 INFO DAGScheduler: Job 37 finished: foreach at SparkStreamingConsumer.scala:70, took 0.032453 s
18/03/02 22:41:40 INFO JobScheduler: Finished job streaming job 1520010700000 ms.1 from job set of time 1520010700000 ms
18/03/02 22:41:40 INFO JobScheduler: Total delay: 0.138 s for time 1520010700000 ms (execution: 0.120 s)
18/03/02 22:41:40 INFO KafkaRDD: Removing RDD 17 from persistence list
18/03/02 22:41:40 INFO BlockManager: Removing RDD 17
18/03/02 22:41:40 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:41:40 INFO InputInfoTracker: remove old batch metadata: 1520010690000 ms
18/03/02 22:41:45 INFO JobScheduler: Added jobs for time 1520010705000 ms
18/03/02 22:41:45 INFO JobScheduler: Starting job streaming job 1520010705000 ms.0 from job set of time 1520010705000 ms
18/03/02 22:41:45 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:41:45 INFO DAGScheduler: Got job 38 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:41:45 INFO DAGScheduler: Final stage: ResultStage 38 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:41:45 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:45 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:45 INFO DAGScheduler: Submitting ResultStage 38 (KafkaRDD[19] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:45 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:41:45 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.3 MB)
18/03/02 22:41:45 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:41:45 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (KafkaRDD[19] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:45 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks
18/03/02 22:41:45 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 38, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:41:45 INFO Executor: Running task 0.0 in stage 38.0 (TID 38)
18/03/02 22:41:45 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:41:45 INFO Executor: Finished task 0.0 in stage 38.0 (TID 38). 925 bytes result sent to driver
18/03/02 22:41:45 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 38) in 5 ms on localhost (executor driver) (1/1)
18/03/02 22:41:45 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
18/03/02 22:41:45 INFO DAGScheduler: ResultStage 38 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.005 s
18/03/02 22:41:45 INFO DAGScheduler: Job 38 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.012946 s
18/03/02 22:41:45 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:41:45 INFO JobScheduler: Finished job streaming job 1520010705000 ms.0 from job set of time 1520010705000 ms
18/03/02 22:41:45 INFO JobScheduler: Starting job streaming job 1520010705000 ms.1 from job set of time 1520010705000 ms
18/03/02 22:41:45 INFO DAGScheduler: Got job 39 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:41:45 INFO DAGScheduler: Final stage: ResultStage 39 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:41:45 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:45 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:45 INFO DAGScheduler: Submitting ResultStage 39 (KafkaRDD[19] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:45 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:41:45 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.3 MB)
18/03/02 22:41:45 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:41:45 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (KafkaRDD[19] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:45 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks
18/03/02 22:41:45 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 39, localhost, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes)
18/03/02 22:41:45 INFO Executor: Running task 0.0 in stage 39.0 (TID 39)
18/03/02 22:41:45 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:41:45 INFO Executor: Finished task 0.0 in stage 39.0 (TID 39). 846 bytes result sent to driver
18/03/02 22:41:45 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 39) in 6 ms on localhost (executor driver) (1/1)
18/03/02 22:41:45 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
18/03/02 22:41:45 INFO DAGScheduler: ResultStage 39 (foreach at SparkStreamingConsumer.scala:70) finished in 0.011 s
18/03/02 22:41:45 INFO DAGScheduler: Job 39 finished: foreach at SparkStreamingConsumer.scala:70, took 0.020056 s
18/03/02 22:41:45 INFO JobScheduler: Finished job streaming job 1520010705000 ms.1 from job set of time 1520010705000 ms
18/03/02 22:41:45 INFO JobScheduler: Total delay: 0.053 s for time 1520010705000 ms (execution: 0.041 s)
18/03/02 22:41:45 INFO KafkaRDD: Removing RDD 18 from persistence list
18/03/02 22:41:45 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:41:45 INFO InputInfoTracker: remove old batch metadata: 1520010695000 ms
18/03/02 22:41:45 INFO BlockManager: Removing RDD 18
18/03/02 22:41:50 INFO JobScheduler: Added jobs for time 1520010710000 ms
18/03/02 22:41:50 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:41:50 INFO JobScheduler: Starting job streaming job 1520010710000 ms.0 from job set of time 1520010710000 ms
18/03/02 22:41:50 INFO DAGScheduler: Got job 40 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:41:50 INFO DAGScheduler: Final stage: ResultStage 40 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:41:50 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:50 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:50 INFO DAGScheduler: Submitting ResultStage 40 (KafkaRDD[20] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:50 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:41:50 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.3 MB)
18/03/02 22:41:50 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:41:50 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (KafkaRDD[20] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:50 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks
18/03/02 22:41:50 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 40, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:41:50 INFO Executor: Running task 0.0 in stage 40.0 (TID 40)
18/03/02 22:41:50 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:41:50 INFO Executor: Finished task 0.0 in stage 40.0 (TID 40). 925 bytes result sent to driver
18/03/02 22:41:50 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 40) in 7 ms on localhost (executor driver) (1/1)
18/03/02 22:41:50 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
18/03/02 22:41:50 INFO DAGScheduler: ResultStage 40 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.016 s
18/03/02 22:41:50 INFO DAGScheduler: Job 40 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.037549 s
18/03/02 22:41:50 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:41:50 INFO JobScheduler: Finished job streaming job 1520010710000 ms.0 from job set of time 1520010710000 ms
18/03/02 22:41:50 INFO JobScheduler: Starting job streaming job 1520010710000 ms.1 from job set of time 1520010710000 ms
18/03/02 22:41:50 INFO DAGScheduler: Got job 41 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:41:50 INFO DAGScheduler: Final stage: ResultStage 41 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:41:50 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:50 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:50 INFO DAGScheduler: Submitting ResultStage 41 (KafkaRDD[20] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:50 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:41:50 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.3 MB)
18/03/02 22:41:50 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:41:50 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (KafkaRDD[20] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:50 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks
18/03/02 22:41:50 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 41, localhost, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes)
18/03/02 22:41:50 INFO Executor: Running task 0.0 in stage 41.0 (TID 41)
18/03/02 22:41:50 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:41:50 INFO Executor: Finished task 0.0 in stage 41.0 (TID 41). 846 bytes result sent to driver
18/03/02 22:41:50 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 41) in 12 ms on localhost (executor driver) (1/1)
18/03/02 22:41:50 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
18/03/02 22:41:50 INFO DAGScheduler: ResultStage 41 (foreach at SparkStreamingConsumer.scala:70) finished in 0.017 s
18/03/02 22:41:50 INFO DAGScheduler: Job 41 finished: foreach at SparkStreamingConsumer.scala:70, took 0.040676 s
18/03/02 22:41:50 INFO JobScheduler: Finished job streaming job 1520010710000 ms.1 from job set of time 1520010710000 ms
18/03/02 22:41:50 INFO JobScheduler: Total delay: 0.102 s for time 1520010710000 ms (execution: 0.084 s)
18/03/02 22:41:50 INFO KafkaRDD: Removing RDD 19 from persistence list
18/03/02 22:41:50 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:41:50 INFO InputInfoTracker: remove old batch metadata: 1520010700000 ms
18/03/02 22:41:50 INFO BlockManager: Removing RDD 19
18/03/02 22:41:55 INFO JobScheduler: Added jobs for time 1520010715000 ms
18/03/02 22:41:55 INFO JobScheduler: Starting job streaming job 1520010715000 ms.0 from job set of time 1520010715000 ms
18/03/02 22:41:55 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:41:55 INFO DAGScheduler: Got job 42 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:41:55 INFO DAGScheduler: Final stage: ResultStage 42 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:41:55 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:55 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:55 INFO DAGScheduler: Submitting ResultStage 42 (KafkaRDD[21] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:55 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:41:55 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.3 MB)
18/03/02 22:41:55 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:41:55 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (KafkaRDD[21] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:55 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks
18/03/02 22:41:55 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 42, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:41:55 INFO Executor: Running task 0.0 in stage 42.0 (TID 42)
18/03/02 22:41:55 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:41:55 INFO Executor: Finished task 0.0 in stage 42.0 (TID 42). 846 bytes result sent to driver
18/03/02 22:41:55 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 42) in 18 ms on localhost (executor driver) (1/1)
18/03/02 22:41:55 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
18/03/02 22:41:55 INFO DAGScheduler: ResultStage 42 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.021 s
18/03/02 22:41:55 INFO DAGScheduler: Job 42 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.037897 s
18/03/02 22:41:55 INFO JobScheduler: Finished job streaming job 1520010715000 ms.0 from job set of time 1520010715000 ms
18/03/02 22:41:55 INFO JobScheduler: Starting job streaming job 1520010715000 ms.1 from job set of time 1520010715000 ms
18/03/02 22:41:55 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:41:55 INFO DAGScheduler: Got job 43 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:41:55 INFO DAGScheduler: Final stage: ResultStage 43 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:41:55 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:41:55 INFO DAGScheduler: Missing parents: List()
18/03/02 22:41:55 INFO DAGScheduler: Submitting ResultStage 43 (KafkaRDD[21] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:41:55 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:41:55 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.3 MB)
18/03/02 22:41:55 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:41:55 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:996
18/03/02 22:41:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 43 (KafkaRDD[21] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:41:55 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks
18/03/02 22:41:55 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 43, localhost, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes)
18/03/02 22:41:55 INFO Executor: Running task 0.0 in stage 43.0 (TID 43)
18/03/02 22:41:55 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:41:55 INFO Executor: Finished task 0.0 in stage 43.0 (TID 43). 925 bytes result sent to driver
18/03/02 22:41:55 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 43) in 6 ms on localhost (executor driver) (1/1)
18/03/02 22:41:55 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool 
18/03/02 22:41:55 INFO DAGScheduler: ResultStage 43 (foreach at SparkStreamingConsumer.scala:70) finished in 0.008 s
18/03/02 22:41:55 INFO DAGScheduler: Job 43 finished: foreach at SparkStreamingConsumer.scala:70, took 0.016197 s
18/03/02 22:41:55 INFO JobScheduler: Finished job streaming job 1520010715000 ms.1 from job set of time 1520010715000 ms
18/03/02 22:41:55 INFO JobScheduler: Total delay: 0.092 s for time 1520010715000 ms (execution: 0.071 s)
18/03/02 22:41:55 INFO KafkaRDD: Removing RDD 20 from persistence list
18/03/02 22:41:55 INFO BlockManager: Removing RDD 20
18/03/02 22:41:55 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:41:55 INFO InputInfoTracker: remove old batch metadata: 1520010705000 ms
18/03/02 22:42:00 INFO JobScheduler: Added jobs for time 1520010720000 ms
18/03/02 22:42:00 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:42:00 INFO JobScheduler: Starting job streaming job 1520010720000 ms.0 from job set of time 1520010720000 ms
18/03/02 22:42:00 INFO DAGScheduler: Got job 44 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:42:00 INFO DAGScheduler: Final stage: ResultStage 44 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:42:00 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:42:00 INFO DAGScheduler: Missing parents: List()
18/03/02 22:42:00 INFO DAGScheduler: Submitting ResultStage 44 (KafkaRDD[22] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:42:00 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 22:42:00 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.2 MB)
18/03/02 22:42:00 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:42:00 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:996
18/03/02 22:42:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (KafkaRDD[22] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:42:00 INFO TaskSchedulerImpl: Adding task set 44.0 with 1 tasks
18/03/02 22:42:00 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 44, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:42:00 INFO Executor: Running task 0.0 in stage 44.0 (TID 44)
18/03/02 22:42:00 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:42:00 INFO Executor: Finished task 0.0 in stage 44.0 (TID 44). 846 bytes result sent to driver
18/03/02 22:42:00 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 44) in 5 ms on localhost (executor driver) (1/1)
18/03/02 22:42:00 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool 
18/03/02 22:42:00 INFO DAGScheduler: ResultStage 44 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.008 s
18/03/02 22:42:00 INFO DAGScheduler: Job 44 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.020936 s
18/03/02 22:42:00 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:42:00 INFO JobScheduler: Finished job streaming job 1520010720000 ms.0 from job set of time 1520010720000 ms
18/03/02 22:42:00 INFO JobScheduler: Starting job streaming job 1520010720000 ms.1 from job set of time 1520010720000 ms
18/03/02 22:42:00 INFO DAGScheduler: Got job 45 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:42:00 INFO DAGScheduler: Final stage: ResultStage 45 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:42:00 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:42:00 INFO DAGScheduler: Missing parents: List()
18/03/02 22:42:00 INFO DAGScheduler: Submitting ResultStage 45 (KafkaRDD[22] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:42:00 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 2.4 KB, free 366.2 MB)
18/03/02 22:42:00 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.2 MB)
18/03/02 22:42:00 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:42:00 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:996
18/03/02 22:42:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (KafkaRDD[22] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:42:00 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks
18/03/02 22:42:00 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 45, localhost, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes)
18/03/02 22:42:00 INFO Executor: Running task 0.0 in stage 45.0 (TID 45)
18/03/02 22:42:00 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:42:00 INFO Executor: Finished task 0.0 in stage 45.0 (TID 45). 925 bytes result sent to driver
18/03/02 22:42:00 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 45) in 6 ms on localhost (executor driver) (1/1)
18/03/02 22:42:00 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
18/03/02 22:42:00 INFO DAGScheduler: ResultStage 45 (foreach at SparkStreamingConsumer.scala:70) finished in 0.017 s
18/03/02 22:42:00 INFO DAGScheduler: Job 45 finished: foreach at SparkStreamingConsumer.scala:70, took 0.043674 s
18/03/02 22:42:00 INFO JobScheduler: Finished job streaming job 1520010720000 ms.1 from job set of time 1520010720000 ms
18/03/02 22:42:00 INFO JobScheduler: Total delay: 0.081 s for time 1520010720000 ms (execution: 0.069 s)
18/03/02 22:42:00 INFO KafkaRDD: Removing RDD 21 from persistence list
18/03/02 22:42:00 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:42:00 INFO InputInfoTracker: remove old batch metadata: 1520010710000 ms
18/03/02 22:42:00 INFO BlockManager: Removing RDD 21
18/03/02 22:42:05 INFO JobScheduler: Added jobs for time 1520010725000 ms
18/03/02 22:42:05 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:42:05 INFO JobScheduler: Starting job streaming job 1520010725000 ms.0 from job set of time 1520010725000 ms
18/03/02 22:42:05 INFO DAGScheduler: Got job 46 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:42:05 INFO DAGScheduler: Final stage: ResultStage 46 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:42:05 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:42:05 INFO DAGScheduler: Missing parents: List()
18/03/02 22:42:05 INFO DAGScheduler: Submitting ResultStage 46 (KafkaRDD[23] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:42:05 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 2.4 KB, free 366.2 MB)
18/03/02 22:42:05 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.2 MB)
18/03/02 22:42:05 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:42:05 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:996
18/03/02 22:42:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (KafkaRDD[23] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:42:05 INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks
18/03/02 22:42:05 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 46, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:42:05 INFO Executor: Running task 0.0 in stage 46.0 (TID 46)
18/03/02 22:42:05 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 25 25
18/03/02 22:42:05 INFO Executor: Finished task 0.0 in stage 46.0 (TID 46). 925 bytes result sent to driver
18/03/02 22:42:05 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 46) in 5 ms on localhost (executor driver) (1/1)
18/03/02 22:42:05 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool 
18/03/02 22:42:05 INFO DAGScheduler: ResultStage 46 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.007 s
18/03/02 22:42:05 INFO DAGScheduler: Job 46 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.030720 s
18/03/02 22:42:05 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:42:05 INFO JobScheduler: Finished job streaming job 1520010725000 ms.0 from job set of time 1520010725000 ms
18/03/02 22:42:05 INFO JobScheduler: Starting job streaming job 1520010725000 ms.1 from job set of time 1520010725000 ms
18/03/02 22:42:05 INFO DAGScheduler: Got job 47 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:42:05 INFO DAGScheduler: Final stage: ResultStage 47 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:42:05 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:42:05 INFO DAGScheduler: Missing parents: List()
18/03/02 22:42:05 INFO DAGScheduler: Submitting ResultStage 47 (KafkaRDD[23] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:42:05 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 2.4 KB, free 366.2 MB)
18/03/02 22:42:05 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.2 MB)
18/03/02 22:42:05 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:42:05 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:996
18/03/02 22:42:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 47 (KafkaRDD[23] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:42:05 INFO TaskSchedulerImpl: Adding task set 47.0 with 1 tasks
18/03/02 22:42:05 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 47, localhost, executor driver, partition 0, PROCESS_LOCAL, 7555 bytes)
18/03/02 22:42:05 INFO Executor: Running task 0.0 in stage 47.0 (TID 47)
18/03/02 22:42:05 INFO KafkaRDD: Beginning offset 25 is the same as ending offset skipping kafka_topic 0
18/03/02 22:42:05 INFO Executor: Finished task 0.0 in stage 47.0 (TID 47). 925 bytes result sent to driver
18/03/02 22:42:05 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 47) in 6 ms on localhost (executor driver) (1/1)
18/03/02 22:42:05 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool 
18/03/02 22:42:05 INFO DAGScheduler: ResultStage 47 (foreach at SparkStreamingConsumer.scala:70) finished in 0.012 s
18/03/02 22:42:05 INFO DAGScheduler: Job 47 finished: foreach at SparkStreamingConsumer.scala:70, took 0.027641 s
18/03/02 22:42:05 INFO JobScheduler: Finished job streaming job 1520010725000 ms.1 from job set of time 1520010725000 ms
18/03/02 22:42:05 INFO JobScheduler: Total delay: 0.080 s for time 1520010725000 ms (execution: 0.066 s)
18/03/02 22:42:05 INFO KafkaRDD: Removing RDD 22 from persistence list
18/03/02 22:42:05 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:42:05 INFO InputInfoTracker: remove old batch metadata: 1520010715000 ms
18/03/02 22:42:05 INFO BlockManager: Removing RDD 22
18/03/02 22:42:10 INFO JobScheduler: Added jobs for time 1520010730000 ms
18/03/02 22:42:10 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:42:10 INFO DAGScheduler: Got job 48 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:42:10 INFO DAGScheduler: Final stage: ResultStage 48 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:42:10 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:42:10 INFO DAGScheduler: Missing parents: List()
18/03/02 22:42:10 INFO DAGScheduler: Submitting ResultStage 48 (KafkaRDD[24] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:42:10 INFO JobScheduler: Starting job streaming job 1520010730000 ms.0 from job set of time 1520010730000 ms
18/03/02 22:42:10 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 2.4 KB, free 366.2 MB)
18/03/02 22:42:10 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.2 MB)
18/03/02 22:42:10 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:42:10 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:996
18/03/02 22:42:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 48 (KafkaRDD[24] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:42:10 INFO TaskSchedulerImpl: Adding task set 48.0 with 1 tasks
18/03/02 22:42:10 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 48, localhost, executor driver, partition 0, PROCESS_LOCAL, 7564 bytes)
18/03/02 22:42:10 INFO Executor: Running task 0.0 in stage 48.0 (TID 48)
18/03/02 22:42:10 INFO KafkaRDD: Computing topic kafka_topic, partition 0 offsets 25 -> 35
kafka_topic 0 25 35
18/03/02 22:42:10 INFO Executor: Finished task 0.0 in stage 48.0 (TID 48). 925 bytes result sent to driver
18/03/02 22:42:10 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 48) in 8 ms on localhost (executor driver) (1/1)
18/03/02 22:42:10 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool 
18/03/02 22:42:10 INFO DAGScheduler: ResultStage 48 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.010 s
18/03/02 22:42:10 INFO DAGScheduler: Job 48 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.037865 s
18/03/02 22:42:10 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:42:10 INFO JobScheduler: Finished job streaming job 1520010730000 ms.0 from job set of time 1520010730000 ms
18/03/02 22:42:10 INFO JobScheduler: Starting job streaming job 1520010730000 ms.1 from job set of time 1520010730000 ms
18/03/02 22:42:10 INFO DAGScheduler: Got job 49 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:42:10 INFO DAGScheduler: Final stage: ResultStage 49 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:42:10 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:42:10 INFO DAGScheduler: Missing parents: List()
18/03/02 22:42:10 INFO DAGScheduler: Submitting ResultStage 49 (KafkaRDD[24] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:42:10 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 2.4 KB, free 366.2 MB)
18/03/02 22:42:10 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.2 MB)
18/03/02 22:42:10 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:42:10 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:996
18/03/02 22:42:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (KafkaRDD[24] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:42:10 INFO TaskSchedulerImpl: Adding task set 49.0 with 1 tasks
18/03/02 22:42:10 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 49, localhost, executor driver, partition 0, PROCESS_LOCAL, 7556 bytes)
18/03/02 22:42:10 INFO Executor: Running task 0.0 in stage 49.0 (TID 49)
18/03/02 22:42:10 INFO KafkaRDD: Computing topic kafka_topic, partition 0 offsets 25 -> 35
null,0,115610216,0,115610216
null,1000,1838683,1000,1838683
null,10000,335707,10000,335707
null,10001,58524,10001,58524
null,10003,200739,10003,200739
null,10005,76444,10005,76444
null,1001,20071,1001,20071
null,1003,73283,1003,73283
null,1005,9200,1005,9200
null,1007,7091,1007,7091
18/03/02 22:42:10 INFO Executor: Finished task 0.0 in stage 49.0 (TID 49). 925 bytes result sent to driver
18/03/02 22:42:10 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 49) in 16 ms on localhost (executor driver) (1/1)
18/03/02 22:42:10 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool 
18/03/02 22:42:10 INFO DAGScheduler: ResultStage 49 (foreach at SparkStreamingConsumer.scala:70) finished in 0.017 s
18/03/02 22:42:10 INFO DAGScheduler: Job 49 finished: foreach at SparkStreamingConsumer.scala:70, took 0.034182 s
18/03/02 22:42:10 INFO JobScheduler: Finished job streaming job 1520010730000 ms.1 from job set of time 1520010730000 ms
18/03/02 22:42:10 INFO JobScheduler: Total delay: 0.086 s for time 1520010730000 ms (execution: 0.057 s)
18/03/02 22:42:10 INFO KafkaRDD: Removing RDD 23 from persistence list
18/03/02 22:42:10 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:42:10 INFO InputInfoTracker: remove old batch metadata: 1520010720000 ms
18/03/02 22:42:10 INFO BlockManager: Removing RDD 23
18/03/02 22:42:15 INFO JobScheduler: Added jobs for time 1520010735000 ms
18/03/02 22:42:15 INFO JobScheduler: Starting job streaming job 1520010735000 ms.0 from job set of time 1520010735000 ms
18/03/02 22:42:15 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 22:42:15 INFO DAGScheduler: Got job 50 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 22:42:15 INFO DAGScheduler: Final stage: ResultStage 50 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 22:42:15 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:42:15 INFO DAGScheduler: Missing parents: List()
18/03/02 22:42:15 INFO DAGScheduler: Submitting ResultStage 50 (KafkaRDD[25] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:42:15 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 2.4 KB, free 366.2 MB)
18/03/02 22:42:15 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.2 MB)
18/03/02 22:42:15 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 10.0.2.15:39100 (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:42:15 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:996
18/03/02 22:42:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 50 (KafkaRDD[25] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:42:15 INFO TaskSchedulerImpl: Adding task set 50.0 with 1 tasks
18/03/02 22:42:15 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 50, localhost, executor driver, partition 0, PROCESS_LOCAL, 7565 bytes)
18/03/02 22:42:15 INFO Executor: Running task 0.0 in stage 50.0 (TID 50)
18/03/02 22:42:15 INFO KafkaRDD: Beginning offset 35 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 35 35
18/03/02 22:42:15 INFO Executor: Finished task 0.0 in stage 50.0 (TID 50). 925 bytes result sent to driver
18/03/02 22:42:15 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 50) in 16 ms on localhost (executor driver) (1/1)
18/03/02 22:42:15 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool 
18/03/02 22:42:15 INFO DAGScheduler: ResultStage 50 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.016 s
18/03/02 22:42:15 INFO DAGScheduler: Job 50 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.035136 s
18/03/02 22:42:15 INFO JobScheduler: Finished job streaming job 1520010735000 ms.0 from job set of time 1520010735000 ms
18/03/02 22:42:15 INFO JobScheduler: Starting job streaming job 1520010735000 ms.1 from job set of time 1520010735000 ms
18/03/02 22:42:15 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 22:42:15 INFO DAGScheduler: Got job 51 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 22:42:15 INFO DAGScheduler: Final stage: ResultStage 51 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 22:42:15 INFO DAGScheduler: Parents of final stage: List()
18/03/02 22:42:15 INFO DAGScheduler: Missing parents: List()
18/03/02 22:42:15 INFO DAGScheduler: Submitting ResultStage 51 (KafkaRDD[25] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 22:42:15 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 2.4 KB, free 366.2 MB)
18/03/02 22:42:15 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.2 MB)
18/03/02 22:42:15 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 10.0.2.15:39100 (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:42:15 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:996
18/03/02 22:42:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 51 (KafkaRDD[25] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 22:42:15 INFO TaskSchedulerImpl: Adding task set 51.0 with 1 tasks
18/03/02 22:42:15 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 51, localhost, executor driver, partition 0, PROCESS_LOCAL, 7556 bytes)
18/03/02 22:42:15 INFO Executor: Running task 0.0 in stage 51.0 (TID 51)
18/03/02 22:42:15 INFO KafkaRDD: Beginning offset 35 is the same as ending offset skipping kafka_topic 0
18/03/02 22:42:15 INFO Executor: Finished task 0.0 in stage 51.0 (TID 51). 846 bytes result sent to driver
18/03/02 22:42:15 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 51) in 7 ms on localhost (executor driver) (1/1)
18/03/02 22:42:15 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool 
18/03/02 22:42:15 INFO DAGScheduler: ResultStage 51 (foreach at SparkStreamingConsumer.scala:70) finished in 0.008 s
18/03/02 22:42:15 INFO DAGScheduler: Job 51 finished: foreach at SparkStreamingConsumer.scala:70, took 0.016386 s
18/03/02 22:42:15 INFO JobScheduler: Finished job streaming job 1520010735000 ms.1 from job set of time 1520010735000 ms
18/03/02 22:42:15 INFO JobScheduler: Total delay: 0.066 s for time 1520010735000 ms (execution: 0.062 s)
18/03/02 22:42:15 INFO KafkaRDD: Removing RDD 24 from persistence list
18/03/02 22:42:15 INFO BlockManager: Removing RDD 24
18/03/02 22:42:15 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 22:42:15 INFO InputInfoTracker: remove old batch metadata: 1520010725000 ms
^C18/03/02 22:42:18 INFO StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
18/03/02 22:42:18 INFO ReceiverTracker: ReceiverTracker stopped
18/03/02 22:42:18 INFO JobGenerator: Stopping JobGenerator immediately
18/03/02 22:42:18 INFO RecurringTimer: Stopped timer for JobGenerator after time 1520010735000
18/03/02 22:42:18 INFO JobGenerator: Stopped JobGenerator
18/03/02 22:42:18 INFO JobScheduler: Stopped JobScheduler
18/03/02 22:42:18 INFO StreamingContext: StreamingContext stopped successfully
18/03/02 22:42:18 INFO SparkContext: Invoking stop() from shutdown hook
18/03/02 22:42:18 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 10.0.2.15:39100 in memory (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:42:18 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 10.0.2.15:39100 in memory (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:42:18 INFO BlockManagerInfo: Removed broadcast_46_piece0 on 10.0.2.15:39100 in memory (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:42:18 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 10.0.2.15:39100 in memory (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:42:18 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 10.0.2.15:39100 in memory (size: 1613.0 B, free: 366.3 MB)
18/03/02 22:42:18 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 10.0.2.15:39100 in memory (size: 1630.0 B, free: 366.3 MB)
18/03/02 22:42:18 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
18/03/02 22:42:18 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 10.0.2.15:39100 in memory (size: 1614.0 B, free: 366.3 MB)
18/03/02 22:42:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/03/02 22:42:18 INFO MemoryStore: MemoryStore cleared
18/03/02 22:42:18 INFO BlockManager: BlockManager stopped
18/03/02 22:42:18 INFO BlockManagerMaster: BlockManagerMaster stopped
18/03/02 22:42:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/03/02 22:42:18 INFO SparkContext: Successfully stopped SparkContext
18/03/02 22:42:18 INFO ShutdownHookManager: Shutdown hook called
18/03/02 22:42:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-b21a009b-ec0b-4c3e-a073-0bfaece00b10
----

[edureka@localhost sparkjobs]$ spark-submit --verbose --master local --deploy-mode client --conf spark.driver.extraClassPath=/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/mysql-connector-java-5.1.43-bin.jar:/usr/lib/kafka_2.12-0.11.0.0/libs/kafka-clients-0.11.0.0.jar --repositories https://repo.maven.apache.org/maven2 --packages org.apache.spark:spark-streaming-kafka-0-10_2.11:2.0.0 --class com.laboros.spark.sql.stream.SparkStreamingConsumer sparkdemo_2.11-0.1.0-SNAPSHOT.jar kafka_topic
Using properties file: /usr/lib/spark-2.1.1-bin-hadoop2.7/conf/spark-defaults.conf
Adding default property: spark.master=spark://localhost:7077
Parsed arguments:
  master                  local
  deployMode              client
  executorMemory          null
  executorCores           null
  totalExecutorCores      null
  propertiesFile          /usr/lib/spark-2.1.1-bin-hadoop2.7/conf/spark-defaults.conf
  driverMemory            null
  driverCores             null
  driverExtraClassPath    /home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/mysql-connector-java-5.1.43-bin.jar:/usr/lib/kafka_2.12-0.11.0.0/libs/kafka-clients-0.11.0.0.jar
  driverExtraLibraryPath  null
  driverExtraJavaOptions  null
  supervise               false
  queue                   null
  numExecutors            null
  files                   null
  pyFiles                 null
  archives                null
  mainClass               com.laboros.spark.sql.stream.SparkStreamingConsumer
  primaryResource         file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar
  name                    com.laboros.spark.sql.stream.SparkStreamingConsumer
  childArgs               [kafka_topic]
  jars                    null
  packages                org.apache.spark:spark-streaming-kafka-0-10_2.11:2.0.0
  packagesExclusions      null
  repositories            https://repo.maven.apache.org/maven2
  verbose                 true

Spark properties used, including those specified through
 --conf and those from the properties file /usr/lib/spark-2.1.1-bin-hadoop2.7/conf/spark-defaults.conf:
  spark.master -> spark://localhost:7077
  spark.driver.extraClassPath -> /home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/mysql-connector-java-5.1.43-bin.jar:/usr/lib/kafka_2.12-0.11.0.0/libs/kafka-clients-0.11.0.0.jar

    
Ivy Default Cache set to: /home/edureka/.ivy2/cache
The jars for the packages stored in: /home/edureka/.ivy2/jars
https://repo.maven.apache.org/maven2 added as a remote repository with the name: repo-1
:: loading settings :: url = jar:file:/usr/lib/spark-2.1.1-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
org.apache.spark#spark-streaming-kafka-0-10_2.11 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
	confs: [default]
	found org.apache.spark#spark-streaming-kafka-0-10_2.11;2.0.0 in repo-1
	found org.apache.kafka#kafka_2.11;0.10.0.0 in repo-1
	found com.101tec#zkclient;0.8 in repo-1
	found org.slf4j#slf4j-api;1.7.16 in repo-1
	found org.slf4j#slf4j-log4j12;1.7.16 in repo-1
	found log4j#log4j;1.2.17 in repo-1
	found com.yammer.metrics#metrics-core;2.2.0 in repo-1
	found org.scala-lang.modules#scala-parser-combinators_2.11;1.0.4 in repo-1
	found org.apache.kafka#kafka-clients;0.10.0.0 in repo-1
	found net.jpountz.lz4#lz4;1.3.0 in repo-1
	found org.xerial.snappy#snappy-java;1.1.2.4 in repo-1
	found org.apache.spark#spark-tags_2.11;2.0.0 in repo-1
	found org.scalatest#scalatest_2.11;2.2.6 in repo-1
	found org.scala-lang#scala-reflect;2.11.8 in repo-1
	found org.scala-lang.modules#scala-xml_2.11;1.0.2 in repo-1
	found org.spark-project.spark#unused;1.0.0 in repo-1
:: resolution report :: resolve 1954ms :: artifacts dl 23ms
	:: modules in use:
	com.101tec#zkclient;0.8 from repo-1 in [default]
	com.yammer.metrics#metrics-core;2.2.0 from repo-1 in [default]
	log4j#log4j;1.2.17 from repo-1 in [default]
	net.jpountz.lz4#lz4;1.3.0 from repo-1 in [default]
	org.apache.kafka#kafka-clients;0.10.0.0 from repo-1 in [default]
	org.apache.kafka#kafka_2.11;0.10.0.0 from repo-1 in [default]
	org.apache.spark#spark-streaming-kafka-0-10_2.11;2.0.0 from repo-1 in [default]
	org.apache.spark#spark-tags_2.11;2.0.0 from repo-1 in [default]
	org.scala-lang#scala-reflect;2.11.8 from repo-1 in [default]
	org.scala-lang.modules#scala-parser-combinators_2.11;1.0.4 from repo-1 in [default]
	org.scala-lang.modules#scala-xml_2.11;1.0.2 from repo-1 in [default]
	org.scalatest#scalatest_2.11;2.2.6 from repo-1 in [default]
	org.slf4j#slf4j-api;1.7.16 from repo-1 in [default]
	org.slf4j#slf4j-log4j12;1.7.16 from repo-1 in [default]
	org.spark-project.spark#unused;1.0.0 from repo-1 in [default]
	org.xerial.snappy#snappy-java;1.1.2.4 from repo-1 in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   16  |   0   |   0   |   0   ||   16  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent
	confs: [default]
	0 artifacts copied, 16 already retrieved (0kB/70ms)
Main class:
com.laboros.spark.sql.stream.SparkStreamingConsumer
Arguments:
kafka_topic
System properties:
SPARK_SUBMIT -> true
spark.app.name -> com.laboros.spark.sql.stream.SparkStreamingConsumer
spark.jars -> file:/home/edureka/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-10_2.11-2.0.0.jar,file:/home/edureka/.ivy2/jars/org.apache.kafka_kafka_2.11-0.10.0.0.jar,file:/home/edureka/.ivy2/jars/org.apache.spark_spark-tags_2.11-2.0.0.jar,file:/home/edureka/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar,file:/home/edureka/.ivy2/jars/com.101tec_zkclient-0.8.jar,file:/home/edureka/.ivy2/jars/org.slf4j_slf4j-log4j12-1.7.16.jar,file:/home/edureka/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar,file:/home/edureka/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.0.4.jar,file:/home/edureka/.ivy2/jars/org.apache.kafka_kafka-clients-0.10.0.0.jar,file:/home/edureka/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar,file:/home/edureka/.ivy2/jars/log4j_log4j-1.2.17.jar,file:/home/edureka/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar,file:/home/edureka/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.2.4.jar,file:/home/edureka/.ivy2/jars/org.scalatest_scalatest_2.11-2.2.6.jar,file:/home/edureka/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar,file:/home/edureka/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar,file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar
spark.submit.deployMode -> client
spark.master -> local
spark.driver.extraClassPath -> /home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/mysql-connector-java-5.1.43-bin.jar:/usr/lib/kafka_2.12-0.11.0.0/libs/kafka-clients-0.11.0.0.jar
Classpath elements:
file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar
/home/edureka/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-10_2.11-2.0.0.jar
/home/edureka/.ivy2/jars/org.apache.kafka_kafka_2.11-0.10.0.0.jar
/home/edureka/.ivy2/jars/org.apache.spark_spark-tags_2.11-2.0.0.jar
/home/edureka/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar
/home/edureka/.ivy2/jars/com.101tec_zkclient-0.8.jar
/home/edureka/.ivy2/jars/org.slf4j_slf4j-log4j12-1.7.16.jar
/home/edureka/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar
/home/edureka/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.0.4.jar
/home/edureka/.ivy2/jars/org.apache.kafka_kafka-clients-0.10.0.0.jar
/home/edureka/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar
/home/edureka/.ivy2/jars/log4j_log4j-1.2.17.jar
/home/edureka/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar
/home/edureka/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.2.4.jar
/home/edureka/.ivy2/jars/org.scalatest_scalatest_2.11-2.2.6.jar
/home/edureka/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar
/home/edureka/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar


Current Topic Name kafka_topic
18/03/02 23:06:51 INFO SparkContext: Running Spark version 2.1.1
18/03/02 23:06:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/03/02 23:06:52 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 10.0.2.15 instead (on interface enp0s3)
18/03/02 23:06:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/03/02 23:06:52 INFO SecurityManager: Changing view acls to: edureka
18/03/02 23:06:52 INFO SecurityManager: Changing modify acls to: edureka
18/03/02 23:06:52 INFO SecurityManager: Changing view acls groups to: 
18/03/02 23:06:52 INFO SecurityManager: Changing modify acls groups to: 
18/03/02 23:06:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(edureka); groups with view permissions: Set(); users  with modify permissions: Set(edureka); groups with modify permissions: Set()
18/03/02 23:06:52 INFO Utils: Successfully started service 'sparkDriver' on port 36275.
18/03/02 23:06:52 INFO SparkEnv: Registering MapOutputTracker
18/03/02 23:06:52 INFO SparkEnv: Registering BlockManagerMaster
18/03/02 23:06:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/03/02 23:06:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/03/02 23:06:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1fc7e8a7-a798-46db-a8c8-905127d5b11a
18/03/02 23:06:53 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
18/03/02 23:06:53 INFO SparkEnv: Registering OutputCommitCoordinator
18/03/02 23:06:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/03/02 23:06:53 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4040
18/03/02 23:06:53 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.apache.spark_spark-streaming-kafka-0-10_2.11-2.0.0.jar at spark://10.0.2.15:36275/jars/org.apache.spark_spark-streaming-kafka-0-10_2.11-2.0.0.jar with timestamp 1520012213489
18/03/02 23:06:53 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.apache.kafka_kafka_2.11-0.10.0.0.jar at spark://10.0.2.15:36275/jars/org.apache.kafka_kafka_2.11-0.10.0.0.jar with timestamp 1520012213490
18/03/02 23:06:53 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.apache.spark_spark-tags_2.11-2.0.0.jar at spark://10.0.2.15:36275/jars/org.apache.spark_spark-tags_2.11-2.0.0.jar with timestamp 1520012213491
18/03/02 23:06:53 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://10.0.2.15:36275/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1520012213491
18/03/02 23:06:53 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/com.101tec_zkclient-0.8.jar at spark://10.0.2.15:36275/jars/com.101tec_zkclient-0.8.jar with timestamp 1520012213491
18/03/02 23:06:53 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.slf4j_slf4j-log4j12-1.7.16.jar at spark://10.0.2.15:36275/jars/org.slf4j_slf4j-log4j12-1.7.16.jar with timestamp 1520012213491
18/03/02 23:06:53 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/com.yammer.metrics_metrics-core-2.2.0.jar at spark://10.0.2.15:36275/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1520012213491
18/03/02 23:06:53 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.0.4.jar at spark://10.0.2.15:36275/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.0.4.jar with timestamp 1520012213519
18/03/02 23:06:53 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.apache.kafka_kafka-clients-0.10.0.0.jar at spark://10.0.2.15:36275/jars/org.apache.kafka_kafka-clients-0.10.0.0.jar with timestamp 1520012213519
18/03/02 23:06:53 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://10.0.2.15:36275/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1520012213519
18/03/02 23:06:53 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/log4j_log4j-1.2.17.jar at spark://10.0.2.15:36275/jars/log4j_log4j-1.2.17.jar with timestamp 1520012213519
18/03/02 23:06:53 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar at spark://10.0.2.15:36275/jars/net.jpountz.lz4_lz4-1.3.0.jar with timestamp 1520012213519
18/03/02 23:06:53 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.2.4.jar at spark://10.0.2.15:36275/jars/org.xerial.snappy_snappy-java-1.1.2.4.jar with timestamp 1520012213520
18/03/02 23:06:53 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.scalatest_scalatest_2.11-2.2.6.jar at spark://10.0.2.15:36275/jars/org.scalatest_scalatest_2.11-2.2.6.jar with timestamp 1520012213520
18/03/02 23:06:53 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar at spark://10.0.2.15:36275/jars/org.scala-lang_scala-reflect-2.11.8.jar with timestamp 1520012213520
18/03/02 23:06:53 INFO SparkContext: Added JAR file:/home/edureka/.ivy2/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar at spark://10.0.2.15:36275/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1520012213520
18/03/02 23:06:53 INFO SparkContext: Added JAR file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar at spark://10.0.2.15:36275/jars/sparkdemo_2.11-0.1.0-SNAPSHOT.jar with timestamp 1520012213520
18/03/02 23:06:53 INFO Executor: Starting executor ID driver on host localhost
18/03/02 23:06:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34202.
18/03/02 23:06:53 INFO NettyBlockTransferService: Server created on 10.0.2.15:34202
18/03/02 23:06:53 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/03/02 23:06:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 34202, None)
18/03/02 23:06:53 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:34202 with 366.3 MB RAM, BlockManagerId(driver, 10.0.2.15, 34202, None)
18/03/02 23:06:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 34202, None)
18/03/02 23:06:53 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 34202, None)
18/03/02 23:06:54 WARN KafkaUtils: overriding enable.auto.commit to false for executor
18/03/02 23:06:54 WARN KafkaUtils: overriding auto.offset.reset to none for executor
18/03/02 23:06:54 WARN KafkaUtils: overriding executor group.id to spark-executor-use_a_separate_group_id_for_each_stream
18/03/02 23:06:54 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
18/03/02 23:06:54 INFO DirectKafkaInputDStream: Slide time = 5000 ms
18/03/02 23:06:54 INFO DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
18/03/02 23:06:54 INFO DirectKafkaInputDStream: Checkpoint interval = null
18/03/02 23:06:54 INFO DirectKafkaInputDStream: Remember interval = 5000 ms
18/03/02 23:06:54 INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@1fb50ad8
18/03/02 23:06:54 INFO ForEachDStream: Slide time = 5000 ms
18/03/02 23:06:54 INFO ForEachDStream: Storage level = Serialized 1x Replicated
18/03/02 23:06:54 INFO ForEachDStream: Checkpoint interval = null
18/03/02 23:06:54 INFO ForEachDStream: Remember interval = 5000 ms
18/03/02 23:06:54 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@164b222a
18/03/02 23:06:54 INFO DirectKafkaInputDStream: Slide time = 5000 ms
18/03/02 23:06:54 INFO DirectKafkaInputDStream: Storage level = Serialized 1x Replicated
18/03/02 23:06:54 INFO DirectKafkaInputDStream: Checkpoint interval = null
18/03/02 23:06:54 INFO DirectKafkaInputDStream: Remember interval = 5000 ms
18/03/02 23:06:54 INFO DirectKafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@1fb50ad8
18/03/02 23:06:54 INFO ForEachDStream: Slide time = 5000 ms
18/03/02 23:06:54 INFO ForEachDStream: Storage level = Serialized 1x Replicated
18/03/02 23:06:54 INFO ForEachDStream: Checkpoint interval = null
18/03/02 23:06:54 INFO ForEachDStream: Remember interval = 5000 ms
18/03/02 23:06:54 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@11e8406d
18/03/02 23:06:55 INFO ConsumerConfig: ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = use_a_separate_group_id_for_each_stream
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

18/03/02 23:06:55 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 23:06:55 INFO AppInfoParser: Kafka commitId : cb8625948210849f
18/03/02 23:06:55 INFO AbstractCoordinator: Discovered coordinator localhost:9092 (id: 2147483647 rack: null) for group use_a_separate_group_id_for_each_stream.
18/03/02 23:06:55 INFO ConsumerCoordinator: Revoking previously assigned partitions [] for group use_a_separate_group_id_for_each_stream
18/03/02 23:06:55 INFO AbstractCoordinator: (Re-)joining group use_a_separate_group_id_for_each_stream
18/03/02 23:06:55 INFO AbstractCoordinator: Successfully joined group use_a_separate_group_id_for_each_stream with generation 1
18/03/02 23:06:55 INFO ConsumerCoordinator: Setting newly assigned partitions [kafka_topic-0] for group use_a_separate_group_id_for_each_stream
18/03/02 23:06:55 INFO RecurringTimer: Started timer for JobGenerator at time 1520012215000
18/03/02 23:06:55 INFO JobGenerator: Started JobGenerator at 1520012215000 ms
18/03/02 23:06:55 INFO JobScheduler: Started JobScheduler
18/03/02 23:06:55 INFO StreamingContext: StreamingContext started
18/03/02 23:06:55 INFO JobScheduler: Added jobs for time 1520012215000 ms
18/03/02 23:06:55 INFO JobScheduler: Starting job streaming job 1520012215000 ms.0 from job set of time 1520012215000 ms
18/03/02 23:06:55 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 23:06:55 INFO DAGScheduler: Got job 0 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 23:06:55 INFO DAGScheduler: Final stage: ResultStage 0 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 23:06:55 INFO DAGScheduler: Parents of final stage: List()
18/03/02 23:06:55 INFO DAGScheduler: Missing parents: List()
18/03/02 23:06:55 INFO DAGScheduler: Submitting ResultStage 0 (KafkaRDD[0] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 23:06:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 23:06:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1621.0 B, free 366.3 MB)
18/03/02 23:06:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:34202 (size: 1621.0 B, free: 366.3 MB)
18/03/02 23:06:56 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:996
18/03/02 23:06:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (KafkaRDD[0] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 23:06:56 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
18/03/02 23:06:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7563 bytes)
18/03/02 23:06:56 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
18/03/02 23:06:56 INFO Executor: Fetching spark://10.0.2.15:36275/jars/org.slf4j_slf4j-log4j12-1.7.16.jar with timestamp 1520012213491
18/03/02 23:06:56 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:36275 after 52 ms (0 ms spent in bootstraps)
18/03/02 23:06:56 INFO Utils: Fetching spark://10.0.2.15:36275/jars/org.slf4j_slf4j-log4j12-1.7.16.jar to /tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/fetchFileTemp5465066888674537166.tmp
18/03/02 23:06:56 INFO Executor: Adding file:/tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/org.slf4j_slf4j-log4j12-1.7.16.jar to class loader
18/03/02 23:06:56 INFO Executor: Fetching spark://10.0.2.15:36275/jars/org.apache.spark_spark-streaming-kafka-0-10_2.11-2.0.0.jar with timestamp 1520012213489
18/03/02 23:06:56 INFO Utils: Fetching spark://10.0.2.15:36275/jars/org.apache.spark_spark-streaming-kafka-0-10_2.11-2.0.0.jar to /tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/fetchFileTemp8255879576105452399.tmp
18/03/02 23:06:56 INFO Executor: Adding file:/tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/org.apache.spark_spark-streaming-kafka-0-10_2.11-2.0.0.jar to class loader
18/03/02 23:06:56 INFO Executor: Fetching spark://10.0.2.15:36275/jars/sparkdemo_2.11-0.1.0-SNAPSHOT.jar with timestamp 1520012213520
18/03/02 23:06:56 INFO Utils: Fetching spark://10.0.2.15:36275/jars/sparkdemo_2.11-0.1.0-SNAPSHOT.jar to /tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/fetchFileTemp8659306271268605561.tmp
18/03/02 23:06:56 INFO Executor: Adding file:/tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/sparkdemo_2.11-0.1.0-SNAPSHOT.jar to class loader
18/03/02 23:06:56 INFO Executor: Fetching spark://10.0.2.15:36275/jars/log4j_log4j-1.2.17.jar with timestamp 1520012213519
18/03/02 23:06:56 INFO Utils: Fetching spark://10.0.2.15:36275/jars/log4j_log4j-1.2.17.jar to /tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/fetchFileTemp5122228076164920079.tmp
18/03/02 23:06:56 INFO Executor: Adding file:/tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/log4j_log4j-1.2.17.jar to class loader
18/03/02 23:06:56 INFO Executor: Fetching spark://10.0.2.15:36275/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1520012213491
18/03/02 23:06:56 INFO Utils: Fetching spark://10.0.2.15:36275/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/fetchFileTemp3189295450723641947.tmp
18/03/02 23:06:56 INFO Executor: Adding file:/tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/org.spark-project.spark_unused-1.0.0.jar to class loader
18/03/02 23:06:56 INFO Executor: Fetching spark://10.0.2.15:36275/jars/com.yammer.metrics_metrics-core-2.2.0.jar with timestamp 1520012213491
18/03/02 23:06:56 INFO Utils: Fetching spark://10.0.2.15:36275/jars/com.yammer.metrics_metrics-core-2.2.0.jar to /tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/fetchFileTemp3875012287210327985.tmp
18/03/02 23:06:56 INFO Executor: Adding file:/tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/com.yammer.metrics_metrics-core-2.2.0.jar to class loader
18/03/02 23:06:56 INFO Executor: Fetching spark://10.0.2.15:36275/jars/org.scala-lang_scala-reflect-2.11.8.jar with timestamp 1520012213520
18/03/02 23:06:56 INFO Utils: Fetching spark://10.0.2.15:36275/jars/org.scala-lang_scala-reflect-2.11.8.jar to /tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/fetchFileTemp1047386136147868639.tmp
18/03/02 23:06:56 INFO Executor: Adding file:/tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/org.scala-lang_scala-reflect-2.11.8.jar to class loader
18/03/02 23:06:56 INFO Executor: Fetching spark://10.0.2.15:36275/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar with timestamp 1520012213520
18/03/02 23:06:56 INFO Utils: Fetching spark://10.0.2.15:36275/jars/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to /tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/fetchFileTemp4977127228498161598.tmp
18/03/02 23:06:56 INFO Executor: Adding file:/tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/org.scala-lang.modules_scala-xml_2.11-1.0.2.jar to class loader
18/03/02 23:06:56 INFO Executor: Fetching spark://10.0.2.15:36275/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1520012213519
18/03/02 23:06:56 INFO Utils: Fetching spark://10.0.2.15:36275/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/fetchFileTemp1572919735431503124.tmp
18/03/02 23:06:57 INFO Executor: Adding file:/tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/org.slf4j_slf4j-api-1.7.16.jar to class loader
18/03/02 23:06:57 INFO Executor: Fetching spark://10.0.2.15:36275/jars/com.101tec_zkclient-0.8.jar with timestamp 1520012213491
18/03/02 23:06:57 INFO Utils: Fetching spark://10.0.2.15:36275/jars/com.101tec_zkclient-0.8.jar to /tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/fetchFileTemp8638248683542298840.tmp
18/03/02 23:06:57 INFO Executor: Adding file:/tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/com.101tec_zkclient-0.8.jar to class loader
18/03/02 23:06:57 INFO Executor: Fetching spark://10.0.2.15:36275/jars/net.jpountz.lz4_lz4-1.3.0.jar with timestamp 1520012213519
18/03/02 23:06:57 INFO Utils: Fetching spark://10.0.2.15:36275/jars/net.jpountz.lz4_lz4-1.3.0.jar to /tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/fetchFileTemp1124706602048999875.tmp
18/03/02 23:06:57 INFO Executor: Adding file:/tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/net.jpountz.lz4_lz4-1.3.0.jar to class loader
18/03/02 23:06:57 INFO Executor: Fetching spark://10.0.2.15:36275/jars/org.apache.spark_spark-tags_2.11-2.0.0.jar with timestamp 1520012213491
18/03/02 23:06:57 INFO Utils: Fetching spark://10.0.2.15:36275/jars/org.apache.spark_spark-tags_2.11-2.0.0.jar to /tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/fetchFileTemp1776473087246264188.tmp
18/03/02 23:06:57 INFO Executor: Adding file:/tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/org.apache.spark_spark-tags_2.11-2.0.0.jar to class loader
18/03/02 23:06:57 INFO Executor: Fetching spark://10.0.2.15:36275/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.0.4.jar with timestamp 1520012213519
18/03/02 23:06:57 INFO Utils: Fetching spark://10.0.2.15:36275/jars/org.scala-lang.modules_scala-parser-combinators_2.11-1.0.4.jar to /tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/fetchFileTemp2906404645832741772.tmp
18/03/02 23:06:57 INFO Executor: Adding file:/tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/org.scala-lang.modules_scala-parser-combinators_2.11-1.0.4.jar to class loader
18/03/02 23:06:57 INFO Executor: Fetching spark://10.0.2.15:36275/jars/org.apache.kafka_kafka_2.11-0.10.0.0.jar with timestamp 1520012213490
18/03/02 23:06:57 INFO Utils: Fetching spark://10.0.2.15:36275/jars/org.apache.kafka_kafka_2.11-0.10.0.0.jar to /tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/fetchFileTemp5097829618913806339.tmp
18/03/02 23:06:57 INFO Executor: Adding file:/tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/org.apache.kafka_kafka_2.11-0.10.0.0.jar to class loader
18/03/02 23:06:57 INFO Executor: Fetching spark://10.0.2.15:36275/jars/org.apache.kafka_kafka-clients-0.10.0.0.jar with timestamp 1520012213519
18/03/02 23:06:57 INFO Utils: Fetching spark://10.0.2.15:36275/jars/org.apache.kafka_kafka-clients-0.10.0.0.jar to /tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/fetchFileTemp2127780574440825564.tmp
18/03/02 23:06:57 INFO Executor: Adding file:/tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/org.apache.kafka_kafka-clients-0.10.0.0.jar to class loader
18/03/02 23:06:57 INFO Executor: Fetching spark://10.0.2.15:36275/jars/org.xerial.snappy_snappy-java-1.1.2.4.jar with timestamp 1520012213520
18/03/02 23:06:57 INFO Utils: Fetching spark://10.0.2.15:36275/jars/org.xerial.snappy_snappy-java-1.1.2.4.jar to /tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/fetchFileTemp6908713258816331764.tmp
18/03/02 23:06:57 INFO Executor: Adding file:/tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/org.xerial.snappy_snappy-java-1.1.2.4.jar to class loader
18/03/02 23:06:57 INFO Executor: Fetching spark://10.0.2.15:36275/jars/org.scalatest_scalatest_2.11-2.2.6.jar with timestamp 1520012213520
18/03/02 23:06:57 INFO Utils: Fetching spark://10.0.2.15:36275/jars/org.scalatest_scalatest_2.11-2.2.6.jar to /tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/fetchFileTemp2682339684446546358.tmp
18/03/02 23:06:57 INFO Executor: Adding file:/tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71/userFiles-19c3d66b-0267-4c1a-88f4-12fb149e9211/org.scalatest_scalatest_2.11-2.2.6.jar to class loader
18/03/02 23:06:57 INFO KafkaRDD: Computing topic kafka_topic, partition 0 offsets 0 -> 45
18/03/02 23:06:57 INFO CachedKafkaConsumer: Initializing cache 16 64 0.75
18/03/02 23:06:57 INFO CachedKafkaConsumer: Cache miss for CacheKey(spark-executor-use_a_separate_group_id_for_each_stream,kafka_topic,0)
18/03/02 23:06:57 INFO ConsumerConfig: ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = none
	bootstrap.servers = [127.0.0.1:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-executor-use_a_separate_group_id_for_each_stream
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

18/03/02 23:06:57 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 23:06:57 INFO AppInfoParser: Kafka commitId : cb8625948210849f
kafka_topic 0 0 45
18/03/02 23:06:57 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1012 bytes result sent to driver
18/03/02 23:06:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1187 ms on localhost (executor driver) (1/1)
18/03/02 23:06:57 INFO DAGScheduler: ResultStage 0 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 1.257 s
18/03/02 23:06:57 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
18/03/02 23:06:57 INFO DAGScheduler: Job 0 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 1.686568 s
18/03/02 23:06:57 INFO JobScheduler: Finished job streaming job 1520012215000 ms.0 from job set of time 1520012215000 ms
18/03/02 23:06:57 INFO JobScheduler: Starting job streaming job 1520012215000 ms.1 from job set of time 1520012215000 ms
18/03/02 23:06:57 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 23:06:57 INFO DAGScheduler: Got job 1 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 23:06:57 INFO DAGScheduler: Final stage: ResultStage 1 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 23:06:57 INFO DAGScheduler: Parents of final stage: List()
18/03/02 23:06:57 INFO DAGScheduler: Missing parents: List()
18/03/02 23:06:57 INFO DAGScheduler: Submitting ResultStage 1 (KafkaRDD[0] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 23:06:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 23:06:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1605.0 B, free 366.3 MB)
18/03/02 23:06:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:34202 (size: 1605.0 B, free: 366.3 MB)
18/03/02 23:06:57 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:996
18/03/02 23:06:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (KafkaRDD[0] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 23:06:57 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
18/03/02 23:06:57 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7554 bytes)
18/03/02 23:06:57 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
18/03/02 23:06:57 INFO KafkaRDD: Computing topic kafka_topic, partition 0 offsets 0 -> 45
18/03/02 23:06:57 INFO CachedKafkaConsumer: Initial fetch for spark-executor-use_a_separate_group_id_for_each_stream kafka_topic 0 0
null,Hi hello,Hi hello
null,how r u?,how r u?
null,aFFDVF,aFFDVF
null,FA,FA
null,FA,FA
null,FSAF,FSAF
null,FS,FS
null,SFF,SFF
null,SF,SF
null,FSA,FSA
null,SF,SF
null,DSDA,DSDA
null,FSA,FSA
null,F,F
null,FA,FA
null,SFA,SFA
null,A,A
null,FSAAF,FSAAF
null,SAF,SAF
null,FSAD,FSAD
null,FAA,FAA
null,SF,SF
null,AF,AF
null,F,F
null,FADS,FADS
null,0,115610216,0,115610216
null,1000,1838683,1000,1838683
null,10000,335707,10000,335707
null,10001,58524,10001,58524
null,10003,200739,10003,200739
null,10005,76444,10005,76444
null,1001,20071,1001,20071
null,1003,73283,1003,73283
null,1005,9200,1005,9200
null,1007,7091,1007,7091
null,0,115610216,0,115610216
null,1000,1838683,1000,1838683
null,10000,335707,10000,335707
null,10001,58524,10001,58524
null,10003,200739,10003,200739
null,10005,76444,10005,76444
null,1001,20071,1001,20071
null,1003,73283,1003,73283
null,1005,9200,1005,9200
null,1007,7091,1007,7091
18/03/02 23:06:57 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1012 bytes result sent to driver
18/03/02 23:06:57 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 133 ms on localhost (executor driver) (1/1)
18/03/02 23:06:57 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
18/03/02 23:06:57 INFO DAGScheduler: ResultStage 1 (foreach at SparkStreamingConsumer.scala:70) finished in 0.144 s
18/03/02 23:06:57 INFO DAGScheduler: Job 1 finished: foreach at SparkStreamingConsumer.scala:70, took 0.175317 s
18/03/02 23:06:57 INFO JobScheduler: Finished job streaming job 1520012215000 ms.1 from job set of time 1520012215000 ms
18/03/02 23:06:57 INFO JobScheduler: Total delay: 2.746 s for time 1520012215000 ms (execution: 2.018 s)
18/03/02 23:06:57 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 23:06:57 INFO InputInfoTracker: remove old batch metadata: 
18/03/02 23:07:00 INFO JobScheduler: Added jobs for time 1520012220000 ms
18/03/02 23:07:00 INFO JobScheduler: Starting job streaming job 1520012220000 ms.0 from job set of time 1520012220000 ms
18/03/02 23:07:00 INFO SparkContext: Starting job: foreachPartition at SparkStreamingConsumer.scala:62
18/03/02 23:07:00 INFO DAGScheduler: Got job 2 (foreachPartition at SparkStreamingConsumer.scala:62) with 1 output partitions
18/03/02 23:07:00 INFO DAGScheduler: Final stage: ResultStage 2 (foreachPartition at SparkStreamingConsumer.scala:62)
18/03/02 23:07:00 INFO DAGScheduler: Parents of final stage: List()
18/03/02 23:07:00 INFO DAGScheduler: Missing parents: List()
18/03/02 23:07:00 INFO DAGScheduler: Submitting ResultStage 2 (KafkaRDD[1] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 23:07:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 23:07:00 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1630.0 B, free 366.3 MB)
18/03/02 23:07:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:34202 (size: 1630.0 B, free: 366.3 MB)
18/03/02 23:07:00 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996
18/03/02 23:07:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (KafkaRDD[1] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 23:07:00 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
18/03/02 23:07:00 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7563 bytes)
18/03/02 23:07:00 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
18/03/02 23:07:00 INFO KafkaRDD: Beginning offset 45 is the same as ending offset skipping kafka_topic 0
kafka_topic 0 45 45
18/03/02 23:07:00 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1012 bytes result sent to driver
18/03/02 23:07:00 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 65 ms on localhost (executor driver) (1/1)
18/03/02 23:07:00 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
18/03/02 23:07:00 INFO DAGScheduler: ResultStage 2 (foreachPartition at SparkStreamingConsumer.scala:62) finished in 0.070 s
18/03/02 23:07:00 INFO DAGScheduler: Job 2 finished: foreachPartition at SparkStreamingConsumer.scala:62, took 0.086037 s
18/03/02 23:07:00 INFO SparkContext: Starting job: foreach at SparkStreamingConsumer.scala:70
18/03/02 23:07:00 INFO JobScheduler: Finished job streaming job 1520012220000 ms.0 from job set of time 1520012220000 ms
18/03/02 23:07:00 INFO JobScheduler: Starting job streaming job 1520012220000 ms.1 from job set of time 1520012220000 ms
18/03/02 23:07:00 INFO DAGScheduler: Got job 3 (foreach at SparkStreamingConsumer.scala:70) with 1 output partitions
18/03/02 23:07:00 INFO DAGScheduler: Final stage: ResultStage 3 (foreach at SparkStreamingConsumer.scala:70)
18/03/02 23:07:00 INFO DAGScheduler: Parents of final stage: List()
18/03/02 23:07:00 INFO DAGScheduler: Missing parents: List()
18/03/02 23:07:00 INFO DAGScheduler: Submitting ResultStage 3 (KafkaRDD[1] at createDirectStream at SparkStreamingConsumer.scala:53), which has no missing parents
18/03/02 23:07:00 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 2.4 KB, free 366.3 MB)
18/03/02 23:07:00 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 1614.0 B, free 366.3 MB)
18/03/02 23:07:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.2.15:34202 (size: 1614.0 B, free: 366.3 MB)
18/03/02 23:07:00 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:996
18/03/02 23:07:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (KafkaRDD[1] at createDirectStream at SparkStreamingConsumer.scala:53)
18/03/02 23:07:00 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
18/03/02 23:07:00 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 7554 bytes)
18/03/02 23:07:00 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
18/03/02 23:07:00 INFO KafkaRDD: Beginning offset 45 is the same as ending offset skipping kafka_topic 0
18/03/02 23:07:00 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 925 bytes result sent to driver
18/03/02 23:07:00 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 54 ms on localhost (executor driver) (1/1)
18/03/02 23:07:00 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
18/03/02 23:07:00 INFO DAGScheduler: ResultStage 3 (foreach at SparkStreamingConsumer.scala:70) finished in 0.055 s
18/03/02 23:07:00 INFO DAGScheduler: Job 3 finished: foreach at SparkStreamingConsumer.scala:70, took 0.084149 s
18/03/02 23:07:00 INFO JobScheduler: Finished job streaming job 1520012220000 ms.1 from job set of time 1520012220000 ms
18/03/02 23:07:00 INFO JobScheduler: Total delay: 0.193 s for time 1520012220000 ms (execution: 0.186 s)
18/03/02 23:07:00 INFO KafkaRDD: Removing RDD 0 from persistence list
18/03/02 23:07:00 INFO ReceivedBlockTracker: Deleting batches: 
18/03/02 23:07:00 INFO InputInfoTracker: remove old batch metadata: 
18/03/02 23:07:00 INFO BlockManager: Removing RDD 0
^C18/03/02 23:07:02 INFO StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
18/03/02 23:07:02 INFO ReceiverTracker: ReceiverTracker stopped
18/03/02 23:07:02 INFO JobGenerator: Stopping JobGenerator immediately
18/03/02 23:07:02 INFO RecurringTimer: Stopped timer for JobGenerator after time 1520012220000
18/03/02 23:07:02 INFO JobGenerator: Stopped JobGenerator
18/03/02 23:07:02 INFO JobScheduler: Stopped JobScheduler
18/03/02 23:07:02 INFO StreamingContext: StreamingContext stopped successfully
18/03/02 23:07:02 INFO SparkContext: Invoking stop() from shutdown hook
18/03/02 23:07:02 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
18/03/02 23:07:02 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/03/02 23:07:02 INFO MemoryStore: MemoryStore cleared
18/03/02 23:07:02 INFO BlockManager: BlockManager stopped
18/03/02 23:07:02 INFO BlockManagerMaster: BlockManagerMaster stopped
18/03/02 23:07:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/03/02 23:07:02 INFO SparkContext: Successfully stopped SparkContext
18/03/02 23:07:02 INFO ShutdownHookManager: Shutdown hook called
18/03/02 23:07:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-cd27b72a-0652-4892-973b-de4c90eefb71
[edureka@localhost sparkjobs]$ cd /tmp/kafka-logs
[edureka@localhost kafka-logs]$ ls -lrt
total 16
-rw-rw-r--. 1 edureka edureka    0 Feb 25 12:03 cleaner-offset-checkpoint
-rw-rw-r--. 1 edureka edureka   54 Feb 25 12:03 meta.properties
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-0
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-29
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-48
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-45
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-26
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-7
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-42
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-4
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-23
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-1
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-20
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-39
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-17
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-36
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-14
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-33
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-49
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-11
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-30
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-46
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-27
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-8
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-24
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-43
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-5
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-21
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-2
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-40
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-37
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-18
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-34
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-15
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-12
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-31
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-9
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-47
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-19
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-28
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-38
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-35
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-44
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-6
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-25
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-16
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-22
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-41
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-32
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-3
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-13
drwxrwxr-x. 2 edureka edureka  141 Mar  2 22:40 __consumer_offsets-10
drwxrwxr-x. 2 edureka edureka  178 Mar  2 22:42 kafka_topic-0
-rw-rw-r--. 1 edureka edureka 1212 Mar  2 23:10 recovery-point-offset-checkpoint
-rw-rw-r--. 1 edureka edureka    4 Mar  2 23:10 log-start-offset-checkpoint
-rw-rw-r--. 1 edureka edureka 1212 Mar  2 23:10 replication-offset-checkpoint

---kafka producer
[edureka@localhost ~]$ cd /usr/lib/kafka_2.12-0.11.0.0
[edureka@localhost kafka_2.12-0.11.0.0]$ bin/kafka-topics.sh --zookeeper localhost:2181 --list
kafka_topic
[edureka@localhost kafka_2.12-0.11.0.0]$ cd /home/edureka/SHAHBAZWS/BATCH170917/sparkjobs
[edureka@localhost sparkjobs]$ spark-submit --verbose --master local --deploy-mode client --conf spark.driver.extraClassPath=/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/mysql-connector-java-5.1.43-bin.jar:/usr/lib/kafka_2.12-0.11.0.0/libs/kafka-clients-0.11.0.0.jar --class com.laboros.spark.sql.stream.SparkSqlProducer sparkdemo_2.11-0.1.0-SNAPSHOT.jar kafka_topic
Using properties file: /usr/lib/spark-2.1.1-bin-hadoop2.7/conf/spark-defaults.conf
Adding default property: spark.master=spark://localhost:7077
Parsed arguments:
  master                  local
  deployMode              client
  executorMemory          null
  executorCores           null
  totalExecutorCores      null
  propertiesFile          /usr/lib/spark-2.1.1-bin-hadoop2.7/conf/spark-defaults.conf
  driverMemory            null
  driverCores             null
  driverExtraClassPath    /home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/mysql-connector-java-5.1.43-bin.jar:/usr/lib/kafka_2.12-0.11.0.0/libs/kafka-clients-0.11.0.0.jar
  driverExtraLibraryPath  null
  driverExtraJavaOptions  null
  supervise               false
  queue                   null
  numExecutors            null
  files                   null
  pyFiles                 null
  archives                null
  mainClass               com.laboros.spark.sql.stream.SparkSqlProducer
  primaryResource         file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar
  name                    com.laboros.spark.sql.stream.SparkSqlProducer
  childArgs               [kafka_topic]
  jars                    null
  packages                null
  packagesExclusions      null
  repositories            null
  verbose                 true

Spark properties used, including those specified through
 --conf and those from the properties file /usr/lib/spark-2.1.1-bin-hadoop2.7/conf/spark-defaults.conf:
  spark.master -> spark://localhost:7077
  spark.driver.extraClassPath -> /home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/mysql-connector-java-5.1.43-bin.jar:/usr/lib/kafka_2.12-0.11.0.0/libs/kafka-clients-0.11.0.0.jar

    
Main class:
com.laboros.spark.sql.stream.SparkSqlProducer
Arguments:
kafka_topic
System properties:
SPARK_SUBMIT -> true
spark.app.name -> com.laboros.spark.sql.stream.SparkSqlProducer
spark.jars -> file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar
spark.submit.deployMode -> client
spark.master -> local
spark.driver.extraClassPath -> /home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/mysql-connector-java-5.1.43-bin.jar:/usr/lib/kafka_2.12-0.11.0.0/libs/kafka-clients-0.11.0.0.jar
Classpath elements:
file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar


18/03/02 22:41:50 INFO SparkContext: Running Spark version 2.1.1
18/03/02 22:41:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/03/02 22:41:51 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 10.0.2.15 instead (on interface enp0s3)
18/03/02 22:41:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/03/02 22:41:51 INFO SecurityManager: Changing view acls to: edureka
18/03/02 22:41:51 INFO SecurityManager: Changing modify acls to: edureka
18/03/02 22:41:51 INFO SecurityManager: Changing view acls groups to: 
18/03/02 22:41:51 INFO SecurityManager: Changing modify acls groups to: 
18/03/02 22:41:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(edureka); groups with view permissions: Set(); users  with modify permissions: Set(edureka); groups with modify permissions: Set()
18/03/02 22:41:52 INFO Utils: Successfully started service 'sparkDriver' on port 37700.
18/03/02 22:41:52 INFO SparkEnv: Registering MapOutputTracker
18/03/02 22:41:52 INFO SparkEnv: Registering BlockManagerMaster
18/03/02 22:41:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/03/02 22:41:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/03/02 22:41:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4d891803-225f-4a32-8145-5ab0ab6cbe62
18/03/02 22:41:52 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
18/03/02 22:41:52 INFO SparkEnv: Registering OutputCommitCoordinator
18/03/02 22:41:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
18/03/02 22:41:52 INFO Utils: Successfully started service 'SparkUI' on port 4041.
18/03/02 22:41:52 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4041
18/03/02 22:41:52 INFO SparkContext: Added JAR file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar at spark://10.0.2.15:37700/jars/sparkdemo_2.11-0.1.0-SNAPSHOT.jar with timestamp 1520010712926
18/03/02 22:41:53 INFO Executor: Starting executor ID driver on host localhost
18/03/02 22:41:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45343.
18/03/02 22:41:53 INFO NettyBlockTransferService: Server created on 10.0.2.15:45343
18/03/02 22:41:53 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/03/02 22:41:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 45343, None)
18/03/02 22:41:53 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:45343 with 366.3 MB RAM, BlockManagerId(driver, 10.0.2.15, 45343, None)
18/03/02 22:41:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 45343, None)
18/03/02 22:41:53 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 45343, None)
18/03/02 22:41:53 INFO SharedState: Warehouse path is 'file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/spark-warehouse/'.
Current Topic Name kafka_topic
Fri Mar 02 22:41:55 IST 2018 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
18/03/02 22:42:00 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
18/03/02 22:42:02 INFO ContextCleaner: Cleaned accumulator 2
18/03/02 22:42:02 INFO CodeGenerator: Code generated in 720.422905 ms
18/03/02 22:42:02 INFO CodeGenerator: Code generated in 60.501126 ms
18/03/02 22:42:02 INFO SparkContext: Starting job: foreach at SparkSqlProducer.scala:80
18/03/02 22:42:03 INFO DAGScheduler: Registering RDD 3 (foreach at SparkSqlProducer.scala:80)
18/03/02 22:42:03 INFO DAGScheduler: Got job 0 (foreach at SparkSqlProducer.scala:80) with 1 output partitions
18/03/02 22:42:03 INFO DAGScheduler: Final stage: ResultStage 1 (foreach at SparkSqlProducer.scala:80)
18/03/02 22:42:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
18/03/02 22:42:03 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
18/03/02 22:42:03 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at foreach at SparkSqlProducer.scala:80), which has no missing parents
18/03/02 22:42:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.7 KB, free 366.3 MB)
18/03/02 22:42:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.2 KB, free 366.3 MB)
18/03/02 22:42:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:45343 (size: 5.2 KB, free: 366.3 MB)
18/03/02 22:42:04 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:996
18/03/02 22:42:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at foreach at SparkSqlProducer.scala:80)
18/03/02 22:42:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
18/03/02 22:42:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5868 bytes)
18/03/02 22:42:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
18/03/02 22:42:04 INFO Executor: Fetching spark://10.0.2.15:37700/jars/sparkdemo_2.11-0.1.0-SNAPSHOT.jar with timestamp 1520010712926
18/03/02 22:42:04 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:37700 after 60 ms (0 ms spent in bootstraps)
18/03/02 22:42:04 INFO Utils: Fetching spark://10.0.2.15:37700/jars/sparkdemo_2.11-0.1.0-SNAPSHOT.jar to /tmp/spark-dc737865-55a6-4f2b-9b34-044899c8d3d5/userFiles-20f49b66-58b6-45eb-8ca9-52b84732d8a3/fetchFileTemp7798757376542300101.tmp
18/03/02 22:42:04 INFO Executor: Adding file:/tmp/spark-dc737865-55a6-4f2b-9b34-044899c8d3d5/userFiles-20f49b66-58b6-45eb-8ca9-52b84732d8a3/sparkdemo_2.11-0.1.0-SNAPSHOT.jar to class loader
Fri Mar 02 22:42:05 IST 2018 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
18/03/02 22:42:05 INFO JDBCRDD: closed connection
18/03/02 22:42:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1816 bytes result sent to driver
18/03/02 22:42:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1550 ms on localhost (executor driver) (1/1)
18/03/02 22:42:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
18/03/02 22:42:05 INFO DAGScheduler: ShuffleMapStage 0 (foreach at SparkSqlProducer.scala:80) finished in 1.624 s
18/03/02 22:42:05 INFO DAGScheduler: looking for newly runnable stages
18/03/02 22:42:05 INFO DAGScheduler: running: Set()
18/03/02 22:42:05 INFO DAGScheduler: waiting: Set(ResultStage 1)
18/03/02 22:42:05 INFO DAGScheduler: failed: Set()
18/03/02 22:42:05 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at foreach at SparkSqlProducer.scala:80), which has no missing parents
18/03/02 22:42:05 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.1 KB, free 366.3 MB)
18/03/02 22:42:05 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.9 KB, free 366.3 MB)
18/03/02 22:42:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:45343 (size: 6.9 KB, free: 366.3 MB)
18/03/02 22:42:05 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:996
18/03/02 22:42:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at foreach at SparkSqlProducer.scala:80)
18/03/02 22:42:05 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
18/03/02 22:42:05 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 5956 bytes)
18/03/02 22:42:05 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
18/03/02 22:42:05 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
18/03/02 22:42:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
18/03/02 22:42:06 INFO CodeGenerator: Code generated in 36.80545 ms
18/03/02 22:42:06 INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

18/03/02 22:42:06 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 22:42:06 INFO AppInfoParser: Kafka commitId : cb8625948210849f
18/03/02 22:42:06 INFO KafkaProducer: Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
18/03/02 22:42:06 INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

18/03/02 22:42:06 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 22:42:06 INFO AppInfoParser: Kafka commitId : cb8625948210849f
18/03/02 22:42:06 INFO KafkaProducer: Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
18/03/02 22:42:06 INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

18/03/02 22:42:06 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 22:42:06 INFO AppInfoParser: Kafka commitId : cb8625948210849f
18/03/02 22:42:06 INFO KafkaProducer: Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
18/03/02 22:42:06 INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

18/03/02 22:42:06 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 22:42:06 INFO AppInfoParser: Kafka commitId : cb8625948210849f
18/03/02 22:42:06 INFO KafkaProducer: Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
18/03/02 22:42:06 INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

18/03/02 22:42:06 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 22:42:06 INFO AppInfoParser: Kafka commitId : cb8625948210849f
18/03/02 22:42:06 INFO KafkaProducer: Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
18/03/02 22:42:06 INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

18/03/02 22:42:06 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 22:42:06 INFO AppInfoParser: Kafka commitId : cb8625948210849f
18/03/02 22:42:06 INFO KafkaProducer: Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
18/03/02 22:42:06 INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

18/03/02 22:42:06 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 22:42:06 INFO AppInfoParser: Kafka commitId : cb8625948210849f
18/03/02 22:42:06 INFO KafkaProducer: Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
18/03/02 22:42:06 INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

18/03/02 22:42:06 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 22:42:06 INFO AppInfoParser: Kafka commitId : cb8625948210849f
18/03/02 22:42:06 INFO KafkaProducer: Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
18/03/02 22:42:06 INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

18/03/02 22:42:06 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 22:42:06 INFO AppInfoParser: Kafka commitId : cb8625948210849f
18/03/02 22:42:07 INFO KafkaProducer: Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
18/03/02 22:42:07 INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

18/03/02 22:42:07 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 22:42:07 INFO AppInfoParser: Kafka commitId : cb8625948210849f
18/03/02 22:42:07 INFO KafkaProducer: Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
18/03/02 22:42:07 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2053 bytes result sent to driver
18/03/02 22:42:07 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1278 ms on localhost (executor driver) (1/1)
18/03/02 22:42:07 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
18/03/02 22:42:07 INFO DAGScheduler: ResultStage 1 (foreach at SparkSqlProducer.scala:80) finished in 1.279 s
18/03/02 22:42:07 INFO DAGScheduler: Job 0 finished: foreach at SparkSqlProducer.scala:80, took 4.149050 s
18/03/02 22:42:07 INFO SparkContext: Invoking stop() from shutdown hook
18/03/02 22:42:07 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4041
18/03/02 22:42:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/03/02 22:42:07 INFO MemoryStore: MemoryStore cleared
18/03/02 22:42:07 INFO BlockManager: BlockManager stopped
18/03/02 22:42:07 INFO BlockManagerMaster: BlockManagerMaster stopped
18/03/02 22:42:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/03/02 22:42:07 INFO SparkContext: Successfully stopped SparkContext
18/03/02 22:42:07 INFO ShutdownHookManager: Shutdown hook called
18/03/02 22:42:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-dc737865-55a6-4f2b-9b34-044899c8d3d5
----
[edureka@localhost sparkjobs]$ spark-submit --verbose --master local --deploy-mode client --conf spark.driver.extraClassPath=/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/mysql-connector-java-5.1.43-bin.jar:/usr/lib/kafka_2.12-0.11.0.0/libs/kafka-clients-0.11.0.0.jar --class com.laboros.spark.sql.stream.SparkSqlProducer sparkdemo_2.11-0.1.0-SNAPSHOT.jar kafka_topic
Using properties file: /usr/lib/spark-2.1.1-bin-hadoop2.7/conf/spark-defaults.conf
Adding default property: spark.master=spark://localhost:7077
Parsed arguments:
  master                  local
  deployMode              client
  executorMemory          null
  executorCores           null
  totalExecutorCores      null
  propertiesFile          /usr/lib/spark-2.1.1-bin-hadoop2.7/conf/spark-defaults.conf
  driverMemory            null
  driverCores             null
  driverExtraClassPath    /home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/mysql-connector-java-5.1.43-bin.jar:/usr/lib/kafka_2.12-0.11.0.0/libs/kafka-clients-0.11.0.0.jar
  driverExtraLibraryPath  null
  driverExtraJavaOptions  null
  supervise               false
  queue                   null
  numExecutors            null
  files                   null
  pyFiles                 null
  archives                null
  mainClass               com.laboros.spark.sql.stream.SparkSqlProducer
  primaryResource         file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar
  name                    com.laboros.spark.sql.stream.SparkSqlProducer
  childArgs               [kafka_topic]
  jars                    null
  packages                null
  packagesExclusions      null
  repositories            null
  verbose                 true

Spark properties used, including those specified through
 --conf and those from the properties file /usr/lib/spark-2.1.1-bin-hadoop2.7/conf/spark-defaults.conf:
  spark.master -> spark://localhost:7077
  spark.driver.extraClassPath -> /home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/mysql-connector-java-5.1.43-bin.jar:/usr/lib/kafka_2.12-0.11.0.0/libs/kafka-clients-0.11.0.0.jar

    
Main class:
com.laboros.spark.sql.stream.SparkSqlProducer
Arguments:
kafka_topic
System properties:
SPARK_SUBMIT -> true
spark.app.name -> com.laboros.spark.sql.stream.SparkSqlProducer
spark.jars -> file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar
spark.submit.deployMode -> client
spark.master -> local
spark.driver.extraClassPath -> /home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/mysql-connector-java-5.1.43-bin.jar:/usr/lib/kafka_2.12-0.11.0.0/libs/kafka-clients-0.11.0.0.jar
Classpath elements:
file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar


18/03/02 23:06:17 INFO SparkContext: Running Spark version 2.1.1
18/03/02 23:06:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/03/02 23:06:18 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 10.0.2.15 instead (on interface enp0s3)
18/03/02 23:06:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/03/02 23:06:18 INFO SecurityManager: Changing view acls to: edureka
18/03/02 23:06:18 INFO SecurityManager: Changing modify acls to: edureka
18/03/02 23:06:18 INFO SecurityManager: Changing view acls groups to: 
18/03/02 23:06:18 INFO SecurityManager: Changing modify acls groups to: 
18/03/02 23:06:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(edureka); groups with view permissions: Set(); users  with modify permissions: Set(edureka); groups with modify permissions: Set()
18/03/02 23:06:19 INFO Utils: Successfully started service 'sparkDriver' on port 39519.
18/03/02 23:06:19 INFO SparkEnv: Registering MapOutputTracker
18/03/02 23:06:19 INFO SparkEnv: Registering BlockManagerMaster
18/03/02 23:06:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/03/02 23:06:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/03/02 23:06:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-57c64d47-ecb3-48f2-948c-c2b73185fbc6
18/03/02 23:06:19 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
18/03/02 23:06:19 INFO SparkEnv: Registering OutputCommitCoordinator
18/03/02 23:06:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/03/02 23:06:20 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4040
18/03/02 23:06:20 INFO SparkContext: Added JAR file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar at spark://10.0.2.15:39519/jars/sparkdemo_2.11-0.1.0-SNAPSHOT.jar with timestamp 1520012180203
18/03/02 23:06:20 INFO Executor: Starting executor ID driver on host localhost
18/03/02 23:06:20 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40627.
18/03/02 23:06:20 INFO NettyBlockTransferService: Server created on 10.0.2.15:40627
18/03/02 23:06:20 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/03/02 23:06:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 40627, None)
18/03/02 23:06:20 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:40627 with 366.3 MB RAM, BlockManagerId(driver, 10.0.2.15, 40627, None)
18/03/02 23:06:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 40627, None)
18/03/02 23:06:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 40627, None)
18/03/02 23:06:20 INFO SharedState: Warehouse path is 'file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/spark-warehouse/'.
Current Topic Name kafka_topic
Fri Mar 02 23:06:21 IST 2018 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
18/03/02 23:06:25 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
18/03/02 23:06:27 INFO ContextCleaner: Cleaned accumulator 2
18/03/02 23:06:27 INFO CodeGenerator: Code generated in 465.743861 ms
18/03/02 23:06:27 INFO CodeGenerator: Code generated in 32.908485 ms
18/03/02 23:06:27 INFO SparkContext: Starting job: foreach at SparkSqlProducer.scala:80
18/03/02 23:06:27 INFO DAGScheduler: Registering RDD 3 (foreach at SparkSqlProducer.scala:80)
18/03/02 23:06:27 INFO DAGScheduler: Got job 0 (foreach at SparkSqlProducer.scala:80) with 1 output partitions
18/03/02 23:06:27 INFO DAGScheduler: Final stage: ResultStage 1 (foreach at SparkSqlProducer.scala:80)
18/03/02 23:06:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
18/03/02 23:06:27 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
18/03/02 23:06:27 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at foreach at SparkSqlProducer.scala:80), which has no missing parents
18/03/02 23:06:27 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.7 KB, free 366.3 MB)
18/03/02 23:06:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.2 KB, free 366.3 MB)
18/03/02 23:06:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:40627 (size: 5.2 KB, free: 366.3 MB)
18/03/02 23:06:27 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:996
18/03/02 23:06:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at foreach at SparkSqlProducer.scala:80)
18/03/02 23:06:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
18/03/02 23:06:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5868 bytes)
18/03/02 23:06:27 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
18/03/02 23:06:27 INFO Executor: Fetching spark://10.0.2.15:39519/jars/sparkdemo_2.11-0.1.0-SNAPSHOT.jar with timestamp 1520012180203
18/03/02 23:06:27 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:39519 after 45 ms (0 ms spent in bootstraps)
18/03/02 23:06:28 INFO Utils: Fetching spark://10.0.2.15:39519/jars/sparkdemo_2.11-0.1.0-SNAPSHOT.jar to /tmp/spark-d3ebc33b-af46-4904-9f4c-b5f56d27b0ed/userFiles-68076523-107b-46a1-a85b-f7d524914fd0/fetchFileTemp629357251130564030.tmp
18/03/02 23:06:28 INFO Executor: Adding file:/tmp/spark-d3ebc33b-af46-4904-9f4c-b5f56d27b0ed/userFiles-68076523-107b-46a1-a85b-f7d524914fd0/sparkdemo_2.11-0.1.0-SNAPSHOT.jar to class loader
Fri Mar 02 23:06:28 IST 2018 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
18/03/02 23:06:28 INFO JDBCRDD: closed connection
18/03/02 23:06:28 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1816 bytes result sent to driver
18/03/02 23:06:28 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 769 ms on localhost (executor driver) (1/1)
18/03/02 23:06:28 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
18/03/02 23:06:28 INFO DAGScheduler: ShuffleMapStage 0 (foreach at SparkSqlProducer.scala:80) finished in 0.813 s
18/03/02 23:06:28 INFO DAGScheduler: looking for newly runnable stages
18/03/02 23:06:28 INFO DAGScheduler: running: Set()
18/03/02 23:06:28 INFO DAGScheduler: waiting: Set(ResultStage 1)
18/03/02 23:06:28 INFO DAGScheduler: failed: Set()
18/03/02 23:06:28 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at foreach at SparkSqlProducer.scala:80), which has no missing parents
18/03/02 23:06:28 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.1 KB, free 366.3 MB)
18/03/02 23:06:28 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.9 KB, free 366.3 MB)
18/03/02 23:06:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:40627 (size: 6.9 KB, free: 366.3 MB)
18/03/02 23:06:28 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:996
18/03/02 23:06:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at foreach at SparkSqlProducer.scala:80)
18/03/02 23:06:28 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
18/03/02 23:06:28 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 5956 bytes)
18/03/02 23:06:28 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
18/03/02 23:06:28 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
18/03/02 23:06:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
18/03/02 23:06:28 INFO CodeGenerator: Code generated in 34.433023 ms
18/03/02 23:06:28 INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

18/03/02 23:06:28 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 23:06:28 INFO AppInfoParser: Kafka commitId : cb8625948210849f
18/03/02 23:06:29 INFO KafkaProducer: Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
18/03/02 23:06:29 INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

18/03/02 23:06:29 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 23:06:29 INFO AppInfoParser: Kafka commitId : cb8625948210849f
18/03/02 23:06:29 INFO KafkaProducer: Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
18/03/02 23:06:29 INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

18/03/02 23:06:29 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 23:06:29 INFO AppInfoParser: Kafka commitId : cb8625948210849f
18/03/02 23:06:29 INFO KafkaProducer: Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
18/03/02 23:06:29 INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

18/03/02 23:06:29 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 23:06:29 INFO AppInfoParser: Kafka commitId : cb8625948210849f
18/03/02 23:06:29 INFO KafkaProducer: Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
18/03/02 23:06:29 INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

18/03/02 23:06:29 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 23:06:29 INFO AppInfoParser: Kafka commitId : cb8625948210849f
18/03/02 23:06:29 INFO KafkaProducer: Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
18/03/02 23:06:29 INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

18/03/02 23:06:29 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 23:06:29 INFO AppInfoParser: Kafka commitId : cb8625948210849f
18/03/02 23:06:29 INFO KafkaProducer: Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
18/03/02 23:06:29 INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

18/03/02 23:06:29 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 23:06:29 INFO AppInfoParser: Kafka commitId : cb8625948210849f
18/03/02 23:06:29 INFO KafkaProducer: Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
18/03/02 23:06:29 INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

18/03/02 23:06:29 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 23:06:29 INFO AppInfoParser: Kafka commitId : cb8625948210849f
18/03/02 23:06:29 INFO KafkaProducer: Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
18/03/02 23:06:29 INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

18/03/02 23:06:29 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 23:06:29 INFO AppInfoParser: Kafka commitId : cb8625948210849f
18/03/02 23:06:29 INFO KafkaProducer: Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
18/03/02 23:06:29 INFO ProducerConfig: ProducerConfig values: 
	acks = all
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

18/03/02 23:06:29 INFO AppInfoParser: Kafka version : 0.11.0.0
18/03/02 23:06:29 INFO AppInfoParser: Kafka commitId : cb8625948210849f
18/03/02 23:06:29 INFO KafkaProducer: Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
18/03/02 23:06:29 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2053 bytes result sent to driver
18/03/02 23:06:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 869 ms on localhost (executor driver) (1/1)
18/03/02 23:06:29 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
18/03/02 23:06:29 INFO DAGScheduler: ResultStage 1 (foreach at SparkSqlProducer.scala:80) finished in 0.871 s
18/03/02 23:06:29 INFO DAGScheduler: Job 0 finished: foreach at SparkSqlProducer.scala:80, took 2.190866 s
18/03/02 23:06:29 INFO SparkContext: Invoking stop() from shutdown hook
18/03/02 23:06:29 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
18/03/02 23:06:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/03/02 23:06:29 INFO MemoryStore: MemoryStore cleared
18/03/02 23:06:29 INFO BlockManager: BlockManager stopped
18/03/02 23:06:29 INFO BlockManagerMaster: BlockManagerMaster stopped
18/03/02 23:06:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/03/02 23:06:29 INFO SparkContext: Successfully stopped SparkContext
18/03/02 23:06:29 INFO ShutdownHookManager: Shutdown hook called
18/03/02 23:06:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-d3ebc33b-af46-4904-9f4c-b5f56d27b0ed
[edureka@localhost sparkjobs]$ 

