Spark---

$spark-shell

$spark-submit --class driverClass clientjar input output

cmd> scala -version

cmd> sbt sbt-version

vm - 	scala version - 2.12.2,	spark -2.1.1

cmd-WS> sbt new https://github.com/sbt/scala-seed.g8
	
project name: SparkDemo

cmd-ws> cd SparkDemo

change inside buildt.sbt file name as SparkDemo instead of Hello
add spark-core dependencies in this file and make scala and sbt version consitent with your env

cmd-ws> sbt update 

cmd-ws> sbt eclipse:clean eclipse

import into eclipse

----RUN on hdfs
$ spark-submit --verbose --class com.laboros.spark.driver.WordCountDriver sparkdemo_2.11-0.1.0-SNAPSHOT.jar /user/edureka/WordCount.txt /user/edureka/WCOP_SP1

OR

$ spark-submit --verbose --class com.laboros.spark.driver.WordCountDriver sparkdemo_2.11-0.1.0-SNAPSHOT.jar hdfs://localhost:9000/user/edureka/WordCount.txt hdfs://localhost:9000/user/edureka/WCOP_SP1

--------------
Using properties file: /usr/lib/spark-2.1.1-bin-hadoop2.7/conf/spark-defaults.conf
Adding default property: spark.master=spark://localhost:7077
Parsed arguments:
  master                  spark://localhost:7077
  deployMode              null
  executorMemory          null
  executorCores           null
  totalExecutorCores      null
  propertiesFile          /usr/lib/spark-2.1.1-bin-hadoop2.7/conf/spark-defaults.conf
  driverMemory            null
  driverCores             null
  driverExtraClassPath    null
  driverExtraLibraryPath  null
  driverExtraJavaOptions  null
  supervise               false
  queue                   null
  numExecutors            null
  files                   null
  pyFiles                 null
  archives                null
  mainClass               com.laboros.spark.driver.WordCountDriver
  primaryResource         file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar
  name                    com.laboros.spark.driver.WordCountDriver
  childArgs               [/user/edureka/WordCount.txt /user/edureka/WCOP_SP1]
  jars                    null
  packages                null
  packagesExclusions      null
  repositories            null
  verbose                 true

Spark properties used, including those specified through
 --conf and those from the properties file /usr/lib/spark-2.1.1-bin-hadoop2.7/conf/spark-defaults.conf:
  spark.master -> spark://localhost:7077

    
Main class:
com.laboros.spark.driver.WordCountDriver
Arguments:
/user/edureka/WordCount.txt
/user/edureka/WCOP_SP1
System properties:
SPARK_SUBMIT -> true
spark.app.name -> com.laboros.spark.driver.WordCountDriver
spark.jars -> file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar
spark.submit.deployMode -> client
spark.master -> spark://localhost:7077
Classpath elements:
file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar


18/02/07 10:50:20 INFO SparkContext: Running Spark version 2.1.1
18/02/07 10:50:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/07 10:50:21 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 10.0.2.15 instead (on interface enp0s3)
18/02/07 10:50:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/02/07 10:50:21 INFO SecurityManager: Changing view acls to: edureka
18/02/07 10:50:21 INFO SecurityManager: Changing modify acls to: edureka
18/02/07 10:50:21 INFO SecurityManager: Changing view acls groups to: 
18/02/07 10:50:21 INFO SecurityManager: Changing modify acls groups to: 
18/02/07 10:50:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(edureka); groups with view permissions: Set(); users  with modify permissions: Set(edureka); groups with modify permissions: Set()
18/02/07 10:50:22 INFO Utils: Successfully started service 'sparkDriver' on port 37629.
18/02/07 10:50:22 INFO SparkEnv: Registering MapOutputTracker
18/02/07 10:50:22 INFO SparkEnv: Registering BlockManagerMaster
18/02/07 10:50:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/02/07 10:50:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/02/07 10:50:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-862ed95a-7d0f-4361-ae57-a8c910622614
18/02/07 10:50:22 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
18/02/07 10:50:23 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/07 10:50:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/07 10:50:23 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4040
18/02/07 10:50:23 INFO SparkContext: Added JAR file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar at spark://10.0.2.15:37629/jars/sparkdemo_2.11-0.1.0-SNAPSHOT.jar with timestamp 1517980823482
18/02/07 10:50:23 INFO Executor: Starting executor ID driver on host localhost
18/02/07 10:50:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45130.
18/02/07 10:50:23 INFO NettyBlockTransferService: Server created on 10.0.2.15:45130
18/02/07 10:50:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/02/07 10:50:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 45130, None)
18/02/07 10:50:23 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:45130 with 366.3 MB RAM, BlockManagerId(driver, 10.0.2.15, 45130, None)
18/02/07 10:50:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 45130, None)
18/02/07 10:50:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 45130, None)
18/02/07 10:50:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 242.2 KB, free 366.1 MB)
18/02/07 10:50:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.2 KB, free 366.0 MB)
18/02/07 10:50:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:45130 (size: 23.2 KB, free: 366.3 MB)
18/02/07 10:50:25 INFO SparkContext: Created broadcast 0 from textFile at WordCountDriver.scala:26
18/02/07 10:50:26 INFO FileInputFormat: Total input paths to process : 1
18/02/07 10:50:27 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
18/02/07 10:50:27 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
18/02/07 10:50:27 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
18/02/07 10:50:27 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
18/02/07 10:50:27 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
18/02/07 10:50:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
18/02/07 10:50:27 INFO SparkContext: Starting job: saveAsTextFile at WordCountDriver.scala:39
18/02/07 10:50:27 INFO DAGScheduler: Registering RDD 3 (map at WordCountDriver.scala:36)
18/02/07 10:50:27 INFO DAGScheduler: Got job 0 (saveAsTextFile at WordCountDriver.scala:39) with 1 output partitions
18/02/07 10:50:27 INFO DAGScheduler: Final stage: ResultStage 1 (saveAsTextFile at WordCountDriver.scala:39)
18/02/07 10:50:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
18/02/07 10:50:27 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
18/02/07 10:50:27 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCountDriver.scala:36), which has no missing parents
18/02/07 10:50:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.8 KB, free 366.0 MB)
18/02/07 10:50:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.8 KB, free 366.0 MB)
18/02/07 10:50:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:45130 (size: 2.8 KB, free: 366.3 MB)
18/02/07 10:50:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:996
18/02/07 10:50:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCountDriver.scala:36)
18/02/07 10:50:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
18/02/07 10:50:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6064 bytes)
18/02/07 10:50:27 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
18/02/07 10:50:27 INFO Executor: Fetching spark://10.0.2.15:37629/jars/sparkdemo_2.11-0.1.0-SNAPSHOT.jar with timestamp 1517980823482
18/02/07 10:50:28 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:37629 after 60 ms (0 ms spent in bootstraps)
18/02/07 10:50:28 INFO Utils: Fetching spark://10.0.2.15:37629/jars/sparkdemo_2.11-0.1.0-SNAPSHOT.jar to /tmp/spark-b1bc1e07-6ec9-4c69-8963-eddebf1766dd/userFiles-a5de2679-14a5-4866-a1f5-450962b4dae9/fetchFileTemp3456527237333941237.tmp
18/02/07 10:50:28 INFO Executor: Adding file:/tmp/spark-b1bc1e07-6ec9-4c69-8963-eddebf1766dd/userFiles-a5de2679-14a5-4866-a1f5-450962b4dae9/sparkdemo_2.11-0.1.0-SNAPSHOT.jar to class loader
18/02/07 10:50:28 INFO HadoopRDD: Input split: hdfs://localhost:9000/user/edureka/WordCount.txt:0+64
18/02/07 10:50:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1814 bytes result sent to driver
18/02/07 10:50:29 INFO DAGScheduler: ShuffleMapStage 0 (map at WordCountDriver.scala:36) finished in 1.304 s
18/02/07 10:50:29 INFO DAGScheduler: looking for newly runnable stages
18/02/07 10:50:29 INFO DAGScheduler: running: Set()
18/02/07 10:50:29 INFO DAGScheduler: waiting: Set(ResultStage 1)
18/02/07 10:50:29 INFO DAGScheduler: failed: Set()
18/02/07 10:50:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1253 ms on localhost (executor driver) (1/1)
18/02/07 10:50:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
18/02/07 10:50:29 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at saveAsTextFile at WordCountDriver.scala:39), which has no missing parents
18/02/07 10:50:29 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 73.1 KB, free 366.0 MB)
18/02/07 10:50:29 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 26.6 KB, free 365.9 MB)
18/02/07 10:50:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:45130 (size: 26.6 KB, free: 366.2 MB)
18/02/07 10:50:29 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:996
18/02/07 10:50:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at saveAsTextFile at WordCountDriver.scala:39)
18/02/07 10:50:29 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
18/02/07 10:50:29 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 5829 bytes)
18/02/07 10:50:29 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
18/02/07 10:50:29 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
18/02/07 10:50:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
18/02/07 10:50:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
18/02/07 10:50:30 INFO FileOutputCommitter: Saved output of task 'attempt_20180207105027_0001_m_000000_1' to hdfs://localhost:9000/user/edureka/WCOP_SP1/_temporary/0/task_20180207105027_0001_m_000000
18/02/07 10:50:30 INFO SparkHadoopMapRedUtil: attempt_20180207105027_0001_m_000000_1: Committed
18/02/07 10:50:30 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1890 bytes result sent to driver
18/02/07 10:50:30 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1314 ms on localhost (executor driver) (1/1)
18/02/07 10:50:30 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
18/02/07 10:50:30 INFO DAGScheduler: ResultStage 1 (saveAsTextFile at WordCountDriver.scala:39) finished in 1.317 s
18/02/07 10:50:30 INFO DAGScheduler: Job 0 finished: saveAsTextFile at WordCountDriver.scala:39, took 3.349920 s
18/02/07 10:50:31 INFO SparkContext: Invoking stop() from shutdown hook
18/02/07 10:50:31 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
18/02/07 10:50:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/02/07 10:50:31 INFO MemoryStore: MemoryStore cleared
18/02/07 10:50:31 INFO BlockManager: BlockManager stopped
18/02/07 10:50:31 INFO BlockManagerMaster: BlockManagerMaster stopped
18/02/07 10:50:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/02/07 10:50:31 INFO SparkContext: Successfully stopped SparkContext
18/02/07 10:50:31 INFO ShutdownHookManager: Shutdown hook called
18/02/07 10:50:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-b1bc1e07-6ec9-4c69-8963-eddebf1766dd

--------
----RUN on local file system

$ spark-submit --verbose --class com.laboros.spark.driver.WordCountDriver sparkdemo_2.11-0.1.0-SNAPSHOT.jar file:///home/edureka/SHAHBAZWS/BATCH170917/WordCount.txt /user/edureka/WCOP_SP2

[edureka@localhost sparkjobs]$ hadoop fs -ls /user/edureka/WCOP_SP2
Found 2 items
-rw-r--r--   1 edureka supergroup          0 2018-02-07 11:04 /user/edureka/WCOP_SP2/_SUCCESS
-rw-r--r--   1 edureka supergroup         96 2018-02-07 11:04 /user/edureka/WCOP_SP2/part-00000
[edureka@localhost sparkjobs]$ hadoop fs -cat /user/edureka/WCOP_SP2/part-00000
(learning,1)
(it,1)
(is,2)
(hurryyyyy,1)
(mazing,1)
(This,1)
(first,1)
(my,1)
(hdfs,1)
(file,1)

------------
If spark-shell not work then start all spark services
$ cd /usr/lib/spark-2.1.1-bin-hadoop2.7/sbin/
$ sudo ./start-all.sh

--------Run Weather programe
$ spark-submit --verbose --class com.laboros.spark.driver.WeatherDriver sparkdemo_2.11-0.1.0-SNAPSHOT.jar /user/edureka/Weather
OR
$ spark-submit --verbose --class com.laboros.spark.driver.WeatherDriver sparkdemo_2.11-0.1.0-SNAPSHOT.jar hdfs://localhost:9000/user/edureka/Weather

for yarn cluster its actually failing

$ $ spark-submit --verbose --master yarn --deploy-mode cluster --class com.laboros.spark.driver.WeatherDriver sparkdemo_2.11-0.1.0-SNAPSHOT.jar hdfs://localhost:9000/user/edureka/Weather

-----RUN DeIdentify and JOIN programe with properties file
--load data sets 
[edureka@localhost joindata]$ hadoop fs -put customer /user/edureka/
[edureka@localhost joindata]$ hadoop fs -ls /user/edureka/customer
Found 1 items
-rw-r--r--   1 edureka supergroup     391355 2018-02-11 11:19 /user/edureka/customer/custs.txt
[edureka@localhost joindata]$ hadoop fs -put transaction /user/edureka/
[edureka@localhost joindata]$ hadoop fs -ls /user/edureka/transaction
Found 1 items
-rw-r--r--   1 edureka supergroup    4418139 2018-02-11 11:20 /user/edureka/transaction/txns.txt
[edureka@localhost joindata]$ 

--my-spark.properties

#spark.app.name=TestApp
spark.master=yarn
#spark.master.deloyMode=cluster
spark.eventLog.enabled=true
spark.eventLog.dir=hdfs://localhost:9000/user/edureka/EVENTLOG
spark.submit.deployMode=cluster

--create EVENTLOG dir in hdfs for loggging as per properties file
$ hadoop fs -mkdir EVENTLOG

--to view hdfs file on UI
http://localhost:50070  // click utilities and brows /

----run deidentify programe

$ spark-submit --verbose --properties-file my-spark.properties --class com.laboros.spark.driver.DeIdentifyDriver2 sparkdemo_2.11-0.1.0-SNAPSHOT.jar /user/edureka/DeIdentify

Using properties file: my-spark.properties
Adding default property: spark.eventLog.enabled=true
Adding default property: spark.master=yarn
Adding default property: spark.submit.deployMode=cluster
Adding default property: spark.eventLog.dir=hdfs://localhost:9000/user/edureka/EVENTLOG
Parsed arguments:
  master                  yarn
  deployMode              cluster
  executorMemory          null
  executorCores           null
  totalExecutorCores      null
  propertiesFile          my-spark.properties
  driverMemory            null
  driverCores             null
  driverExtraClassPath    null
  driverExtraLibraryPath  null
  driverExtraJavaOptions  null
  supervise               false
  queue                   null
  numExecutors            null
  files                   null
  pyFiles                 null
  archives                null
  mainClass               com.laboros.spark.driver.DeIdentifyDriver2
  primaryResource         file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar
  name                    com.laboros.spark.driver.DeIdentifyDriver2
  childArgs               [/user/edureka/DeIdentify]
  jars                    null
  packages                null
  packagesExclusions      null
  repositories            null
  verbose                 true

Spark properties used, including those specified through
 --conf and those from the properties file my-spark.properties:
  spark.eventLog.enabled -> true
  spark.submit.deployMode -> cluster
  spark.eventLog.dir -> hdfs://localhost:9000/user/edureka/EVENTLOG
  spark.master -> yarn

    
Main class:
org.apache.spark.deploy.yarn.Client
Arguments:
--jar
file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar
--class
com.laboros.spark.driver.DeIdentifyDriver2
--arg
/user/edureka/DeIdentify
System properties:
spark.eventLog.enabled -> true
SPARK_SUBMIT -> true
spark.app.name -> com.laboros.spark.driver.DeIdentifyDriver2
spark.submit.deployMode -> cluster
spark.eventLog.dir -> hdfs://localhost:9000/user/edureka/EVENTLOG
spark.master -> yarn
Classpath elements:
file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar


18/02/11 11:30:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/11 11:30:19 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 10.0.2.15 instead (on interface enp0s3)
18/02/11 11:30:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/02/11 11:30:25 INFO RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
18/02/11 11:30:26 INFO Client: Requesting a new application from cluster with 1 NodeManagers
18/02/11 11:30:26 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)
18/02/11 11:30:26 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead
18/02/11 11:30:26 INFO Client: Setting up container launch context for our AM
18/02/11 11:30:26 INFO Client: Setting up the launch environment for our AM container
18/02/11 11:30:26 INFO Client: Preparing resources for our AM container
18/02/11 11:30:29 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
18/02/11 11:30:34 INFO Client: Uploading resource file:/tmp/spark-5abd54d3-042e-41ef-bf94-fc6da13f14ee/__spark_libs__4316778135264312186.zip -> hdfs://localhost:9000/user/edureka/.sparkStaging/application_1518327512030_0001/__spark_libs__4316778135264312186.zip
18/02/11 11:30:43 INFO Client: Uploading resource file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar -> hdfs://localhost:9000/user/edureka/.sparkStaging/application_1518327512030_0001/sparkdemo_2.11-0.1.0-SNAPSHOT.jar
18/02/11 11:30:47 INFO Client: Uploading resource file:/tmp/spark-5abd54d3-042e-41ef-bf94-fc6da13f14ee/__spark_conf__4124996036396685151.zip -> hdfs://localhost:9000/user/edureka/.sparkStaging/application_1518327512030_0001/__spark_conf__.zip
18/02/11 11:30:49 INFO SecurityManager: Changing view acls to: edureka
18/02/11 11:30:49 INFO SecurityManager: Changing modify acls to: edureka
18/02/11 11:30:49 INFO SecurityManager: Changing view acls groups to: 
18/02/11 11:30:49 INFO SecurityManager: Changing modify acls groups to: 
18/02/11 11:30:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(edureka); groups with view permissions: Set(); users  with modify permissions: Set(edureka); groups with modify permissions: Set()
18/02/11 11:30:49 INFO Client: Submitting application application_1518327512030_0001 to ResourceManager
18/02/11 11:30:55 INFO YarnClientImpl: Submitted application application_1518327512030_0001
18/02/11 11:30:56 INFO Client: Application report for application_1518327512030_0001 (state: ACCEPTED)
18/02/11 11:30:56 INFO Client: 
	 client token: N/A
	 diagnostics: [Sun Feb 11 11:30:55 +0530 2018] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:8192, vCores:8> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 0.0 % ; Queue's Absolute max capacity = 100.0 % ; 
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: default
	 start time: 1518328851656
	 final status: UNDEFINED
	 tracking URL: http://localhost:8088/proxy/application_1518327512030_0001/
	 user: edureka
18/02/11 11:31:36 INFO Client: Application report for application_1518327512030_0001 (state: ACCEPTED)
18/02/11 11:31:37 INFO Client: Application report for application_1518327512030_0001 (state: ACCEPTED)
18/02/11 11:31:38 INFO Client: Application report for application_1518327512030_0001 (state: RUNNING)
18/02/11 11:31:38 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 10.0.2.15
	 ApplicationMaster RPC port: 0
	 queue: default
	 start time: 1518328851656
	 final status: UNDEFINED
	 tracking URL: http://localhost:8088/proxy/application_1518327512030_0001/
	 user: edureka
18/02/11 11:32:34 INFO Client: Application report for application_1518327512030_0001 (state: RUNNING)
18/02/11 11:32:35 INFO Client: Application report for application_1518327512030_0001 (state: RUNNING)
18/02/11 11:32:36 INFO Client: Application report for application_1518327512030_0001 (state: FINISHED)
18/02/11 11:32:36 INFO Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: 10.0.2.15
	 ApplicationMaster RPC port: 0
	 queue: default
	 start time: 1518328851656
	 final status: SUCCEEDED
	 tracking URL: http://localhost:8088/proxy/application_1518327512030_0001/
	 user: edureka
18/02/11 11:32:37 INFO ShutdownHookManager: Shutdown hook called
18/02/11 11:32:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-5abd54d3-042e-41ef-bf94-fc6da13f14ee

[edureka@localhost sparkjobs]$ hadoop fs -cat /user/edureka/EVENTLOG/application_1518327512030_0001_1
{"Event":"SparkListenerLogStart","Spark Version":"2.1.1"}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"driver","Host":"10.0.2.15","Port":41690},"Maximum Memory":384093388,"Timestamp":1518328895880}
{"Event":"SparkListenerEnvironmentUpdate","JVM Information":{"Java Home":"/usr/lib/jvm/jdk1.8.0_144/jre","Java Version":"1.8.0_144 (Oracle Corporation)","Scala Version":"version 2.11.8"},"Spark Properties":{"spark.driver.host":"10.0.2.15","spark.eventLog.enabled":"true","spark.ui.port":"0","spark.driver.port":"46045","spark.yarn.app.id":"application_1518327512030_0001","spark.app.name":"DeIdentifyDriver2","spark.scheduler.mode":"FIFO","spark.executor.id":"driver","spark.yarn.app.container.log.dir":"/usr/lib/hadoop-2.8.1/logs/userlogs/application_1518327512030_0001/container_1518327512030_0001_01_000001","spark.submit.deployMode":"cluster","spark.master":"yarn","spark.ui.filters":"org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter","spark.eventLog.dir":"hdfs://localhost:9000/user/edureka/EVENTLOG","spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS":"localhost","spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES":"http://localhost:8088/proxy/application_1518327512030_0001","spark.app.id":"application_1518327512030_0001"},"System Properties":{"java.io.tmpdir":"/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/tmp","line.separator":"\n","path.separator":":","sun.management.compiler":"HotSpot 64-Bit Tiered Compilers","sun.cpu.endian":"little","java.specification.version":"1.8","java.vm.specification.name":"Java Virtual Machine Specification","java.vendor":"Oracle Corporation","java.vm.specification.version":"1.8","user.home":"/root","file.encoding.pkg":"sun.io","sun.nio.ch.bugLevel":"","sun.arch.data.model":"64","sun.boot.library.path":"/usr/lib/jvm/jdk1.8.0_144/jre/lib/amd64","user.dir":"/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001","java.library.path":":/usr/lib/hadoop-2.8.1/lib/native:/usr/lib/hadoop-2.8.1/lib/native:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib","sun.cpu.isalist":"","os.arch":"amd64","java.vm.version":"25.144-b01","java.endorsed.dirs":"/usr/lib/jvm/jdk1.8.0_144/jre/lib/endorsed","java.runtime.version":"1.8.0_144-b01","java.vm.info":"mixed mode","java.ext.dirs":"/usr/lib/jvm/jdk1.8.0_144/jre/lib/ext:/usr/java/packages/lib/ext","java.runtime.name":"Java(TM) SE Runtime Environment","file.separator":"/","io.netty.maxDirectMemory":"0","java.class.version":"52.0","java.specification.name":"Java Platform API Specification","sun.boot.class.path":"/usr/lib/jvm/jdk1.8.0_144/jre/lib/resources.jar:/usr/lib/jvm/jdk1.8.0_144/jre/lib/rt.jar:/usr/lib/jvm/jdk1.8.0_144/jre/lib/sunrsasign.jar:/usr/lib/jvm/jdk1.8.0_144/jre/lib/jsse.jar:/usr/lib/jvm/jdk1.8.0_144/jre/lib/jce.jar:/usr/lib/jvm/jdk1.8.0_144/jre/lib/charsets.jar:/usr/lib/jvm/jdk1.8.0_144/jre/lib/jfr.jar:/usr/lib/jvm/jdk1.8.0_144/jre/classes","file.encoding":"UTF-8","user.timezone":"Asia/Kolkata","java.specification.vendor":"Oracle Corporation","sun.java.launcher":"SUN_STANDARD","os.version":"3.10.0-514.26.2.el7.x86_64","sun.os.patch.level":"unknown","java.vm.specification.vendor":"Oracle Corporation","user.country":"US","sun.jnu.encoding":"UTF-8","user.language":"en","java.vendor.url":"http://java.oracle.com/","java.awt.printerjob":"sun.print.PSPrinterJob","java.awt.graphicsenv":"sun.awt.X11GraphicsEnvironment","awt.toolkit":"sun.awt.X11.XToolkit","os.name":"Linux","java.vm.vendor":"Oracle Corporation","java.vendor.url.bug":"http://bugreport.sun.com/bugreport/","user.name":"root","java.vm.name":"Java HotSpot(TM) 64-Bit Server VM","sun.java.command":"org.apache.spark.deploy.yarn.ApplicationMaster --class com.laboros.spark.driver.DeIdentifyDriver2 --jar file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar --arg /user/edureka/DeIdentify --properties-file /tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_conf__/__spark_conf__.properties","java.home":"/usr/lib/jvm/jdk1.8.0_144/jre","java.version":"1.8.0_144","sun.io.unicode.encoding":"UnicodeLittle"},"Classpath Entries":{"/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/slf4j-api-1.7.16.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/parquet-hadoop-1.8.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/univocity-parsers-2.2.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/stax-api-1.0.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jackson-xc-1.9.13.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/protobuf-java-2.5.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jackson-annotations-2.6.5.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/mesos-1.0.0-shaded-protobuf.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hadoop-yarn-common-2.7.3.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jdo-api-3.0.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jersey-container-servlet-2.22.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jackson-jaxrs-1.9.13.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/avro-mapred-1.7.7-hadoop2.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/spark-launcher_2.11-2.1.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/parquet-encoding-1.8.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/parquet-column-1.8.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/leveldbjni-all-1.8.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/guice-3.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_conf__":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hadoop-mapreduce-client-common-2.7.3.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/bcprov-jdk15on-1.51.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hk2-locator-2.4.0-b34.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hk2-api-2.4.0-b34.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jets3t-0.9.3.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jackson-module-paranamer-2.6.5.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/commons-io-2.4.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/netty-all-4.0.42.Final.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/javax.annotation-api-1.2.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hadoop-auth-2.7.3.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hadoop-yarn-server-common-2.7.3.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/pyrolite-4.13.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jersey-client-2.22.2.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/spark-streaming_2.11-2.1.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/commons-compress-1.4.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/scala-xml_2.11-1.0.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/gson-2.2.4.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/apacheds-i18n-2.0.0-M15.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jackson-databind-2.6.5.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jersey-common-2.22.2.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hadoop-yarn-api-2.7.3.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jackson-core-asl-1.9.13.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/commons-dbcp-1.4.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/spark-core_2.11-2.1.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jsr305-1.3.9.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/bonecp-0.8.0.RELEASE.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/parquet-jackson-1.8.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/opencsv-2.3.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/datanucleus-core-3.2.10.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/xmlenc-0.52.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/pmml-model-1.2.15.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/janino-3.0.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/commons-net-2.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hive-exec-1.2.1.spark2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jta-1.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jline-2.12.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jpam-1.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/osgi-resource-locator-1.0.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/chill_2.11-0.8.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/mx4j-3.0.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/spark-hive_2.11-2.1.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/etc/hadoop":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/spark-repl_2.11-2.1.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/metrics-jvm-3.1.2.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jcl-over-slf4j-1.7.16.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hadoop-mapreduce-client-core-2.7.3.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/spark-sketch_2.11-2.1.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/calcite-core-1.2.0-incubating.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/spark-mllib-local_2.11-2.1.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/commons-math3-3.4.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jersey-server-2.22.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/spark-network-shuffle_2.11-2.1.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jaxb-api-2.2.2.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/json4s-ast_2.11-3.2.11.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/lz4-1.3.0.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/commons-logging-1.1.3.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hadoop-client-2.7.3.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/javax.ws.rs-api-2.0.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/commons-collections-3.2.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/guice-servlet-3.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/minlog-1.3.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/httpcore-4.4.4.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/libfb303-0.9.3.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/spark-sql_2.11-2.1.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jersey-media-jaxb-2.22.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/scala-reflect-2.11.8.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/joda-time-2.9.3.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/kryo-shaded-3.0.3.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/calcite-avatica-1.2.0-incubating.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/spire-macros_2.11-0.7.4.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/java-xmlbuilder-1.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jul-to-slf4j-1.7.16.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hadoop-annotations-2.7.3.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/httpclient-4.5.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/metrics-json-3.1.2.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hadoop-yarn-server-web-proxy-2.7.3.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/datanucleus-rdbms-3.2.9.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/metrics-core-3.1.2.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jackson-core-2.6.5.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/py4j-0.10.4.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/arpack_combined_all-0.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/core-1.1.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/spark-network-common_2.11-2.1.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/ST4-4.0.4.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/commons-crypto-1.0.0.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/oro-2.0.8.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hive-cli-1.2.1.spark2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/avro-1.7.7.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/commons-beanutils-1.7.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jtransforms-2.4.0.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/datanucleus-api-jdo-3.2.6.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/htrace-core-3.1.0-incubating.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/javolution-5.5.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/super-csv-2.2.0.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/spark-mllib_2.11-2.1.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/spark-yarn_2.11-2.1.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/antlr4-runtime-4.5.3.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/avro-ipc-1.7.7.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.3.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/shapeless_2.11-2.0.0.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/commons-lang3-3.5.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/libthrift-0.9.3.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/calcite-linq4j-1.2.0-incubating.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/RoaringBitmap-0.5.11.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/activation-1.1.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.3.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/scala-compiler-2.11.8.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/stringtemplate-3.2.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/curator-recipes-2.6.0.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/scalap-2.11.8.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/antlr-2.7.7.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hadoop-mapreduce-client-app-2.7.3.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/javax.servlet-api-3.1.0.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/log4j-1.2.17.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/guava-14.0.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/antlr-runtime-3.4.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/validation-api-1.1.0.Final.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/commons-compiler-3.0.0.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/slf4j-log4j12-1.7.16.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/xbean-asm5-shaded-4.4.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hive-jdbc-1.2.1.spark2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/paranamer-2.3.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/spark-tags_2.11-2.1.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/pmml-schema-1.2.15.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/parquet-common-1.8.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/spark-catalyst_2.11-2.1.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jsp-api-2.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/breeze-macros_2.11-0.12.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/json4s-core_2.11-3.2.11.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/ivy-2.4.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/stax-api-1.0-2.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/objenesis-2.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/api-asn1-api-1.0.0-M20.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jodd-core-3.5.2.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/scala-library-2.11.8.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jersey-guava-2.22.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hive-beeline-1.2.1.spark2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/snappy-0.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jackson-module-scala_2.11-2.6.5.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/javax.inject-1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/commons-pool-1.5.4.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/commons-lang-2.6.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/javassist-3.18.1-GA.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/apache-log4j-extras-1.2.17.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/compress-lzf-1.0.3.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/spark-graphx_2.11-2.1.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/commons-digester-1.8.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/xz-1.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hadoop-common-2.7.3.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/commons-codec-1.10.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/mail-1.4.7.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/derby-10.12.1.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/xercesImpl-2.9.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/spark-unsafe_2.11-2.1.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/javax.inject-2.4.0-b34.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/snappy-java-1.1.2.6.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/spire_2.11-0.7.4.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hadoop-yarn-client-2.7.3.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/parquet-hadoop-bundle-1.6.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/breeze_2.11-0.12.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/chill-java-0.8.0.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/commons-configuration-1.6.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jersey-container-servlet-core-2.22.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jackson-mapper-asl-1.9.13.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/spark-hive-thriftserver_2.11-2.1.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/metrics-graphite-3.1.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/json4s-jackson_2.11-3.2.11.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/commons-beanutils-core-1.8.0.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/base64-2.3.8.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hive-metastore-1.2.1.spark2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/commons-httpclient-3.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/parquet-format-2.3.0-incubating.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hk2-utils-2.4.0-b34.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/commons-cli-1.2.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/netty-3.8.0.Final.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jetty-6.1.26.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/stream-2.7.0.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/api-util-1.0.0-M20.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/spark-mesos_2.11-2.1.1.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/JavaEWAH-0.3.2.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/aopalliance-1.0.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/eigenbase-properties-1.1.5.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/curator-client-2.6.0.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/curator-framework-2.6.0.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/jetty-util-6.1.26.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/hadoop-hdfs-2.7.3.jar":"System Classpath","/tmp/hadoop-root/nm-local-dir/usercache/edureka/appcache/application_1518327512030_0001/container_1518327512030_0001_01_000001/__spark_libs__/zookeeper-3.4.6.jar":"System Classpath","/usr/lib/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar":"System Classpath"}}
{"Event":"SparkListenerApplicationStart","App Name":"DeIdentifyDriver2","App ID":"application_1518327512030_0001","Timestamp":1518328893923,"User":"edureka","App Attempt ID":"1","Driver Logs":{"stdout":"http://localhost:8042/node/containerlogs/container_1518327512030_0001_01_000001/edureka/stdout?start=-4096","stderr":"http://localhost:8042/node/containerlogs/container_1518327512030_0001_01_000001/edureka/stderr?start=-4096"}}
{"Event":"SparkListenerExecutorAdded","Timestamp":1518328904689,"Executor ID":"1","Executor Info":{"Host":"localhost","Total Cores":1,"Log Urls":{"stdout":"http://localhost:8042/node/containerlogs/container_1518327512030_0001_01_000002/edureka/stdout?start=-4096","stderr":"http://localhost:8042/node/containerlogs/container_1518327512030_0001_01_000002/edureka/stderr?start=-4096"}}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"1","Host":"localhost","Port":44769},"Maximum Memory":384093388,"Timestamp":1518328904978}
{"Event":"SparkListenerExecutorAdded","Timestamp":1518328906921,"Executor ID":"2","Executor Info":{"Host":"localhost","Total Cores":1,"Log Urls":{"stdout":"http://localhost:8042/node/containerlogs/container_1518327512030_0001_01_000003/edureka/stdout?start=-4096","stderr":"http://localhost:8042/node/containerlogs/container_1518327512030_0001_01_000003/edureka/stderr?start=-4096"}}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"2","Host":"localhost","Port":45933},"Maximum Memory":384093388,"Timestamp":1518328907007}
{"Event":"SparkListenerJobStart","Job ID":0,"Submission Time":1518328912736,"Stage Infos":[{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"take at DeIdentifyDriver2.scala:59","Number of Tasks":1,"RDD Info":[{"RDD ID":3,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"2\",\"name\":\"map\"}","Callsite":"map at DeIdentifyDriver2.scala:40","Parent IDs":[2],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Number of Partitions":8,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":1,"Name":"/user/edureka/DeIdentify","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at DeIdentifyDriver2.scala:18","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Number of Partitions":8,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"/user/edureka/DeIdentify","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at DeIdentifyDriver2.scala:18","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Number of Partitions":8,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":2,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"1\",\"name\":\"map\"}","Callsite":"map at DeIdentifyDriver2.scala:28","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Number of Partitions":8,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.take(RDD.scala:1327)\ncom.laboros.spark.driver.DeIdentifyDriver2$.main(DeIdentifyDriver2.scala:59)\ncom.laboros.spark.driver.DeIdentifyDriver2.main(DeIdentifyDriver2.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\norg.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:637)","Accumulables":[]}],"Stage IDs":[0],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"3\",\"name\":\"take\"}"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"take at DeIdentifyDriver2.scala:59","Number of Tasks":1,"RDD Info":[{"RDD ID":3,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"2\",\"name\":\"map\"}","Callsite":"map at DeIdentifyDriver2.scala:40","Parent IDs":[2],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Number of Partitions":8,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":1,"Name":"/user/edureka/DeIdentify","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at DeIdentifyDriver2.scala:18","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Number of Partitions":8,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"/user/edureka/DeIdentify","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at DeIdentifyDriver2.scala:18","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Number of Partitions":8,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":2,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"1\",\"name\":\"map\"}","Callsite":"map at DeIdentifyDriver2.scala:28","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Number of Partitions":8,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.take(RDD.scala:1327)\ncom.laboros.spark.driver.DeIdentifyDriver2$.main(DeIdentifyDriver2.scala:59)\ncom.laboros.spark.driver.DeIdentifyDriver2.main(DeIdentifyDriver2.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\norg.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:637)","Submission Time":1518328913305,"Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"3\",\"name\":\"take\"}"}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":0,"Index":0,"Attempt":0,"Launch Time":1518328913538,"Executor ID":"2","Host":"localhost","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":0,"Index":0,"Attempt":0,"Launch Time":1518328913538,"Executor ID":"2","Host":"localhost","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1518328954470,"Failed":false,"Killed":false,"Accumulables":[{"ID":0,"Name":"internal.metrics.executorDeserializeTime","Update":4208,"Value":4208,"Internal":true,"Count Failed Values":true},{"ID":1,"Name":"internal.metrics.executorDeserializeCpuTime","Update":505016606,"Value":505016606,"Internal":true,"Count Failed Values":true},{"ID":2,"Name":"internal.metrics.executorRunTime","Update":35752,"Value":35752,"Internal":true,"Count Failed Values":true},{"ID":3,"Name":"internal.metrics.executorCpuTime","Update":1226828560,"Value":1226828560,"Internal":true,"Count Failed Values":true},{"ID":4,"Name":"internal.metrics.resultSize","Update":2879,"Value":2879,"Internal":true,"Count Failed Values":true},{"ID":5,"Name":"internal.metrics.jvmGCTime","Update":4272,"Value":4272,"Internal":true,"Count Failed Values":true},{"ID":6,"Name":"internal.metrics.resultSerializationTime","Update":1,"Value":1,"Internal":true,"Count Failed Values":true},{"ID":20,"Name":"internal.metrics.input.bytesRead","Update":65536,"Value":65536,"Internal":true,"Count Failed Values":true},{"ID":21,"Name":"internal.metrics.input.recordsRead","Update":10,"Value":10,"Internal":true,"Count Failed Values":true}]},"Task Metrics":{"Executor Deserialize Time":4208,"Executor Deserialize CPU Time":505016606,"Executor Run Time":35752,"Executor CPU Time":1226828560,"Result Size":2879,"JVM GC Time":4272,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Local Bytes Read":0,"Total Records Read":0},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":65536,"Records Read":10},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[{"Block ID":"broadcast_1_piece0","Status":{"Storage Level":{"Use Disk":false,"Use Memory":true,"Deserialized":false,"Replication":1},"Memory Size":2147,"Disk Size":0}},{"Block ID":"broadcast_1","Status":{"Storage Level":{"Use Disk":false,"Use Memory":true,"Deserialized":true,"Replication":1},"Memory Size":3680,"Disk Size":0}},{"Block ID":"broadcast_0_piece0","Status":{"Storage Level":{"Use Disk":false,"Use Memory":true,"Deserialized":false,"Replication":1},"Memory Size":23730,"Disk Size":0}},{"Block ID":"broadcast_0","Status":{"Storage Level":{"Use Disk":false,"Use Memory":true,"Deserialized":true,"Replication":1},"Memory Size":336752,"Disk Size":0}}]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"take at DeIdentifyDriver2.scala:59","Number of Tasks":1,"RDD Info":[{"RDD ID":3,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"2\",\"name\":\"map\"}","Callsite":"map at DeIdentifyDriver2.scala:40","Parent IDs":[2],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Number of Partitions":8,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":1,"Name":"/user/edureka/DeIdentify","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at DeIdentifyDriver2.scala:18","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Number of Partitions":8,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"/user/edureka/DeIdentify","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at DeIdentifyDriver2.scala:18","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Number of Partitions":8,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":2,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"1\",\"name\":\"map\"}","Callsite":"map at DeIdentifyDriver2.scala:28","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Deserialized":false,"Replication":1},"Number of Partitions":8,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.take(RDD.scala:1327)\ncom.laboros.spark.driver.DeIdentifyDriver2$.main(DeIdentifyDriver2.scala:59)\ncom.laboros.spark.driver.DeIdentifyDriver2.main(DeIdentifyDriver2.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\norg.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:637)","Submission Time":1518328913305,"Completion Time":1518328954542,"Accumulables":[{"ID":20,"Name":"internal.metrics.input.bytesRead","Value":65536,"Internal":true,"Count Failed Values":true},{"ID":2,"Name":"internal.metrics.executorRunTime","Value":35752,"Internal":true,"Count Failed Values":true},{"ID":5,"Name":"internal.metrics.jvmGCTime","Value":4272,"Internal":true,"Count Failed Values":true},{"ID":4,"Name":"internal.metrics.resultSize","Value":2879,"Internal":true,"Count Failed Values":true},{"ID":1,"Name":"internal.metrics.executorDeserializeCpuTime","Value":505016606,"Internal":true,"Count Failed Values":true},{"ID":21,"Name":"internal.metrics.input.recordsRead","Value":10,"Internal":true,"Count Failed Values":true},{"ID":3,"Name":"internal.metrics.executorCpuTime","Value":1226828560,"Internal":true,"Count Failed Values":true},{"ID":6,"Name":"internal.metrics.resultSerializationTime","Value":1,"Internal":true,"Count Failed Values":true},{"ID":0,"Name":"internal.metrics.executorDeserializeTime","Value":4208,"Internal":true,"Count Failed Values":true}]}}
{"Event":"SparkListenerJobEnd","Job ID":0,"Completion Time":1518328954592,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerApplicationEnd","Timestamp":1518328954901}
[edureka@localhost sparkjobs]$ 

---------run on --master local --deploy-mode client

[edureka@localhost sparkjobs]$ spark-submit --verbose --master local --deploy-mode client --class com.laboros.spark.driver.DeIdentifyDriver2 sparkdemo_2.11-0.1.0-SNAPSHOT.jar /user/edureka/DeIdentify
Using properties file: /usr/lib/spark-2.1.1-bin-hadoop2.7/conf/spark-defaults.conf
Adding default property: spark.master=spark://localhost:7077
Parsed arguments:
  master                  local
  deployMode              client
  executorMemory          null
  executorCores           null
  totalExecutorCores      null
  propertiesFile          /usr/lib/spark-2.1.1-bin-hadoop2.7/conf/spark-defaults.conf
  driverMemory            null
  driverCores             null
  driverExtraClassPath    null
  driverExtraLibraryPath  null
  driverExtraJavaOptions  null
  supervise               false
  queue                   null
  numExecutors            null
  files                   null
  pyFiles                 null
  archives                null
  mainClass               com.laboros.spark.driver.DeIdentifyDriver2
  primaryResource         file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar
  name                    com.laboros.spark.driver.DeIdentifyDriver2
  childArgs               [/user/edureka/DeIdentify]
  jars                    null
  packages                null
  packagesExclusions      null
  repositories            null
  verbose                 true

Spark properties used, including those specified through
 --conf and those from the properties file /usr/lib/spark-2.1.1-bin-hadoop2.7/conf/spark-defaults.conf:
  spark.master -> spark://localhost:7077

    
Main class:
com.laboros.spark.driver.DeIdentifyDriver2
Arguments:
/user/edureka/DeIdentify
System properties:
SPARK_SUBMIT -> true
spark.app.name -> com.laboros.spark.driver.DeIdentifyDriver2
spark.jars -> file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar
spark.submit.deployMode -> client
spark.master -> local
Classpath elements:
file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar


18/02/11 12:01:58 INFO SparkContext: Running Spark version 2.1.1
18/02/11 12:01:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/11 12:01:59 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 10.0.2.15 instead (on interface enp0s3)
18/02/11 12:01:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/02/11 12:01:59 INFO SecurityManager: Changing view acls to: edureka
18/02/11 12:01:59 INFO SecurityManager: Changing modify acls to: edureka
18/02/11 12:01:59 INFO SecurityManager: Changing view acls groups to: 
18/02/11 12:01:59 INFO SecurityManager: Changing modify acls groups to: 
18/02/11 12:01:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(edureka); groups with view permissions: Set(); users  with modify permissions: Set(edureka); groups with modify permissions: Set()
18/02/11 12:02:00 INFO Utils: Successfully started service 'sparkDriver' on port 37669.
18/02/11 12:02:00 INFO SparkEnv: Registering MapOutputTracker
18/02/11 12:02:00 INFO SparkEnv: Registering BlockManagerMaster
18/02/11 12:02:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/02/11 12:02:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/02/11 12:02:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0e7a55d0-1de3-414e-82fe-73993860444b
18/02/11 12:02:00 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
18/02/11 12:02:00 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/11 12:02:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/11 12:02:01 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4040
18/02/11 12:02:01 INFO SparkContext: Added JAR file:/home/edureka/SHAHBAZWS/BATCH170917/sparkjobs/sparkdemo_2.11-0.1.0-SNAPSHOT.jar at spark://10.0.2.15:37669/jars/sparkdemo_2.11-0.1.0-SNAPSHOT.jar with timestamp 1518330721416
18/02/11 12:02:01 INFO Executor: Starting executor ID driver on host localhost
18/02/11 12:02:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39422.
18/02/11 12:02:01 INFO NettyBlockTransferService: Server created on 10.0.2.15:39422
18/02/11 12:02:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/02/11 12:02:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 39422, None)
18/02/11 12:02:01 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:39422 with 366.3 MB RAM, BlockManagerId(driver, 10.0.2.15, 39422, None)
18/02/11 12:02:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 39422, None)
18/02/11 12:02:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 39422, None)
18/02/11 12:02:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 242.2 KB, free 366.1 MB)
18/02/11 12:02:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.2 KB, free 366.0 MB)
18/02/11 12:02:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:39422 (size: 23.2 KB, free: 366.3 MB)
18/02/11 12:02:03 INFO SparkContext: Created broadcast 0 from textFile at DeIdentifyDriver2.scala:18
18/02/11 12:02:04 INFO FileInputFormat: Total input paths to process : 2
18/02/11 12:02:04 INFO SparkContext: Starting job: take at DeIdentifyDriver2.scala:59
18/02/11 12:02:04 INFO DAGScheduler: Got job 0 (take at DeIdentifyDriver2.scala:59) with 1 output partitions
18/02/11 12:02:04 INFO DAGScheduler: Final stage: ResultStage 0 (take at DeIdentifyDriver2.scala:59)
18/02/11 12:02:04 INFO DAGScheduler: Parents of final stage: List()
18/02/11 12:02:04 INFO DAGScheduler: Missing parents: List()
18/02/11 12:02:04 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at map at DeIdentifyDriver2.scala:40), which has no missing parents
18/02/11 12:02:04 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.6 KB, free 366.0 MB)
18/02/11 12:02:04 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 366.0 MB)
18/02/11 12:02:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:39422 (size: 2.1 KB, free: 366.3 MB)
18/02/11 12:02:04 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:996
18/02/11 12:02:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at map at DeIdentifyDriver2.scala:40)
18/02/11 12:02:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
18/02/11 12:02:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6093 bytes)
18/02/11 12:02:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
18/02/11 12:02:04 INFO Executor: Fetching spark://10.0.2.15:37669/jars/sparkdemo_2.11-0.1.0-SNAPSHOT.jar with timestamp 1518330721416
18/02/11 12:02:04 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:37669 after 85 ms (0 ms spent in bootstraps)
18/02/11 12:02:04 INFO Utils: Fetching spark://10.0.2.15:37669/jars/sparkdemo_2.11-0.1.0-SNAPSHOT.jar to /tmp/spark-fa51a8f8-2ea0-440f-871e-0a0d65a9af72/userFiles-d35ef002-78f1-44e2-9c74-26a0bb923893/fetchFileTemp3075034262045245941.tmp
18/02/11 12:02:05 INFO Executor: Adding file:/tmp/spark-fa51a8f8-2ea0-440f-871e-0a0d65a9af72/userFiles-d35ef002-78f1-44e2-9c74-26a0bb923893/sparkdemo_2.11-0.1.0-SNAPSHOT.jar to class loader
18/02/11 12:02:05 INFO HadoopRDD: Input split: hdfs://localhost:9000/user/edureka/DeIdentify/healthcare_Sample_dataset1.csv:0+820059
18/02/11 12:02:05 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
18/02/11 12:02:05 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
18/02/11 12:02:05 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
18/02/11 12:02:05 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
18/02/11 12:02:05 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
18/02/11 12:02:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1885 bytes result sent to driver
18/02/11 12:02:05 INFO DAGScheduler: ResultStage 0 (take at DeIdentifyDriver2.scala:59) finished in 0.878 s
18/02/11 12:02:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 826 ms on localhost (executor driver) (1/1)
18/02/11 12:02:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
18/02/11 12:02:05 INFO DAGScheduler: Job 0 finished: take at DeIdentifyDriver2.scala:59, took 1.098275 s
11111,bbb1,12/10/1950,XXXXX-XXXX,XXXXX-XXXX,XXXXX-XXXX,M,Diabetes,78,
11112,bbb2,12/10/1984,XXXXX-XXXX,XXXXX-XXXX,XXXXX-XXXX,F,PCOS,67,
11113,bbb3,712/11/1940,XXXXX-XXXX,XXXXX-XXXX,XXXXX-XXXX,M,Fever,90,
11114,bbb4,12/12/1950,XXXXX-XXXX,XXXXX-XXXX,XXXXX-XXXX,F,Cold,88,
11115,bbb5,12/13/1960,XXXXX-XXXX,XXXXX-XXXX,XXXXX-XXXX,M,Blood Pressure,76,
11116,bbb6,12/14/1970,XXXXX-XXXX,XXXXX-XXXX,XXXXX-XXXX,F,Malaria,84,
11117,bbb7,12/15/1980,XXXXX-XXXX,XXXXX-XXXX,XXXXX-XXXX,M,Swine Flu,64,
11118,bbb8,12/16/1990,XXXXX-XXXX,XXXXX-XXXX,XXXXX-XXXX,F,Fever,33,
11119,bbb9,12/17/2000,XXXXX-XXXX,XXXXX-XXXX,XXXXX-XXXX,F,Fever,29,
11120,bbb1,12/10/1950,XXXXX-XXXX,XXXXX-XXXX,XXXXX-XXXX,M,Diabetes,78,
18/02/11 12:02:05 INFO SparkContext: Invoking stop() from shutdown hook
18/02/11 12:02:05 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
18/02/11 12:02:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/02/11 12:02:05 INFO MemoryStore: MemoryStore cleared
18/02/11 12:02:05 INFO BlockManager: BlockManager stopped
18/02/11 12:02:05 INFO BlockManagerMaster: BlockManagerMaster stopped
18/02/11 12:02:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/02/11 12:02:05 INFO SparkContext: Successfully stopped SparkContext
18/02/11 12:02:05 INFO ShutdownHookManager: Shutdown hook called
18/02/11 12:02:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-fa51a8f8-2ea0-440f-871e-0a0d65a9af72
[edureka@localhost sparkjobs]$  

----run join programe

$ spark-submit --verbose --master local --deploy-mode client --class com.laboros.spark.driver.JoinDriver2 sparkdemo_2.11-0.1.0-SNAPSHOT.jar /user/edureka/customer /user/edureka/transaction > output.txt

//see the result in output.txt

